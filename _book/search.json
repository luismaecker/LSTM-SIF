[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting SIF with climatic drivers",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "quarto_book/intro.html",
    "href": "quarto_book/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCode\n1+1\n\n\n2\n\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/references.html",
    "href": "quarto_book/references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#section",
    "href": "quarto_book/intro.html#section",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 ",
    "text": "1.1 \n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-initalize-gee",
    "href": "quarto_book/intro.html#function-to-initalize-gee",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Function to initalize GEE",
    "text": "1.1 Function to initalize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to download the Corine LC 100m data for year 2000",
    "text": "1.2 Function to download the Corine LC 100m data for year 2000\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function to download the Corine LC 100m data",
    "text": "1.3 Function to download the Corine LC 100m data\nThis function will download the Corine LC 100m data for the year 2000 from google earth engine. Only forest classes are retained: - 311: Decidous forest - 312: Coniferous forest - 313: Mixed forest\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function",
    "href": "quarto_book/intro.html#function",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function",
    "text": "1.3 Function\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-german-border-data",
    "href": "quarto_book/intro.html#function-to-download-german-border-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to Download German border data",
    "text": "1.2 Function to Download German border data\nThis function will download a shapefile delineating the german border, from google earth engine.\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "href": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "title": "1  Downloading Auxillary data",
    "section": "1.4 Function to create a sif sample tif",
    "text": "1.4 Function to create a sif sample tif\nThis tif will late be used as a reference for the cube spatial grid and transform\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_book/intro.html#compiling-all-functions-into-one",
    "href": "quarto_book/intro.html#compiling-all-functions-into-one",
    "title": "1  Downloading Auxillary data",
    "section": "1.5 Compiling all functions into one",
    "text": "1.5 Compiling all functions into one\nThis function\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif\n    create_sif_sample(out_path = tif_sample_path, cube_subset= cube_subset, write=download)"
  },
  {
    "objectID": "quarto_book/intro.html#run-the-script-if-its-called",
    "href": "quarto_book/intro.html#run-the-script-if-its-called",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Run the script if its called",
    "text": "1.2 Run the script if its called\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#packages-and-functions",
    "href": "quarto_book/intro.html#packages-and-functions",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Packages and Functions",
    "text": "1.1 Packages and Functions\nThis code stores 4 Functions. - 1. to initalize Google Earth Engine - 2. to download a shapefile of the German border - 3. to download Corine Landcover data from Google Earth Engine - 4. to create a sample tif of the - 5. A function that loads all\nAdditionaly\n# scripts/load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "href": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "title": "2  Downloading Auxillary data",
    "section": "2.1 Packages and Functions",
    "text": "2.1 Packages and Functions\nThis code stores 5 Functions:\n\nto initialize Google Earth Engine\nto download a shapefile of the German border\nto download Corine Landcover data from Google Earth Engine\nto create a sample tif\nA function that wraps all other functions\n\nAdditionaly two functions are loaded in the beginning from utils.py\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\n# scripts/01_load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "href": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "title": "2  Downloading Auxillary data",
    "section": "2.2 Run the script if its called",
    "text": "2.2 Run the script if its called\n\n\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-functions",
    "href": "quarto_book/01_load_aux_data.html#run-the-functions",
    "title": "2  Loading auxillary data",
    "section": "2.3 Run the functions",
    "text": "2.3 Run the functions\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "href": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "title": "4  Preprocessing ESDC",
    "section": "4.1 Packages and Functions",
    "text": "4.1 Packages and Functions\n\ncalculate_forest_percentage: Computes the percentage of forest cover within a specified window of land cover data by identifying pixels that match predefined forest classes.\nresample_corine_to_sif: Resamples CORINE land cover data to match the resolution and dimensions of a sample SIF raster, calculating forest cover percentages for each resampled cell and returning a flipped array of these percentages.\ncube_preprocess: Clips a given data cube to the borders of Germany, calculates forest cover percentages over the grid, adds this data to the cube, creates a binary forest cover layer, and optionally writes the processed data to disk.\n\nimport xarray as xr\nimport rioxarray as rio\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport geopandas as gpd\nfrom utils import create_paths, create_cube_subset\n\n# Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n    \"\"\"\n    Calculate the percentage of forest cover in a specified window.\n\n    Parameters:\n    lc_window (Window): The window of the land cover data to analyze.\n    lc_data (ndarray): The land cover data array.\n    forest_classes (list): List of land cover classes considered as forest.\n\n    Returns:\n    float: Percentage of forest cover in the specified window.\n    \"\"\"\n    forest_mask = np.isin(\n        lc_data[lc_window.row_off:lc_window.row_off + lc_window.height,\n                lc_window.col_off:lc_window.col_off + lc_window.width],\n        forest_classes\n    )\n\n    total_pixels = forest_mask.size\n    forest_pixels = np.sum(forest_mask)\n    percentage = (forest_pixels / total_pixels) * 100\n\n    return percentage\n\n# Function to calculate the forest percentages of the CORINE land cover data over the cube grid\ndef resample_corine_to_sif(corine_file_path, sample_path):\n    \"\"\"\n    Resample CORINE land cover data to match the resolution and dimensions of a sample SIF raster,\n    and calculate forest cover percentages for each resampled cell.\n\n    Parameters:\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n\n    Returns:\n    ndarray: Array of forest cover percentages for each resampled cell.\n    \"\"\"\n    # Open the land cover raster\n    with rasterio.open(corine_file_path) as src_lc:\n        lc_data = src_lc.read()\n        lc_transform = src_lc.transform\n\n    # Open the sample SIF raster\n    with rasterio.open(sample_path) as src_sif:\n        sif_transform = src_sif.transform\n        sif_meta = src_sif.meta\n\n    # Determine the new shape and transform for the resampled raster\n    new_height = sif_meta['height']\n    new_width = sif_meta['width']\n\n    # Initialize the new resampled data array\n    resampled_forest_percentage = np.zeros((new_height, new_width), dtype=np.float32)\n\n    # Define forest classes\n    forest_classes = [311, 312, 313]\n\n    # Calculate the window size in the original land cover data\n    window_height = int(abs(sif_transform[4] / lc_transform[4]))\n    window_width = int(abs(sif_transform[0] / lc_transform[0]))\n\n    # Loop through each cell in the SIF raster resolution\n    for i in range(new_height):\n        for j in range(new_width):\n            # Define the window in the land cover data\n            window = Window(col_off=j*window_width, row_off=i*window_height, width=window_width, height=window_height)\n            \n            # Calculate the forest percentage in the window\n            forest_percentage = calculate_forest_percentage(window, lc_data.squeeze(), forest_classes)\n            \n            # Assign the percentage to the resampled data array\n            resampled_forest_percentage[i, j] = forest_percentage\n\n    resampled_forest_percentage_flip = np.flipud(resampled_forest_percentage)\n\n    return resampled_forest_percentage_flip\n\ndef cube_preprocess(cube_subset, germany_gpd, corine_file_path, sample_path, out_path_crop, out_path_mask, all_touched=True, write=True):\n    \"\"\"\n    Preprocess the data cube by clipping to Germany's border, calculating forest cover percentages,\n    and adding this data to the cube. Optionally write the processed data to disk.\n\n    Parameters:\n    cube_subset (xarray.Dataset): The data cube subset to preprocess.\n    germany_gpd (GeoDataFrame): GeoDataFrame containing Germany's borders.\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n    out_path_crop (str): Path to save the cropped data cube.\n    out_path_mask (str): Path to save the masked data cube.\n    all_touched (bool): Whether to include all pixels touched by the geometry. Defaults to True.\n    write (bool): Whether to write the output to disk. Defaults to True.\n\n    Returns:\n    xarray.Dataset: The processed data cube subset.\n    \"\"\"\n    print(\"Preprocessing cube\")\n\n    # Clip the xarray dataset using the Germany geometry\n    print(\"Clipping cube to Germany border\")\n    cube_subset_crop = cube_subset.rio.clip(\n        germany_gpd.geometry.values,\n        germany_gpd.crs,\n        drop=False, \n        all_touched=all_touched\n    )\n    \n    # Calculate forest cover percentage over cube grid\n    print(\"Calculate forest cover percentage over cube grid\")\n    resampled_forest_percentages = resample_corine_to_sif(corine_file_path, sample_path)\n\n    # Setup the dimensions for the resampled forest percentage\n    dims = ('lat', 'lon')  \n\n    # Add the resampled forest cover to the cube\n    cube_subset_crop['forest_cover'] = xr.DataArray(\n        resampled_forest_percentages, dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # Add a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\n    cube_subset_crop['forest_cover_50'] = xr.DataArray(\n        (resampled_forest_percentages &gt;= 50).astype(int), dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # Mask the cube where forest cover is less than 50%\n    cube_subset_crop_mask = cube_subset_crop.where(cube_subset_crop['forest_cover_50'] == 1)\n\n    if write:\n        cube_subset_crop.to_netcdf(out_path_crop)\n        cube_subset_crop_mask.to_netcdf(out_path_mask)\n        print(\"Wrote cropped cube with added forest percentages and binary mask to disk at:\", out_path_crop)\n        print(\"Wrote cropped and masked cube to disk at:\", out_path_mask)\n                                       \n    return cube_subset_crop"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html",
    "href": "quarto_book/02_cube_preprocessing.html",
    "title": "3  Preprocessing ESDC",
    "section": "",
    "text": "4 Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n1+1\n```"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages",
    "href": "quarto_book/01_load_aux_data.html#packages",
    "title": "2  Loading auxillary data",
    "section": "2.1 Packages",
    "text": "2.1 Packages\nHere we load packages and additionaly two functions from utils.py.\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n# load custom function from utils.py\nfrom utils import create_paths"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#functions",
    "href": "quarto_book/01_load_aux_data.html#functions",
    "title": "2  Loading auxillary data",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nThis code stores 5 Functions:\n\ninitialize_gee: Authenticates and initializes the Google Earth Engine (GEE) API for further data processing\n\ndownload_german_border: Downloads the German border data from GEE and optionally exports it to a specified file path\n\nload_corine: Downloads and preprocesses CORINE land cover data for a specified region, exporting it to a file if needed\n\ncreate_sif_sample: Creates a sample TIFF file from the SIF data in the provided data cube subset and saves it to a specified output path\n\nload_aux_data: Integrates the auxiliary data loading process by initializing GEE, downloading the German border and CORINE data, and creating a SIF sample TIFF, managing file paths and downloads\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "href": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "title": "4  Preprocessing ESDC",
    "section": "4.2 Run the script",
    "text": "4.2 Run the script\nif __name__ == \"__main__\":\n    from utils import create_cube_subset\n\n    data_path = \"data\"\n\n    # Load the cube subset\n    cube_subset = create_cube_subset()\n\n    # Create file paths and if they don't exist, create folders\n    germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the Germany border geometry\n    germany_gpd = gpd.read_file(germany_shp_path)\n\n    # Preprocess the cube\n    cube_preprocess(\n        cube_subset, germany_gpd, corine_file_path, tif_sample_path, \n        out_path_crop=cube_crop_path, out_path_mask=cube_crop_mask_path, \n        all_touched=True, write=True\n    )"
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#packages-and-functions",
    "href": "quarto_book/03_base_analysis.html#packages-and-functions",
    "title": "5  Basic sif analysis",
    "section": "5.1 Packages and Functions",
    "text": "5.1 Packages and Functions\n\nplot_save_diff: Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two. Saves the figure to a specified path.\nplot_timeseries: Plots and saves the time series of SIF data within a specified time range. Optionally displays the plot.\nbase_analysis: Calculates the summer mean for each year in the dataset and the changes in SIF compared to the baseline period up to 2017. Returns the summer mean cube, the baseline mean to 2017, and the changes for specified years.\n\n# scripts/base_analysis.py\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom utils import create_paths\n\n\ndef base_analysis(cube, years=[2018, 2019]):\n    \"\"\"\n    Perform the base analysis by calculating the summer mean for each year and the change compared to the baseline up to 2017.\n\n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    years : list, optional\n    \"\"\"\n\n\n    # Calculate summer mean for each year\n    summer_data = cube.sel(time=cube['time.season'] == 'JJA')\n    summer_mean_cube = summer_data.groupby('time.year').mean(dim='time')\n\n    # Calculate change in summer mean SIF for each year compared to baseline up to 2017\n    changes = {}\n    summer_mean_to_2017 = summer_mean_cube.sel(year=slice(None, 2017)).mean(dim='year')\n\n    for year in years:\n        summer_mean = summer_mean_cube.sel(year=year)\n        change = summer_mean - summer_mean_to_2017\n        changes[year] = change\n    \n\n    return summer_mean_cube, summer_mean_to_2017, changes \n\n# Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two.\ndef plot_save_diff(ref_period,data_2018, changes, save_path):\n\n    # Create the figure and 2x2 subplots\n    fig, axd = plt.subplot_mosaic([['upleft', 'right'],\n                                ['lowleft', 'right']], layout='constrained', figsize=(10, 7))\n\n    # Plot each time slice on a different subplot\n    img1 = ref_period.plot(ax=axd[\"upleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"upleft\"].set_title(\"Mean 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"upleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"upleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img2 = data_2018.plot(ax=axd[\"lowleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"lowleft\"].set_title(\"Mean 2018\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"lowleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"lowleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img3 = changes[2018].plot(ax=axd[\"right\"], cmap=\"RdBu\", vmin=-0.15, vmax=0.15, add_colorbar=False)\n    axd[\"right\"].set_title(\"Difference SIF 2018 to mean of 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"right\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"right\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    # Add colorbars for each row\n    divider1 = make_axes_locatable(axd[\"upleft\"])\n    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img1, cax=cax1, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider2 = make_axes_locatable(axd[\"lowleft\"])\n    cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img2, cax=cax2, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider3 = make_axes_locatable(axd[\"right\"])\n    cax3 = divider3.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img3, cax=cax3, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n\n    \n    # save the plot\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    return None\n\n\n# TODO: add timeseries plot\n\ndef plot_timeseries(time_series, save_path, time_range=[None, None], show = False):\n    \"\"\"\n    Plot and save the timeseries of the SIF data.\n    \n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    save_path : str \n        The path where the plot should be saved.\n    variable : str, optional\n        The variable to plot.\n    show : bool, optional\n        Whether to show the plot.\n    \n    \"\"\"\n\n    time_series = time_series.sel(time=slice(time_range[0], time_range[1]))\n\n    plt.figure(figsize=(10, 6))\n    \n    time_series.plot(marker='o', color='blue', linestyle='dashed')\n\n    plt.title(f'Time Series of SIF', fontsize=14)\n    plt.xlabel('Time', fontsize=12)\n    plt.ylabel('Sun-Induced Chlorophyll Fluorescence at 757 nm \\n [W m^-2 sr^-1 um^-1]', fontsize=12)\n    plt.grid(True, which='major', axis='both')\n    #plt.ylim(.1, .6) \n            \n            \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    if show:\n        plt.show()\n\n\nif __name__ == \"__main__\":\n    \n    data_path = \"data\"\n\n    # Get path to the cube subset\n    _, _, _, cube_crop_path, _ = create_paths(data_path)\n\n    # Load the cropped cube subset\n    cube_subset_crop = xr.open_dataset(cube_crop_path)\n\n    # only use sif variable\n    cube_subset_crop_sif = cube_subset_crop.sif_gosif\n\n    # Calculate the mean of the SIF data over time\n    cube_sif_mean = cube_subset_crop_sif.mean(dim=['lat', 'lon'])\n\n    # Create the results directory\n    os.makedirs(os.path.join(\"results\", \"figures\"), exist_ok=True)\n\n    # Save plot of timeseries:\n    plot_timeseries(cube_sif_mean, save_path = os.path.join(\"results\", \"figures\", \"timeseries_full.png\"))\n    plot_timeseries(cube_sif_mean, time_range= [\"2015-01-01\", \"2022-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))"
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#run-the-script",
    "href": "quarto_book/03_base_analysis.html#run-the-script",
    "title": "5  Basic sif analysis",
    "section": "5.2 Run the script",
    "text": "5.2 Run the script\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom utils import create_paths\n\n\ndef base_analysis(cube, years=[2018, 2019]):\n    \"\"\"\n    Perform the base analysis by calculating the summer mean for each year and the change compared to the baseline up to 2017.\n\n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    years : list, optional\n    \"\"\"\n\n\n    # Calculate summer mean for each year\n    summer_data = cube.sel(time=cube['time.season'] == 'JJA')\n    summer_mean_cube = summer_data.groupby('time.year').mean(dim='time')\n\n    # Calculate change in summer mean SIF for each year compared to baseline up to 2017\n    changes = {}\n    summer_mean_to_2017 = summer_mean_cube.sel(year=slice(None, 2017)).mean(dim='year')\n\n    for year in years:\n        summer_mean = summer_mean_cube.sel(year=year)\n        change = summer_mean - summer_mean_to_2017\n        changes[year] = change\n    \n\n    return summer_mean_cube, summer_mean_to_2017, changes \n\n# Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two.\ndef plot_save_diff(ref_period,data_2018, changes, save_path):\n\n    # Create the figure and 2x2 subplots\n    fig, axd = plt.subplot_mosaic([['upleft', 'right'],\n                                ['lowleft', 'right']], layout='constrained', figsize=(10, 7))\n\n    # Plot each time slice on a different subplot\n    img1 = ref_period.plot(ax=axd[\"upleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"upleft\"].set_title(\"Mean 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"upleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"upleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img2 = data_2018.plot(ax=axd[\"lowleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"lowleft\"].set_title(\"Mean 2018\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"lowleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"lowleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img3 = changes[2018].plot(ax=axd[\"right\"], cmap=\"RdBu\", vmin=-0.15, vmax=0.15, add_colorbar=False)\n    axd[\"right\"].set_title(\"Difference SIF 2018 to mean of 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"right\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"right\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    # Add colorbars for each row\n    divider1 = make_axes_locatable(axd[\"upleft\"])\n    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img1, cax=cax1, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider2 = make_axes_locatable(axd[\"lowleft\"])\n    cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img2, cax=cax2, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider3 = make_axes_locatable(axd[\"right\"])\n    cax3 = divider3.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img3, cax=cax3, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n\n    \n    # save the plot\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    return None\n\n\n# TODO: add timeseries plot\n\ndef plot_timeseries(time_series, save_path, time_range=[None, None], show = False):\n    \"\"\"\n    Plot and save the timeseries of the SIF data.\n    \n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    save_path : str \n        The path where the plot should be saved.\n    variable : str, optional\n        The variable to plot.\n    show : bool, optional\n        Whether to show the plot.\n    \n    \"\"\"\n\n    time_series = time_series.sel(time=slice(time_range[0], time_range[1]))\n\n    plt.figure(figsize=(10, 6))\n    \n    time_series.plot(marker='o', color='blue', linestyle='dashed')\n\n    plt.title(f'Time Series of SIF', fontsize=14)\n    plt.xlabel('Time', fontsize=12)"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#packages-and-functions",
    "href": "quarto_book/04_test_modelling.html#packages-and-functions",
    "title": "6  Prelimnary Modelling",
    "section": "6.1 Packages and Functions",
    "text": "6.1 Packages and Functions\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import create_paths, start_logging\nfrom config import variables\nfrom modelling_functions import full_modelling, data_preprocess, save_results"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#run-the-script",
    "href": "quarto_book/04_test_modelling.html#run-the-script",
    "title": "6  Prelimnary Modelling",
    "section": "6.2 Run the script",
    "text": "6.2 Run the script\ndef main():\n\n    data_path = \"data\"\n\n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n\n    ############  Modelling ############\n\n    # Create lookback array\n    look_backs = [15,30,45]\n\n    # Define the parameter grid for the local model\n    param_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n    }\n\n\n\n    cv = TimeSeriesSplit(n_splits=3)\n    # Run the local models for a subset\n    # and saving the results - filename is created based on model type and lookback\n    for look_back in look_backs:\n        \n        # os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n        \n        # Create a log file: \n        #start_logging(os.path.join(\"results\", \"logs\", f\"local_models_{look_back}.log\"))\n\n        # not auto regressive\n        output_data_local_auto = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=False, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n\n        save_results(output_data_local_auto, look_back, auto_regressive=False, global_model=False)\n\n        # auto regressive\n        output_data_local_noauto = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=True, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n        \n        save_results(output_data_local_noauto, look_back, auto_regressive=True, global_model=False)\n\n\n    # Run the global model on the full dataset\n    # and saving the results - filename is created based on model type and lookback\n\n    # grid for global model\n    # The parameters were reduced based on a first run for the results of the local model\n\n    param_grid_global = {\n        'units_lstm': [64, 128],\n        'activation': ['tanh'], \n        'epochs': [100],    \n        'learning_rate': [0.0001],\n        'dropout_rate': [0.2],\n        'batch_size': [25],\n        'num_lstm_layers': [1, 2]\n    }\n\n    # The lookback is set to 30 for the global model, as it was the best performing for the local models\n    look_back = 30\n\n    # not auto regressive\n    output_data_global_auto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_auto, look_back, auto_regressive=False, global_model=False)\n\n    # auto regressive\n    output_data_global_noauto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_noauto, look_back, auto_regressive=True, global_model=False)\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_book/05_modelling.html#packages-and-functions",
    "href": "quarto_book/05_modelling.html#packages-and-functions",
    "title": "7  Actual Modelling",
    "section": "7.1 Packages and Functions",
    "text": "7.1 Packages and Functions\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import create_paths, start_logging\nfrom config import variables, param_grid_local, param_grid_global\nfrom modelling_functions import full_modelling, data_preprocess, save_results"
  },
  {
    "objectID": "quarto_book/05_modelling.html#run-the-script",
    "href": "quarto_book/05_modelling.html#run-the-script",
    "title": "7  Actual Modelling",
    "section": "7.2 Run the script",
    "text": "7.2 Run the script\ndef main():\n\n    data_path = \"data\"\n\n    ############  Setup ############\n\n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n\n    ############  Modelling ############\n\n    # Set lookback to 30 as it was the best performing in the test modelling\n    look_back = 30\n\n\n    # print grid searc parameter grid for the local model (can be found in config.py)\n    logging.info(print(param_grid_local))\n\n    cv = TimeSeriesSplit(n_splits=2)\n    \n    os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n\n\n    # Run the gridsearch gridsearch and training again with the local model without auto regression as it was the best perfoming model in 04_test_modelling.py\n    # Also relu is not used as activation function as tanh was performing better in every case\n            \n    # Create a log file: \n    start_logging(os.path.join(\"results\", \"logs\", f\"final_local_models_{look_back}.log\"))\n\n    # not auto regressive\n    output_data_local_auto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_local, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_local_auto, look_back,\n                 auto_regressive=False, global_model=False,\n                 out_path=os.path.join(\"results\", \"modelling\", \"final\", \"results_full_local_auto_l30.json\"))\n\n    \nif __name__ == \"__main__\":\n    main()"
  }
]