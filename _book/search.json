[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "",
    "text": "Outline\nThis is the quarto book for the project “Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning”. This project was created as part of two course projects in the “Earth System Data Science and Remote Sensing” Master at Leipzig University. The courses are Scientific Writing by Prof. Dr. Miguel Mahecha and Spatio-temporal Data by Dr. Guido Kraemer. This quarto book contains all code used for the analysis, shows the main results and briefly discusses them, as well as a Data Management Plan. The project is fully reproducible and all data is openly available. The project can be found at this Gitlab repository."
  },
  {
    "objectID": "quarto_book/intro.html",
    "href": "quarto_book/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCode\n1+1\n\n\n2\n\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/references.html",
    "href": "quarto_book/references.html",
    "title": "References",
    "section": "",
    "text": "Hersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András\nHorányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The\nERA5 Global Reanalysis.” Quarterly Journal of the Royal\nMeteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product\nof Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and\nReanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "quarto_book/intro.html#section",
    "href": "quarto_book/intro.html#section",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 ",
    "text": "1.1 \n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-initalize-gee",
    "href": "quarto_book/intro.html#function-to-initalize-gee",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Function to initalize GEE",
    "text": "1.1 Function to initalize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to download the Corine LC 100m data for year 2000",
    "text": "1.2 Function to download the Corine LC 100m data for year 2000\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function to download the Corine LC 100m data",
    "text": "1.3 Function to download the Corine LC 100m data\nThis function will download the Corine LC 100m data for the year 2000 from google earth engine. Only forest classes are retained: - 311: Decidous forest - 312: Coniferous forest - 313: Mixed forest\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function",
    "href": "quarto_book/intro.html#function",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function",
    "text": "1.3 Function\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-german-border-data",
    "href": "quarto_book/intro.html#function-to-download-german-border-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to Download German border data",
    "text": "1.2 Function to Download German border data\nThis function will download a shapefile delineating the german border, from google earth engine.\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "href": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "title": "1  Downloading Auxillary data",
    "section": "1.4 Function to create a sif sample tif",
    "text": "1.4 Function to create a sif sample tif\nThis tif will late be used as a reference for the cube spatial grid and transform\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_book/intro.html#compiling-all-functions-into-one",
    "href": "quarto_book/intro.html#compiling-all-functions-into-one",
    "title": "1  Downloading Auxillary data",
    "section": "1.5 Compiling all functions into one",
    "text": "1.5 Compiling all functions into one\nThis function\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif\n    create_sif_sample(out_path = tif_sample_path, cube_subset= cube_subset, write=download)"
  },
  {
    "objectID": "quarto_book/intro.html#run-the-script-if-its-called",
    "href": "quarto_book/intro.html#run-the-script-if-its-called",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Run the script if its called",
    "text": "1.2 Run the script if its called\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#packages-and-functions",
    "href": "quarto_book/intro.html#packages-and-functions",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Packages and Functions",
    "text": "1.1 Packages and Functions\nThis code stores 4 Functions. - 1. to initalize Google Earth Engine - 2. to download a shapefile of the German border - 3. to download Corine Landcover data from Google Earth Engine - 4. to create a sample tif of the - 5. A function that loads all\nAdditionaly\n# scripts/load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "href": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "title": "2  Downloading Auxillary data",
    "section": "2.1 Packages and Functions",
    "text": "2.1 Packages and Functions\nThis code stores 5 Functions:\n\nto initialize Google Earth Engine\nto download a shapefile of the German border\nto download Corine Landcover data from Google Earth Engine\nto create a sample tif\nA function that wraps all other functions\n\nAdditionaly two functions are loaded in the beginning from utils.py\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\n# scripts/01_load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "href": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "title": "2  Downloading Auxillary data",
    "section": "2.2 Run the script if its called",
    "text": "2.2 Run the script if its called\n\n\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-functions",
    "href": "quarto_book/01_load_aux_data.html#run-the-functions",
    "title": "2  Loading auxillary data",
    "section": "2.3 Run the functions",
    "text": "2.3 Run the functions"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "href": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "title": "3  Preprocessing ESDC",
    "section": "3.1 Packages and Functions",
    "text": "3.1 Packages and Functions\n\ncalculate_forest_percentage: Computes the percentage of forest cover within a specified window of land cover data by identifying pixels that match predefined forest classes.\nresample_corine_to_sif: Resamples CORINE land cover data to match the resolution and dimensions of a sample SIF raster, calculating forest cover percentages for each resampled cell and returning a flipped array of these percentages.\ncube_preprocess: Clips a given data cube to the borders of Germany, calculates forest cover percentages over the grid, adds this data to the cube, creates a binary forest cover layer, and optionally writes the processed data to disk."
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html",
    "href": "quarto_book/02_cube_preprocessing.html",
    "title": "3  Preprocessing ESDC",
    "section": "",
    "text": "4 Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n1+1\n```"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages",
    "href": "quarto_book/01_load_aux_data.html#packages",
    "title": "2  Loading auxillary data",
    "section": "2.1 Packages",
    "text": "2.1 Packages\nHere we load packages and additionaly two functions from utils.py.\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#functions",
    "href": "quarto_book/01_load_aux_data.html#functions",
    "title": "2  Loading auxillary data",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nThis code stores 5 Functions:\n\ninitialize_gee: Authenticates and initializes the Google Earth Engine (GEE) API for further data processing\n\ndownload_german_border: Downloads the German border data from GEE and optionally exports it to a specified file path\n\nload_corine: Downloads and preprocesses CORINE land cover data for a specified region, exporting it to a file if needed\n\ncreate_sif_sample: Creates a sample TIFF file from the SIF data in the provided data cube subset and saves it to a specified output path\n\nload_aux_data: Integrates the auxiliary data loading process by initializing GEE, downloading the German border and CORINE data, and creating a SIF sample TIFF, managing file paths and downloads"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "href": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "title": "3  Preprocessing ESDC",
    "section": "3.2 Run the script",
    "text": "3.2 Run the script"
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#packages-and-functions",
    "href": "quarto_book/03_base_analysis.html#packages-and-functions",
    "title": "4  Basic sif analysis",
    "section": "4.1 Packages and Functions",
    "text": "4.1 Packages and Functions\n\nplot_save_diff: Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two. Saves the figure to a specified path.\nplot_timeseries: Plots and saves the time series of SIF data within a specified time range. Optionally displays the plot.\nbase_analysis: Calculates the summer mean for each year in the dataset and the changes in SIF compared to the baseline period up to 2017. Returns the summer mean cube, the baseline mean to 2017, and the changes for specified years."
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#run-the-script",
    "href": "quarto_book/03_base_analysis.html#run-the-script",
    "title": "4  Basic sif analysis",
    "section": "4.2 Run the script",
    "text": "4.2 Run the script"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#packages-and-functions",
    "href": "quarto_book/04_test_modelling.html#packages-and-functions",
    "title": "5  Prelimnary Modelling",
    "section": "5.1 Packages and Functions",
    "text": "5.1 Packages and Functions\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import create_paths, start_logging\nfrom config import variables\nfrom modelling_functions import  setup_model_data, save_results, full_modelling\n\n\n\ndef main():\n\n    data_path = \"data\"\n   \n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n   \n    # do preprocessing: scaling and creating a dataframe, as well as getting the lat lon pairs defining all pixels\n    all_data_scaled, lat_lon_pairs, scalar_y = setup_model_data(cube_subset_crop_mask, variables)\n\n\n\n    ############  Modelling ############\n\n    # Create lookback array\n    look_backs = [15,30,45]\n\n    # Define the parameter grid for the local model\n    param_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n    }\n\n\n\n    cv = TimeSeriesSplit(n_splits=3)\n    # Run the local models for a subset\n    # and saving the results - filename is created based on model type and lookback\n    for look_back in look_backs:\n        \n        # os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n        \n        # Create a log file: \n        #start_logging(os.path.join(\"results\", \"logs\", f\"local_models_{look_back}.log\"))\n\n        # not auto regressive\n        output_data_local_auto, _ = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=False, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n\n        save_results(output_data_local_auto, look_back, auto_regressive=False, global_model=False)\n\n        # auto regressive\n        output_data_local_noauto, _ = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=True, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n        \n        save_results(output_data_local_noauto, look_back, auto_regressive=True, global_model=False)\n\n\n    # Run the global model on the full dataset\n    # and saving the results - filename is created based on model type and lookback\n\n    # grid for global model\n    # The parameters were reduced based on a first run for the results of the local model\n\n    param_grid_global = {\n        'units_lstm': [64, 128],\n        'activation': ['tanh'], \n        'epochs': [100],    \n        'learning_rate': [0.0001],\n        'dropout_rate': [0.2],\n        'batch_size': [25],\n        'num_lstm_layers': [1, 2]\n    }\n\n    # The lookback is set to 30 for the global model, as it was the best performing for the local models\n    look_back = 30\n\n    # not auto regressive\n    output_data_global_auto, _ = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_auto, look_back, auto_regressive=False, global_model=False)\n\n    # auto regressive\n    output_data_global_noauto, _ = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_noauto, look_back, auto_regressive=True, global_model=False)"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#run-the-script",
    "href": "quarto_book/04_test_modelling.html#run-the-script",
    "title": "5  Prelimnary Modelling",
    "section": "5.2 Run the script",
    "text": "5.2 Run the script\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_book/05_modelling.html#packages-and-functions",
    "href": "quarto_book/05_modelling.html#packages-and-functions",
    "title": "6  Actual Modelling",
    "section": "6.1 Packages and Functions",
    "text": "6.1 Packages and Functions"
  },
  {
    "objectID": "quarto_book/05_modelling.html#run-the-script",
    "href": "quarto_book/05_modelling.html#run-the-script",
    "title": "6  Actual Modelling",
    "section": "6.2 Run the script",
    "text": "6.2 Run the script"
  },
  {
    "objectID": "quarto_book/config.html",
    "href": "quarto_book/config.html",
    "title": "Config",
    "section": "",
    "text": "This scripts stores callable variables, used in mutiple scripts.\n# scripts/config.py\n\n\nvariables = [\n    \"sif_gosif\",\n    \"evaporation_era5\",\n    \"precipitation_era5\",\n    \"radiation_era5\",\n    \"air_temperature_2m\",\n    \"max_air_temperature_2m\",\n    \"min_air_temperature_2m\",\n]\n\n\n# Define the time and spatial subset\nmin_time = '2002-01-01'\nmax_time = '2021-12-31'\nlon_min, lon_max = 5.866, 15.042\nlat_min, lat_max = 47.270, 55.058\n\n\n############ Hyperparameter Grid setup for GridSearchCV  ############  \n\n# for local model\nparam_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n}\n\n# for global model\nparam_grid_global = {\n    'units_lstm': [64, 128],\n    'activation': ['tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2]\n}\n\n# for final model ( without \"relu\", 3 layer setup and dropout of 0.4, as these hyperparameters were not performing well in the test)\nparam_grid_final = {\n    'units_lstm': [64, 128],\n    'activation': ['tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2]\n}"
  },
  {
    "objectID": "quarto_book/utils.html",
    "href": "quarto_book/utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "These script stores some basic functions that are used in multiple scripts.\n\nconfigure_logging: Sets up logging to a specified file with INFO level.\nstart_logging: Resets logging configuration and sets it up with a new filename, ensuring a new log file is created when run in the same session.\ncreate_dir: Creates a directory if it does not already exist based on the provided file path.\ncreate_paths: Creates and returns paths for Germany’s border shapefile, CORINE land cover data for the year 2000, a SIF sample TIFF, the croped datacube, and the croped and masked datacube.\ncreate_cube_subset: Loads the Earth System Data Cube, subsets it based for the time range of sif and spatial boundaries of germany, selects the relevant variables, and adds the CRS (Coordinate Reference System).\n\n# scripts/utils.py\n\n\nfrom config import variables, min_time, max_time, lon_min, lon_max, lat_min, lat_max\nfrom xcube.core.store import new_data_store\nimport rioxarray as rio\nimport os\nimport logging\n\n\n\n# Function to setup logging to file\ndef configure_logging(filename):\n    logging.basicConfig(\n        filename=filename,\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n    )\n\n# Function to reset logging (so it creates a new file when run in the same session)\ndef start_logging(filename):\n    # Get the root logger\n    root_logger = logging.getLogger()\n    \n    # Remove all handlers associated with the root logger\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Configure logging with the new filename\n    configure_logging(filename)\n\n\ndef create_dir(file_path):\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n\ndef create_paths(data_path):\n\n    # Create path and folder for germany border shapefile\n    germany_shp_path = os.path.join(data_path, \"germany_shape\", 'germany_border.shp')\n    create_dir(germany_shp_path)\n\n    # Create path and folder for corine data year 2000\n    corine_file_path = os.path.join(data_path, \"landcover\", f\"forest_cover_2000.tif\")\n    create_dir(corine_file_path)\n\n    # Create path and folderfor sif sample tif\n    tif_sample_path= os.path.join(data_path, \"cubes\", \"cube_sif_sample.tif\")\n    create_dir(tif_sample_path)\n\n    cube_crop_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop.nc\")\n    cube_crop_mask_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop_mask.nc\")\n\n    return germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path\n\n\ndef create_cube_subset(variables = variables, \n                        min_time = min_time, max_time = max_time,\n                        lon_min = lon_min, lon_max = lon_max,\n                        lat_min = lat_min, lat_max = lat_max):\n\n    # Initalize xcube store\n    store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n    store.list_data_ids()\n\n    # Load cube from store\n    cube = store.open_data( 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr')\n\n    # Subset the cube\n    cube_subset = cube.sel(time=slice(min_time, max_time)) \\\n                    .sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n\n    # select only specified variables from esdc\n    cube_subset = cube_subset[variables]\n\n    # Add crs to the cube\n    cube_subset.rio.write_crs(4326, inplace = True)\n\n    return cube_subset"
  },
  {
    "objectID": "quarto_book/modelling_functions.html",
    "href": "quarto_book/modelling_functions.html",
    "title": "Modelling functions",
    "section": "",
    "text": "In this script all functions used for modelling and data specific modelling, as well as model evaluation and plotting.\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y\n\n\n\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index\n\n\n\n\n########################################################\n# Modelling\n########################################################\n\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_\n\n\n\n\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array\n\n\n############  Evaluate model ############\ndef evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }\n\n\n\n############  Writing results ############\n\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")\n\n\n############  Full modelling function ############\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data\n\n\n    # Save the results to a JSON file, in a folder named after the lookback period\n\n\n############  Plotting ############\n\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#section",
    "href": "quarto_book/modelling_functions.html#section",
    "title": "Modelling functions",
    "section": "",
    "text": "import os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y\n\n\n\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index\n\n\n\n\n########################################################\n# Modelling\n########################################################\n\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_\n\n\n\n\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array\n\n\n############  Evaluate model ############\ndef evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }\n\n\n\n############  Writing results ############\n\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")\n\n\n############  Full modelling function ############\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data\n\n\n    # Save the results to a JSON file, in a folder named after the lookback period\n\n\n############  Plotting ############\n\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#data-preprocessing",
    "href": "quarto_book/modelling_functions.html#data-preprocessing",
    "title": "Modelling functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#data-restructuring-and-splitting",
    "href": "quarto_book/modelling_functions.html#data-restructuring-and-splitting",
    "title": "Modelling functions",
    "section": "Data restructuring and splitting",
    "text": "Data restructuring and splitting\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#modelling",
    "href": "quarto_book/modelling_functions.html#modelling",
    "title": "Modelling functions",
    "section": "Modelling",
    "text": "Modelling\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#predicting",
    "href": "quarto_book/modelling_functions.html#predicting",
    "title": "Modelling functions",
    "section": "Predicting",
    "text": "Predicting\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#fit-model-with-best-params-and-evaluating",
    "href": "quarto_book/modelling_functions.html#fit-model-with-best-params-and-evaluating",
    "title": "Modelling functions",
    "section": "Fit model with best params and evaluating",
    "text": "Fit model with best params and evaluating\ndef fit_evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#write-results",
    "href": "quarto_book/modelling_functions.html#write-results",
    "title": "Modelling functions",
    "section": "Write results",
    "text": "Write results\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#modelling-with-gridsearchcv",
    "href": "quarto_book/modelling_functions.html#modelling-with-gridsearchcv",
    "title": "Modelling functions",
    "section": "Modelling with GridSearchCV",
    "text": "Modelling with GridSearchCV\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#full-modelling-workflow-function",
    "href": "quarto_book/modelling_functions.html#full-modelling-workflow-function",
    "title": "Modelling functions",
    "section": "Full modelling workflow function",
    "text": "Full modelling workflow function\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#plotting",
    "href": "quarto_book/modelling_functions.html#plotting",
    "title": "Modelling functions",
    "section": "Plotting",
    "text": "Plotting\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-mod_pre",
    "href": "quarto_book/modelling_functions.html#sec-mod_pre",
    "title": "Modelling functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-restruc",
    "href": "quarto_book/modelling_functions.html#sec-restruc",
    "title": "Modelling functions",
    "section": "Data restructuring and splitting",
    "text": "Data restructuring and splitting\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2018 = df_scaled[df_scaled[\"time\"].dt.year == 2018].index[0]\n    val_end_index =  first_index_2018 - (look_back)\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-model",
    "href": "quarto_book/modelling_functions.html#sec-model",
    "title": "Modelling functions",
    "section": "Modelling with GridSearchCV",
    "text": "Modelling with GridSearchCV\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-pred",
    "href": "quarto_book/modelling_functions.html#sec-pred",
    "title": "Modelling functions",
    "section": "Predicting",
    "text": "Predicting\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-eval",
    "href": "quarto_book/modelling_functions.html#sec-eval",
    "title": "Modelling functions",
    "section": "Fit model with best params and evaluating",
    "text": "Fit model with best params and evaluating\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-write",
    "href": "quarto_book/modelling_functions.html#sec-write",
    "title": "Modelling functions",
    "section": "Write results",
    "text": "Write results\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-full",
    "href": "quarto_book/modelling_functions.html#sec-full",
    "title": "Modelling functions",
    "section": "Full modelling workflow function",
    "text": "Full modelling workflow function\ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data, test_index"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-plot",
    "href": "quarto_book/modelling_functions.html#sec-plot",
    "title": "Modelling functions",
    "section": "Plotting",
    "text": "Plotting\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Authors",
    "text": "Authors\nLuis Maecker - maecker@studserv.uni-leipzig.de\nImke Ott -\nMoritz Mischi -"
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Abstract",
    "text": "Abstract\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. In this study, we use a Long Short-Term Memory (LSTM) neural network to forecast SIF for German forests, with ERA5 climate data as predictors. Data of the years 2018 and 2019 is used to evaluate the models performance during and after the heatwave in Germany. We show that the model does generally well in capturing the temporal dynamics of SIF in response to climatic variations, with an R-squared of 0.75 and a mean absolute error 0.05. However at sites with particularly harsh conditions in 2018 the model did not capture the strong response of the vegetation. Here more research is needed. Still the results show the potential of SIF for ecosystem analysis as well as LSTM for modelling vegetation dynamics based on climatic drivers. Reliable SIF forecasts under differing climatic conditions are a valuable tool to asses potential future impact of climate change on German forests. This can lead to more informed decision-making and better mitigation of these challenges."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Motivation",
    "text": "Motivation\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. In this study, we use a Long Short-Term Memory (LSTM) neural network to forecast SIF for German forests, with ERA5 climate data as predictors. Data of the years 2018 and 2019 is used to evaluate the models performance during and after the heatwave in Germany. Sucessfully modelling SIF using climatic predictors, would allow us to asses the impact of potential future climate changes on the photosynthetic activity of forests and their role in the carbon cycle."
  },
  {
    "objectID": "quarto_book/intro.html#motivation",
    "href": "quarto_book/intro.html#motivation",
    "title": "Introduction",
    "section": "Motivation",
    "text": "Motivation\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. Sucessfully modelling SIF using climatic predictors, would allow us to asses the impact of potential future climate changes on the photosynthetic activity of forests and their role in the carbon cycle."
  },
  {
    "objectID": "quarto_book/intro.html#data",
    "href": "quarto_book/intro.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\n\nEarth System Data Cube\n\nThis dataset is a a data cube provided by the Earth System Data Lab. It is a cube created by preprocessing and harmonising different datasets to a common spatio-temporal grid. The Variables we use:\n\nSIF-Data originally created by by Xing Li and Jingfeng Xiao (Li and Xiao 2019)\n6 ERA-5-variables originally created by Hersbach et al (Hersbach et al. 2020):\n\nevaporation\nprecipitation\nradiation\nair_temperature\nmax_air_temperature\nmin_air_temperature\n\n\n\n\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The ERA5 Global Reanalysis.” Quarterly Journal of the Royal Meteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product of Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "main.html#packages",
    "href": "main.html#packages",
    "title": "1  Full Workflow",
    "section": "1.1 Packages",
    "text": "1.1 Packages\n\n\nCode\nimport sys\nimport os\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rio\nimport json\n\n# Add the parent directory to sys.path\nsys.path.append(os.path.abspath(os.path.join('scripts')))\n\n# from scripts.config import *\nfrom scripts.utils import create_cube_subset, create_paths\nfrom scripts.load_aux_data_01 import load_aux_data\nfrom scripts.cube_preprocessing_02 import cube_preprocess\nfrom scripts.base_analysis_03 import base_analysis, change_plot, plot_timeseries\nfrom scripts.config import variables\nfrom scripts.modelling_functions import plot_multiple_results;\nimport matplotlib.pyplot as plt\n\n# Write data to disk set to False\nwrite_data = False\n\n\ndef plot_multiple_results(results_dict, time_index = None):\n    num_plots = len(results_dict)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(17, num_rows * 7))\n    if num_rows * num_cols &gt; 1:\n        axes = axes.flatten()  # Flatten the axes array for easier iteration\n    else:\n        axes = [axes]  # Ensure axes is iterable\n\n    for i, (lat_lon, data_dict) in enumerate(results_dict.items()):\n        ax = axes[i]\n\n        testY = data_dict['results']['true_values']\n        forecasts = data_dict['results']['predicted_values']\n        mae = data_dict['evaluation']['mae']\n        rmse = data_dict['evaluation']['rmse']\n\n        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n\n        # Generate a time index from the length of the testY data\n        # time_index = range(len(testY))\n\n        ax.plot(time_index, testY, label=\"Actual\")\n        ax.plot(time_index, forecasts, label=\"Predicted\")\n        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Value\")\n        ax.legend()\n        ax.grid(True)\n\n        # Add MSE to the corner\n        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=ax.transAxes, color='red', fontsize=12)\n\n    # Adjust the layout to prevent overlap and make sure all plots are visible\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_forecasts_from_dict(results_dict):\n    \"\"\"\n    This function takes a dictionary containing model results and creates plots for the two locations\n    with the highest and two locations with the lowest RMSE values.\n\n    Args:\n    results_dict (dict): Dictionary containing model results with true and predicted values.\n\n    Returns:\n    None\n    \"\"\"\n    # Extract RMSEs and corresponding locations\n    rmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\n\n    # Sort locations by RMSE\n    sorted_locations = sorted(rmse_values, key=rmse_values.get)\n\n    # Find two locations with highest and two with lowest RMSE\n    lowest_rmse_locations = sorted_locations[:2]\n    highest_rmse_locations = sorted_locations[-2:]\n\n    print(f'Locations with lowest RMSE: {lowest_rmse_locations}, RMSEs: {[rmse_values[loc] for loc in lowest_rmse_locations]}')\n    print(f'Locations with highest RMSE: {highest_rmse_locations}, RMSEs: {[rmse_values[loc] for loc in highest_rmse_locations]}')\n\n    # Function to plot true vs predicted values\n    def plot_forecast(true_values, predicted_values, title):\n        plt.figure(figsize=(14, 7))\n        plt.plot(true_values, label='True Values', color='blue', linewidth=2)\n        plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--', linewidth=2)\n        plt.title(title, fontsize=16)\n        plt.xlabel('Time', fontsize=14)\n        plt.ylabel('SIF', fontsize=14)\n        plt.legend(fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    # Plot for locations with lowest RMSE\n    for loc in lowest_rmse_locations:\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Forecast vs True Values (Lowest RMSE Location: {loc})'\n        )\n\n    # Plot for locations with highest RMSE\n    for loc in highest_rmse_locations:\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Forecast vs True Values (Highest RMSE Location: {loc})'\n        )\n\n\n\n2024-07-28 12:22:16.639184: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-28 12:22:16.641545: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-28 12:22:16.677667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-07-28 12:22:17.475964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\nCode\n\ndef plot_multiple_results(results_dict, keys_to_plot=None, time_index=None):\n    # If no specific keys are provided, plot all entries in the dictionary\n    if keys_to_plot is None:\n        keys_to_plot = list(results_dict.keys())\n    \n    num_plots = len(keys_to_plot)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(17, num_rows * 7))\n    if num_rows * num_cols &gt; 1:\n        axes = axes.flatten()  # Flatten the axes array for easier iteration\n    else:\n        axes = [axes]  # Ensure axes is iterable\n\n    for i, lat_lon in enumerate(keys_to_plot):\n        ax = axes[i]\n        data_dict = results_dict[lat_lon]\n        testY = data_dict['results']['true_values']\n        forecasts = data_dict['results']['predicted_values']\n        mae = data_dict['evaluation']['mae']\n        rmse = data_dict['evaluation']['rmse']\n\n        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n\n        # Generate a time index from the length of the testY data if not provided\n        if time_index is None:\n            time_index = range(len(testY))\n\n        ax.plot(time_index, testY, label=\"Actual\")\n        ax.plot(time_index, forecasts, label=\"Predicted\")\n        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Value\")\n        ax.legend()\n        ax.grid(True)\n\n        # Add RMSE and MAE to the corner\n        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=ax.transAxes, color='red', fontsize=12)\n\n    # Adjust the layout to prevent overlap and make sure all plots are visible\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "main.html#downloading-auxillary-data",
    "href": "main.html#downloading-auxillary-data",
    "title": "1  Full Workflow",
    "section": "1.3 Downloading auxillary data",
    "text": "1.3 Downloading auxillary data\nThe first part of the analysis is to download and preprocess all the necassaty auxillary data:\n\ndownload german border shapefile\ndownload and preprocess Corine data (for the forest mask) and use the border data to define the AOI\ncreate sif sample tif for spatial resolution and transform (used later to resample corine data)\n\nMore Information\n\n\nCode\n# Download auxiliary data (Germany border, Corine landcover data, sample tif)\nload_aux_data(data_path, cube_subset, download = write_data)\n\n# Load the germany border shapefile\ngermany_gpd = gpd.read_file(germany_shp_path)\n\n\n\n            \n            \n\n\nDownloading German border data...\n----------------------------------------------------------------------------------------------------\nProcessing and downloading Corine data...\n----------------------------------------------------------------------------------------------------\nSample path created at: data/cubes/cube_sif_sample.tif\n----------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "main.html#preprocessing-the-cube",
    "href": "main.html#preprocessing-the-cube",
    "title": "1  Full Workflow",
    "section": "1.4 Preprocessing the cube",
    "text": "1.4 Preprocessing the cube\nNext we want to preprocess our data cube.\nThis includes:\n\ncroping the cube with the german border shape\nmasking the cube with the corine forest cover:\n\nCalculate the forest percentage of the Corine landcover data over the cube grid\nadd the forest percentages to the cube\nadd a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\nuse the layer to mask the cube\n\n\nThe results are cube_subset_crop which is croped with the germany border and cube_subset_mask, masked with the forest cover.\nMore Information\n\n\nCode\n# Crop the cube to the extent of Germany and mask it with the Corine landcover data (50% forest cover)\ncube_subset_crop, cube_subset_mask = cube_preprocess(\n        cube_subset, germany_gpd, corine_file_path, cube_sample_path, \n        out_path_crop=cube_crop_path, out_path_mask=cube_crop_mask_path, \n        all_touched=True, write=write_data\n    )\n\n\n\n            \n            \n\n\nPreprocessing cube\nClipping cube to Germany border\nCalculate forest cover percentage over cube grid"
  },
  {
    "objectID": "main.html#setup",
    "href": "main.html#setup",
    "title": "1  Full Workflow",
    "section": "1.2 Setup",
    "text": "1.2 Setup\nFirst we setup the data path and use create_paths() from utils to create paths necessary throughout the analysis.\nSecond we use create_subset() to create a subset of the Earth System Data Cube, croped to:\n\nour AOI, the extent of germany\nthe time span where the SIF variable is avaialble\nto the relevant variables\n\nMore Information\n\n\nCode\n# Create a data directory\ndata_path = \"data\"\nos.makedirs(data_path, exist_ok=True)\n\n# Create paths to the data\ngermany_shp_path, corine_file_path, cube_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n# Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\ncube_subset = create_cube_subset()\n\nprint(cube_subset)\n\n\n\n            \n            \n\n\n&lt;xarray.Dataset&gt; Size: 30MB\nDimensions:                 (time: 920, lat: 31, lon: 37)\nCoordinates:\n  * lat                     (lat) float64 248B 47.38 47.62 47.88 ... 54.62 54.88\n  * lon                     (lon) float64 296B 5.875 6.125 6.375 ... 14.62 14.88\n  * time                    (time) datetime64[ns] 7kB 2002-01-05 ... 2021-12-31\n    spatial_ref             int64 8B 0\nData variables:\n    sif_gosif               (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    evaporation_era5        (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    precipitation_era5      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    radiation_era5          (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    air_temperature_2m      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    max_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    min_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1"
  },
  {
    "objectID": "main.html#basic-analysis",
    "href": "main.html#basic-analysis",
    "title": "1  Full Workflow",
    "section": "1.5 Basic Analysis",
    "text": "1.5 Basic Analysis\n\n# Calculate the temporal changes in the variables \nsummer_sif_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\nShow the summer means cube.\n\nsummer_sif_mean_cube\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'sif_gosif' (year: 20, lat: 31, lon: 37)&gt; Size: 92kB\ndask.array&lt;stack, shape=(20, 31, 37), dtype=float32, chunksize=(1, 31, 25), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat          (lat) float64 248B 47.38 47.62 47.88 ... 54.38 54.62 54.88\n  * lon          (lon) float64 296B 5.875 6.125 6.375 ... 14.38 14.62 14.88\n    spatial_ref  int64 8B 0\n  * year         (year) int64 160B 2002 2003 2004 2005 ... 2018 2019 2020 2021\nAttributes: (12/25)\n    acknowledgment:             https://doi.org/10.3390/rs11050517\n    date_modified:              2022-10-11 22:20:05.841847\n    description:                GOSIF Solar-Induced Chlorophyll Fluorescence ...\n    geospatial_lat_max:         89.87499999999999\n    geospatial_lat_min:         -89.87500000000001\n    geospatial_lat_resolution:  0.25\n    ...                         ...\n    standard_name:              sif\n    temporal_resolution:        8D\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        2000-03-01T00:00:00.000000000\n    time_period:                8D\n    units:                      W m^-2 sr^-1 um^-1xarray.DataArray'sif_gosif'year: 20lat: 31lon: 37dask.array&lt;chunksize=(1, 31, 25), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n89.61 kiB\n3.03 kiB\n\n\nShape\n(20, 31, 37)\n(1, 31, 25)\n\n\nDask graph\n40 chunks in 69 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (4)lat(lat)float6447.38 47.62 47.88 ... 54.62 54.88long_name :latitudestandard_name :latitudeunits :degrees_northaxis :Yarray([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875])lon(lon)float645.875 6.125 6.375 ... 14.62 14.88long_name :longitudestandard_name :longitudeunits :degrees_eastaxis :Xarray([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :5.75 0.25 0.0 47.25 0.0 0.25array(0)year(year)int642002 2003 2004 ... 2019 2020 2021array([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])Indexes: (3)latPandasIndexPandasIndex(Index([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875],\n      dtype='float64', name='lat'))lonPandasIndexPandasIndex(Index([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875],\n      dtype='float64', name='lon'))yearPandasIndexPandasIndex(Index([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021],\n      dtype='int64', name='year'))Attributes: (25)acknowledgment :https://doi.org/10.3390/rs11050517date_modified :2022-10-11 22:20:05.841847description :GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Datageospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 757 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :0.0001processing_steps :['Merging tif files', 'Converting water bodies and snow covered areas to NaN', 'Applying original scale factor', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.3390/rs11050517']reported_day :5.0source :['https://globalecology.unh.edu/data.html']standard_name :siftemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :W m^-2 sr^-1 um^-1\n\n\nPlot summer means for the SIF variable for the reference period 2000-2017, 2018 and the difference of them.\n\nchange_plot(ref_period = summer_mean_to_2017, data_2018 = summer_sif_mean_cube.sel(year=2018), changes = changes);\n\n\n            \n            \n\n\n\n\n\nPlot Sif Time series (mean over germany)\n\nplot_timeseries(cube_sif_mean, time_range= [\"2015-01-01\", \"2022-12-31\"])\n\n\n            \n            \n\n\nNameError: name 'cube_sif_mean' is not defined"
  },
  {
    "objectID": "main.html#sec-main_setup",
    "href": "main.html#sec-main_setup",
    "title": "1  Full Workflow",
    "section": "1.2 Setup",
    "text": "1.2 Setup\nFirst we setup the data path and use create_paths() from utils to create paths necessary throughout the analysis.\nSecond we use create_subset() to create a subset of the Earth System Data Cube, croped to:\n\nour AOI, the extent of germany\nthe time span where the SIF variable is avaialble\nto the relevant variables\n\nMore Information\n\n# Create a data directory\ndata_path = \"data\"\nos.makedirs(data_path, exist_ok=True)\n\n# Create paths to the data\ngermany_shp_path, corine_file_path, cube_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n# Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\ncube_subset = create_cube_subset()\n\nprint(cube_subset)\n\n\n            \n            \n\n\n&lt;xarray.Dataset&gt; Size: 30MB\nDimensions:                 (time: 920, lat: 31, lon: 37)\nCoordinates:\n  * lat                     (lat) float64 248B 47.38 47.62 47.88 ... 54.62 54.88\n  * lon                     (lon) float64 296B 5.875 6.125 6.375 ... 14.62 14.88\n  * time                    (time) datetime64[ns] 7kB 2002-01-05 ... 2021-12-31\n    spatial_ref             int64 8B 0\nData variables:\n    sif_gosif               (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    evaporation_era5        (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    precipitation_era5      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    radiation_era5          (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    air_temperature_2m      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    max_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    min_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1"
  },
  {
    "objectID": "main.html#basic-analysis-of-sif-data",
    "href": "main.html#basic-analysis-of-sif-data",
    "title": "1  Full Workflow",
    "section": "1.5 Basic Analysis of SIF-Data",
    "text": "1.5 Basic Analysis of SIF-Data\nThe following part will\n\nPlot and save the timeseries of the mean SIF using the masked cube\nPerform a change detection by calculating the summer mean for each year and the change for the year 2018 to the baseline up to 2017\nResulting Plots:\n\nTime-Series of SIF, considering only cells with more than 50% forest cover in 2000\nDifference SIF 2018 to mean of 2002 - 2017 over all cells in germany\n\n\n\n\nCode\n# Calculate the temporal changes in the variables \nsummer_sif_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n# Create the results directory\nos.makedirs(os.path.join(\"results\", \"figures\"), exist_ok=True)\n\n# Calculate the summer mean for each year and the change compared to the baseline up to 2017\nsummer_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n# Select only year 2018\nsummer_mean_2018 = summer_sif_mean_cube.sel(year=2018)\n\n\n\n            \n            \n\n\n\nShow the summer means cube.\n\n\nCode\nsummer_sif_mean_cube\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'sif_gosif' (year: 20, lat: 31, lon: 37)&gt; Size: 92kB\ndask.array&lt;stack, shape=(20, 31, 37), dtype=float32, chunksize=(1, 31, 25), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat          (lat) float64 248B 47.38 47.62 47.88 ... 54.38 54.62 54.88\n  * lon          (lon) float64 296B 5.875 6.125 6.375 ... 14.38 14.62 14.88\n    spatial_ref  int64 8B 0\n  * year         (year) int64 160B 2002 2003 2004 2005 ... 2018 2019 2020 2021\nAttributes: (12/25)\n    acknowledgment:             https://doi.org/10.3390/rs11050517\n    date_modified:              2022-10-11 22:20:05.841847\n    description:                GOSIF Solar-Induced Chlorophyll Fluorescence ...\n    geospatial_lat_max:         89.87499999999999\n    geospatial_lat_min:         -89.87500000000001\n    geospatial_lat_resolution:  0.25\n    ...                         ...\n    standard_name:              sif\n    temporal_resolution:        8D\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        2000-03-01T00:00:00.000000000\n    time_period:                8D\n    units:                      W m^-2 sr^-1 um^-1xarray.DataArray'sif_gosif'year: 20lat: 31lon: 37dask.array&lt;chunksize=(1, 31, 25), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n89.61 kiB\n3.03 kiB\n\n\nShape\n(20, 31, 37)\n(1, 31, 25)\n\n\nDask graph\n40 chunks in 69 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (4)lat(lat)float6447.38 47.62 47.88 ... 54.62 54.88long_name :latitudestandard_name :latitudeunits :degrees_northaxis :Yarray([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875])lon(lon)float645.875 6.125 6.375 ... 14.62 14.88long_name :longitudestandard_name :longitudeunits :degrees_eastaxis :Xarray([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :5.75 0.25 0.0 47.25 0.0 0.25array(0)year(year)int642002 2003 2004 ... 2019 2020 2021array([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])Indexes: (3)latPandasIndexPandasIndex(Index([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875],\n      dtype='float64', name='lat'))lonPandasIndexPandasIndex(Index([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875],\n      dtype='float64', name='lon'))yearPandasIndexPandasIndex(Index([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021],\n      dtype='int64', name='year'))Attributes: (25)acknowledgment :https://doi.org/10.3390/rs11050517date_modified :2022-10-11 22:20:05.841847description :GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Datageospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 757 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :0.0001processing_steps :['Merging tif files', 'Converting water bodies and snow covered areas to NaN', 'Applying original scale factor', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.3390/rs11050517']reported_day :5.0source :['https://globalecology.unh.edu/data.html']standard_name :siftemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :W m^-2 sr^-1 um^-1\n\n\n\nPlot summer means for the SIF variable for the reference period 2000-2017, 2018 and the difference of them.\n\n\nCode\nchange_plot(ref_period = summer_mean_to_2017, data_2018 = summer_mean_2018, changes = changes);\n\n\n\n            \n            \n\n\n\n\n\n\nPlot Sif Time series (mean over masked cells in germany)\n\n\nCode\n# Save plot of timeseries:\nplot_timeseries(cube_subset_mask, save_path = os.path.join(\"results\", \"figures\", \"timeseries_full.png\"))\nplot_timeseries(cube_subset_mask, time_range= [\"2015-01-01\", \"2022-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))"
  },
  {
    "objectID": "main.html#modelling-results",
    "href": "main.html#modelling-results",
    "title": "1  Full Workflow",
    "section": "1.6 Modelling results",
    "text": "1.6 Modelling results\n\n\nCode\n# Reading the results and evaluation from the file\nwith open(\"results/modelling/final/results_full_local_auto_l30.json\", 'r') as file:\n    results_dict = json.load(file)\n\n\n\n            \n            \n\n\n\n\nCode\nrmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\n\n# Find locations with highest and lowest RMSE\nhighest_rmse_location = max(rmse_values, key=rmse_values.get)\nlowest_rmse_location = min(rmse_values, key=rmse_values.get)\n\n\nprint(f'Location with highest RMSE: {highest_rmse_location}, RMSE: {rmse_values[highest_rmse_location]}')\nprint(f'Location with lowest RMSE: {lowest_rmse_location}, RMSE: {rmse_values[lowest_rmse_location]}')\n\n# Extracting true and predicted values for these locations\ntrue_values_high_rmse = results_dict[highest_rmse_location]['results']['true_values']\npredicted_values_high_rmse = results_dict[highest_rmse_location]['results']['predicted_values']\ntrue_values_low_rmse = results_dict[lowest_rmse_location]['results']['true_values']\npredicted_values_low_rmse = results_dict[lowest_rmse_location]['results']['predicted_values']\n\n# scores for the locations\nrmse_high = results_dict[highest_rmse_location]['evaluation']['rmse']\nmae_high = results_dict[highest_rmse_location]['evaluation']['mae']\nrmse_low = results_dict[lowest_rmse_location]['evaluation']['rmse']\nmae_low = results_dict[lowest_rmse_location]['evaluation']['mae']\n\n\n\n            \n            \n\n\nLocation with highest RMSE: (52.625, 10.375), RMSE: 0.030957287177443504\nLocation with lowest RMSE: (52.125, 13.875), RMSE: 0.012908714823424816\n\n\n\n\nCode\nplot_multiple_results(results_dict, keys_to_plot=[highest_rmse_location, lowest_rmse_location])\n\n\n\n            \n            \n\n\n\n\n\n\n\nCode\n# unction to plot true vs predicted values\ndef plot_forecast(true_values, predicted_values, title, rmse, mae):\n    plt.figure(figsize=(14, 7))\n    plt.plot(true_values, label='True Values', color='blue', linewidth=2)\n    plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--', linewidth=2)\n    plt.title(title, fontsize=16)\n    plt.xlabel('Time', fontsize=14)\n    plt.ylabel('SIF', fontsize=14)\n    plt.legend(fontsize=12)\n    plt.grid(True)\n    plt.tight_layout()\n\n    # add scores to each plot\n    plt.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=plt.gca().transAxes, color='red', fontsize=12)\n    plt.show()\n\n# Plot for location with highest RMSE\nplot_forecast(\n    [val[0] for val in true_values_high_rmse],\n    [val[0] for val in predicted_values_high_rmse],\n    f'Forecast vs True Values (Highest RMSE Location: {highest_rmse_location})', rmse_high, mae_high\n)\n\n# Plot for location with lowest RMSE\nplot_forecast(\n    [val[0] for val in true_values_low_rmse],\n    [val[0] for val in predicted_values_low_rmse],\n    f'Forecast vs True Values (Lowest RMSE Location: {lowest_rmse_location})', rmse_low, mae_low\n)\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n# Call the function with the example dictionary\nplot_forecasts_from_dict(results_dict)\n\n\n\n            \n            \n\n\nLocations with lowest RMSE: ['(52.125, 13.875)', '(49.375, 11.125)'], RMSEs: [0.012908714823424816, 0.013273913413286209]\nLocations with highest RMSE: ['(50.625, 8.125)', '(52.625, 10.375)'], RMSEs: [0.029641209170222282, 0.030957287177443504]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef subset_results_dict(results_dict):\n    subset_dict = {}\n    for key, value in results_dict.items():\n        subset_dict[key] = {\n            'best_params': value['best_params'],\n            'evaluation': value['evaluation']\n        }\n    return subset_dict\n\nres_sub = subset_results_dict(results_dict)\n\n\n\n            \n            \n\n\n\n\nCode\nimport json\nimport IPython.core.formatters\n\nclass JsonDumpTryingFormatter(\n    IPython.core.formatters.PlainTextFormatter\n):\n    def __call__(self, obj):\n        try:\n            return json.dumps(obj, indent=2)\n        except TypeError:\n            return super().__call__(obj)\n\n_ipy = IPython.get_ipython()\n_formatters = _ipy.display_formatter.formatters\n_formatters[\"text/plain\"] = JsonDumpTryingFormatter()\n\nres_sub[highest_rmse_location]\n\n\n\n            \n            \n\n\n{\n  \"best_params\": {\n    \"activation\": \"tanh\",\n    \"batch_size\": 25,\n    \"dropout_rate\": 0.2,\n    \"epochs\": 100,\n    \"learning_rate\": 0.0001,\n    \"num_lstm_layers\": 2,\n    \"units_lstm\": 128\n  },\n  \"evaluation\": {\n    \"mae\": 0.020632067695260048,\n    \"rmse\": 0.030957287177443504\n  }\n}"
  },
  {
    "objectID": "main_copy.html#packages",
    "href": "main_copy.html#packages",
    "title": "1  Full Workflow",
    "section": "1.1 Packages",
    "text": "1.1 Packages\n\n\nCode\nimport sys\nimport os\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rio\nimport json\nimport numpy as np\nfrom matplotlib.colors import ListedColormap, Normalize\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nfrom matplotlib.gridspec import GridSpec\n\n# Add the parent directory to sys.path\nsys.path.append(os.path.abspath(os.path.join('scripts')))\n\n# from scripts.config import *\nfrom scripts.utils import create_cube_subset, create_paths\nfrom scripts.s03_base_analysis import base_analysis, change_plot, plot_timeseries\nfrom scripts.config import variables\nfrom scripts.modelling_functions import plot_multiple_results;\nimport matplotlib.pyplot as plt\n\n# Write data to disk set to False\nwrite_data = False\n\n\ndef plot_multiple_results(results_dict, time_index = None):\n    num_plots = len(results_dict)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(17, num_rows * 7))\n    if num_rows * num_cols &gt; 1:\n        axes = axes.flatten()  # Flatten the axes array for easier iteration\n    else:\n        axes = [axes]  # Ensure axes is iterable\n\n    for i, (lat_lon, data_dict) in enumerate(results_dict.items()):\n        ax = axes[i]\n\n        testY = data_dict['results']['true_values']\n        forecasts = data_dict['results']['predicted_values']\n        mae = data_dict['evaluation']['mae']\n        rmse = data_dict['evaluation']['rmse']\n\n        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n\n        # Generate a time index from the length of the testY data\n        # time_index = range(len(testY))\n\n        ax.plot(time_index, testY, label=\"Actual\")\n        ax.plot(time_index, forecasts, label=\"Predicted\")\n        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Value\")\n        ax.legend()\n        ax.grid(True)\n\n        # Add MSE to the corner\n        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=ax.transAxes, color='red', fontsize=12)\n\n    # Adjust the layout to prevent overlap and make sure all plots are visible\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_forecasts_from_dict(results_dict):\n    \"\"\"\n    This function takes a dictionary containing model results and creates plots for the two locations\n    with the highest and two locations with the lowest RMSE values.\n\n    Args:\n    results_dict (dict): Dictionary containing model results with true and predicted values.\n\n    Returns:\n    None\n    \"\"\"\n    # Extract RMSEs and corresponding locations\n    rmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\n\n    # Sort locations by RMSE\n    sorted_locations = sorted(rmse_values, key=rmse_values.get)\n\n    # Find two locations with highest and two with lowest RMSE\n    lowest_rmse_locations = sorted_locations[:2]\n    highest_rmse_locations = sorted_locations[-2:]\n\n    print(f'Locations with lowest RMSE: {lowest_rmse_locations}, RMSEs: {[rmse_values[loc] for loc in lowest_rmse_locations]}')\n    print(f'Locations with highest RMSE: {highest_rmse_locations}, RMSEs: {[rmse_values[loc] for loc in highest_rmse_locations]}')\n\n    # Function to plot true vs predicted values\n    def plot_forecast(true_values, predicted_values, title):\n        plt.figure(figsize=(14, 7))\n        plt.plot(true_values, label='True Values', color='blue', linewidth=2)\n        plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--', linewidth=2)\n        plt.title(title, fontsize=16)\n        plt.xlabel('Time', fontsize=14)\n        plt.ylabel('SIF', fontsize=14)\n        plt.legend(fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    # Plot for locations with lowest RMSE\n    for loc in lowest_rmse_locations:\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Forecast vs True Values (Lowest RMSE Location: {loc})'\n        )\n\n    # Plot for locations with highest RMSE\n    for loc in highest_rmse_locations:\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Forecast vs True Values (Highest RMSE Location: {loc})'\n        )"
  },
  {
    "objectID": "main_copy.html#setup",
    "href": "main_copy.html#setup",
    "title": "1  Full Workflow",
    "section": "1.2 Setup",
    "text": "1.2 Setup\nFirst we setup the data path and use create_paths() from utils to create paths necessary throughout the analysis.\nSecond we use create_subset() to create a subset of the Earth System Data Cube, croped to:\n\nour AOI, the extent of germany\nthe time span where the SIF variable is avaialble\nto the relevant variables\n\nMore Information\n\n\nCode\n# Create a data directory\ndata_path = \"data\"\nos.makedirs(data_path, exist_ok=True)\n\n# Create paths to the data\ngermany_shp_path, corine_file_path, cube_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n# Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\ncube_subset = create_cube_subset()\n\n# remove attributes\ncube_subset.attrs = {}\n\nprint(cube_subset)\n\n\n\n            \n            \n\n\n&lt;xarray.Dataset&gt; Size: 30MB\nDimensions:                 (time: 920, lat: 31, lon: 37)\nCoordinates:\n  * lat                     (lat) float64 248B 47.38 47.62 47.88 ... 54.62 54.88\n  * lon                     (lon) float64 296B 5.875 6.125 6.375 ... 14.62 14.88\n  * time                    (time) datetime64[ns] 7kB 2002-01-05 ... 2021-12-31\n    spatial_ref             int64 8B 0\nData variables:\n    sif_gosif               (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    evaporation_era5        (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    precipitation_era5      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    radiation_era5          (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    air_temperature_2m      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    max_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    min_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;"
  },
  {
    "objectID": "main_copy.html#downloading-auxillary-data",
    "href": "main_copy.html#downloading-auxillary-data",
    "title": "1  Full Workflow",
    "section": "1.3 Downloading auxillary data",
    "text": "1.3 Downloading auxillary data\nThe first part of the analysis is to download and preprocess all the necassaty auxillary data:\n\ndownload german border shapefile\ndownload and preprocess Corine data (for the forest mask) and use the border data to define the AOI\ncreate sif sample tif for spatial resolution and transform (used later to resample corine data)\n\nMore Information\n\n\nCode\n# Load the germany border shapefile\ngermany_gpd = gpd.read_file(germany_shp_path)\n\n# Load the CORINE land cover data\ncorine_raster = rio.open_rasterio(corine_file_path)\n\n# set 0 to na\ncorine_raster = corine_raster.where(corine_raster != 0)\n\ncmap = ListedColormap(['#66a61e', '#1b9e77', '#7570b3'])\nvmin = 311; vmax = 313\nlegend_labels=['Decidious Forest', 'Coniferous Forest', 'Mixed Forest']\n\n\n# Plot the CORINE land cover data with the Germany border overlayed\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot Germany with gray fill and black boundary\ngermany_gpd.plot(ax=ax, color='lightgray', edgecolor='black', alpha=1, linewidth=1.5)\n\n# Plot rasterio dataset\ncorine_raster.plot(ax=ax, alpha=1, vmin=vmin, vmax=vmax, cmap= cmap, add_colorbar=False)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm=Normalize(vmin=vmin, vmax=vmax), cmap=cmap),\n                    ax=ax, fraction=0.02, pad=0.04)\n\nticks = np.linspace(vmin, vmax, len(legend_labels))\ncbar.set_ticks(ticks)\ncbar.ax.set_yticklabels(legend_labels)\n\nplt.show()"
  },
  {
    "objectID": "main_copy.html#preprocessing-the-cube",
    "href": "main_copy.html#preprocessing-the-cube",
    "title": "1  Full Workflow",
    "section": "1.4 Preprocessing the cube",
    "text": "1.4 Preprocessing the cube\nNext we want to preprocess our data cube.\nThis includes:\n\ncroping the cube with the german border shape\nmasking the cube with the corine forest cover:\n\nCalculate the forest percentage of the Corine landcover data over the cube grid\nadd the forest percentages to the cube\nadd a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\nuse the layer to mask the cube\n\n\nThe results are cube_subset_crop which is croped with the germany border and cube_subset_mask, masked with the forest cover.\nMore Information\n\nVisualization of Corine Data Processing.\n\n\nCode\n# Load cube subset croped and cube subset mask\ncube_subset_crop = xr.open_dataset(cube_crop_path)\ncube_subset_mask = xr.open_dataset(cube_crop_mask_path)\n\n# Load the forest percentages raster\nforest_percentages = rio.open_rasterio(cube_crop_path.replace(\".nc\", \"_percentages.tif\"))\n\n# Create a grid spec with extra space at the bottom\nfig = plt.figure(figsize=(14, 8))\ngs = GridSpec(2, 3, height_ratios=[1, 0.03], hspace=0.15)\n\n# Create subplots in the grid spec\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[0, 2])\n\n# Plot rasterio dataset\ncorine_raster.plot(ax=ax0, alpha=1, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\nax0.set_title(\"Corine Forest Cover data\")\nax0.set_ylabel(\"Latitude\")\nax0.set_xlabel(\"Longitude\")\n\n# Plot the forest percentages raster\nforest_im = forest_percentages.plot(ax=ax1, alpha=1, add_colorbar=False)\nax1.set_title(\"Resampled to percentages\")\nax1.set_ylabel(\"\")\nax1.set_xlabel(\"Longitude\")\n\n# Add a colorbar at the bottom for the forest percentages raster\ncax = fig.add_subplot(gs[1, 1])  # Add a subplot in the second row, middle column\ncbar = fig.colorbar(forest_im, cax=cax, orientation='horizontal')\ncbar.set_label(\"Forest cover in %\")\n\n# Plot the forest cover 50% threshold\ncube_subset_crop.forest_cover_50.plot(ax=ax2, vmin=0, add_colorbar=False)\nax2.set_title(\"50% Threshold\")\nax2.set_ylabel(\"\")\nax2.set_xlabel(\"Longitude\")\n\n# Main title for the entire figure\nfig.suptitle(\"Corine Data Processing\", fontsize=16)\n\n# Adjust the layout to ensure everything fits well\nplt.subplots_adjust(left=0.05, right=0.95, top=0.90, bottom=0)\nplt.show()\n\n\n\n\n            \n            \n\n\n\n\n\n\nShowing croped and masked + croped cube\n\n\nCode\n\n\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n\n# Plot the croped dataset\nim = cube_subset_crop.sif_gosif.isel(time=0).plot(ax=axes[0], add_colorbar=False, vmin = -0.025, vmax = 0.025, cmap='RdBu_r')\naxes[0].set_title(\"Cube Subset Cropped\")\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\n\n# Plot the masked dataset\ncube_subset_mask.sif_gosif.isel(time=0).plot(ax=axes[1], add_colorbar=True, vmin = -0.025, vmax = 0.025, cmap='RdBu_r')\naxes[1].set_title(\"Cube Subset Masked\")\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"\")\n\n# Add germany border\ngermany_gpd.plot(ax=axes[0], edgecolor='black', alpha=1, linewidth=1.5, facecolor = \"none\")\ngermany_gpd.plot(ax=axes[1], edgecolor='black', alpha=1, linewidth=1.5, facecolor = \"none\" )\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust rect to make room for the colorbar\nplt.show()"
  },
  {
    "objectID": "main_copy.html#basic-analysis-of-sif-data",
    "href": "main_copy.html#basic-analysis-of-sif-data",
    "title": "1  Full Workflow",
    "section": "1.5 Basic Analysis of SIF-Data",
    "text": "1.5 Basic Analysis of SIF-Data\nThe following part will\n\nPerform a change detection by calculating the summer mean for each year and the change for the year 2018 to the baseline up to 2017\nResulting Plots:\n\nTime-Series of SIF, considering only cells with more than 50% forest cover in 2000\nDifference SIF 2018 to mean of 2002 - 2017 over all cells in germany\n\n\n\nPlot summer means for the SIF variable for the reference period 2000-2017, 2018 and the difference of them.\n\n\nCode\n# Calculate the temporal changes in the variables \nsummer_sif_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n\n# Calculate the summer mean for each year and the change compared to the baseline up to 2017\nsummer_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n# Select only year 2018\nsummer_mean_2018 = summer_sif_mean_cube.sel(year=2018)\n\nchange_plot(ref_period = summer_mean_to_2017, data_2018 = summer_mean_2018, changes = changes);\n\n\n\n            \n            \n\n\n\n\n\n\nPlot Sif Time series (mean over masked cells in germany)\n\n\nCode\n# Save plot of timeseries:\nplot_timeseries(cube_subset_mask, save_path = os.path.join(\"results\", \"figures\", \"timeseries_full.png\"))\n# plot_timeseries(cube_subset_mask, time_range= [\"2015-01-01\", \"2022-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))"
  },
  {
    "objectID": "main_copy.html#modelling-results",
    "href": "main_copy.html#modelling-results",
    "title": "1  Full Workflow",
    "section": "1.7 Modelling results",
    "text": "1.7 Modelling results\nBased on the prelimary results a local model with a lookback of 30 time steps (30 * 8 days = 240 days) was chosen.\nApart from that the modelling setup was the same, except that the number of hyperparameters was reduced based on the prelimary results.\n\n\nCode\n# Reading the results and evaluation from the file\nwith open(\"results/modelling/final/results_full_local_auto_l30.json\", 'r') as file:\n    results_dict = json.load(file)\n\nrmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\n\n# Find locations with highest and lowest RMSE\nhighest_rmse_location = max(rmse_values, key=rmse_values.get)\nlowest_rmse_location = min(rmse_values, key=rmse_values.get)\n\n\nprint(f'Location with highest RMSE: {highest_rmse_location}, RMSE: {rmse_values[highest_rmse_location]:.3f}')\nprint(f'Location with lowest RMSE: {lowest_rmse_location}, RMSE: {rmse_values[lowest_rmse_location]:.3f}')\n\n# Extracting true and predicted values for these locations\ntrue_values_high_rmse = results_dict[highest_rmse_location]['results']['true_values']\npredicted_values_high_rmse = results_dict[highest_rmse_location]['results']['predicted_values']\ntrue_values_low_rmse = results_dict[lowest_rmse_location]['results']['true_values']\npredicted_values_low_rmse = results_dict[lowest_rmse_location]['results']['predicted_values']\n\n# scores for the locations\nrmse_high = results_dict[highest_rmse_location]['evaluation']['rmse']\nmae_high = results_dict[highest_rmse_location]['evaluation']['mae']\nrmse_low = results_dict[lowest_rmse_location]['evaluation']['rmse']\nmae_low = results_dict[lowest_rmse_location]['evaluation']['mae']\n\n# mean errors\nmean_rmse = np.mean([data['evaluation']['rmse'] for data in results_dict.values()])\nmean_mae = np.mean([data['evaluation']['mae'] for data in results_dict.values()])\n\nprint(f'Mean MAE: {mean_mae:.3f}, Mean RMSE: {mean_rmse:.3f}')\n\n# print the mean errors\n\n\n\n            \n            \n\n\nLocation with highest RMSE: (52.625, 10.375), RMSE: 0.031\nLocation with lowest RMSE: (52.125, 13.875), RMSE: 0.013\nMean MAE: 0.014, Mean RMSE: 0.020\n\n\n\n\nCode\n\ndef plot_multiple_results(results_dict, keys_to_plot=None, time_index=None):\n    # If no specific keys are provided, plot all entries in the dictionary\n    if keys_to_plot is None:\n        keys_to_plot = list(results_dict.keys())\n    \n    num_plots = len(keys_to_plot)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(17, num_rows * 7))\n    if num_rows * num_cols &gt; 1:\n        axes = axes.flatten()  # Flatten the axes array for easier iteration\n    else:\n        axes = [axes]  # Ensure axes is iterable\n\n    for i, lat_lon in enumerate(keys_to_plot):\n        ax = axes[i]\n        data_dict = results_dict[lat_lon]\n        testY = data_dict['results']['true_values']\n        forecasts = data_dict['results']['predicted_values']\n        mae = data_dict['evaluation']['mae']\n        rmse = data_dict['evaluation']['rmse']\n\n        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n\n        # Generate a time index from the length of the testY data if not provided\n        if time_index is None:\n            time_index = range(len(testY))\n\n        ax.plot(time_index, testY, label=\"Actual\")\n        ax.plot(time_index, forecasts, label=\"Predicted\")\n        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Value\")\n        ax.legend()\n        ax.grid(True)\n\n        # Add RMSE and MAE to the corner\n        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=ax.transAxes, color='red', fontsize=12)\n\n    # Adjust the layout to prevent overlap and make sure all plots are visible\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_multiple_results(results_dict, keys_to_plot=[highest_rmse_location, lowest_rmse_location])\n\n\n\n            \n            \n\n\n\n\n\n\n\nCode\n# unction to plot true vs predicted values\ndef plot_forecast(true_values, predicted_values, title, rmse, mae):\n    plt.figure(figsize=(14, 7))\n    plt.plot(true_values, label='True Values', color='blue', linewidth=2)\n    plt.plot(predicted_values, label='Predicted Values', color='red', linestyle='--', linewidth=2)\n    plt.title(title, fontsize=16)\n    plt.xlabel('Time', fontsize=14)\n    plt.ylabel('SIF', fontsize=14)\n    plt.legend(fontsize=12)\n    plt.grid(True)\n    plt.tight_layout()\n\n    # add scores to each plot\n    plt.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n                verticalalignment='bottom', horizontalalignment='right', \n                transform=plt.gca().transAxes, color='red', fontsize=12)\n    plt.show()\n\n# Plot for location with highest RMSE\nplot_forecast(\n    [val[0] for val in true_values_high_rmse],\n    [val[0] for val in predicted_values_high_rmse],\n    f'Forecast vs True Values (Highest RMSE Location: {highest_rmse_location})', rmse_high, mae_high\n)\n\n# Plot for location with lowest RMSE\nplot_forecast(\n    [val[0] for val in true_values_low_rmse],\n    [val[0] for val in predicted_values_low_rmse],\n    f'Forecast vs True Values (Lowest RMSE Location: {lowest_rmse_location})', rmse_low, mae_low\n)\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n# Call the function with the example dictionary\nplot_forecasts_from_dict(results_dict)\n\n\n\n            \n            \n\n\nLocations with lowest RMSE: ['(52.125, 13.875)', '(49.375, 11.125)'], RMSEs: [0.012908714823424816, 0.013273913413286209]\nLocations with highest RMSE: ['(50.625, 8.125)', '(52.625, 10.375)'], RMSEs: [0.029641209170222282, 0.030957287177443504]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef subset_results_dict(results_dict):\n    subset_dict = {}\n    for key, value in results_dict.items():\n        subset_dict[key] = {\n            'best_params': value['best_params'],\n            'evaluation': value['evaluation']\n        }\n    return subset_dict\n\nres_sub = subset_results_dict(results_dict)\n\n\n\n            \n            \n\n\n\n\nCode\nimport json\nimport IPython.core.formatters\n\nclass JsonDumpTryingFormatter(\n    IPython.core.formatters.PlainTextFormatter\n):\n    def __call__(self, obj):\n        try:\n            return json.dumps(obj, indent=2)\n        except TypeError:\n            return super().__call__(obj)\n\n_ipy = IPython.get_ipython()\n_formatters = _ipy.display_formatter.formatters\n_formatters[\"text/plain\"] = JsonDumpTryingFormatter()\n\nres_sub[highest_rmse_location]\n\n\n\n            \n            \n\n\n{\n  \"best_params\": {\n    \"activation\": \"tanh\",\n    \"batch_size\": 25,\n    \"dropout_rate\": 0.2,\n    \"epochs\": 100,\n    \"learning_rate\": 0.0001,\n    \"num_lstm_layers\": 2,\n    \"units_lstm\": 128\n  },\n  \"evaluation\": {\n    \"mae\": 0.020632067695260048,\n    \"rmse\": 0.030957287177443504\n  }\n}"
  },
  {
    "objectID": "main_copy.html#prelimnary-modelling",
    "href": "main_copy.html#prelimnary-modelling",
    "title": "1  Full Workflow",
    "section": "1.6 Prelimnary Modelling",
    "text": "1.6 Prelimnary Modelling\nTo determine what is the appropriate model structure, we tested for different approaches.\nFour general model setups were tested, which are differing whether they are global or local models and whether they have an auto regressive component or not, meaning whether they encoperate a shifted version of target variable as a predictor.\nTo find the best hyperparameters 3 look back periods were tested and a grid search cross-validation was done.\nMore Information\n\nTable showing mae and rmse model performances\n\n\nCode\nimport os\nimport json\nfrom collections import defaultdict\n\n# Function to read JSON files and extract relevant data\ndef read_json_files(base_dir):\n    # Initialize a dictionary to store the extracted data\n    results = {\n        \"results_l15\": {\"auto\": [], \"noauto\": []},\n        \"results_l30\": {\"auto\": [], \"noauto\": [], \"global_auto\": [], \"global_noauto\": []},\n        \"results_l45\": {\"auto\": [], \"noauto\": []},\n    }\n\n    # Traverse through the base directory and subdirectories\n    for lookback_dir in [\"results_l15\", \"results_l30\", \"results_l45\"]:\n        lookback_path = os.path.join(base_dir, lookback_dir)\n        if not os.path.isdir(lookback_path):\n            continue\n\n        # Identify files containing \"auto\", \"noauto\", or \"global\" in their names\n        for filename in os.listdir(lookback_path):\n            if \"global\" in filename:\n                continue\n\n            if \"noauto\" in filename:\n                key = \"noauto\"\n            elif \"auto\" in filename:\n                key = \"auto\"\n            else:\n                continue\n\n            file_path = os.path.join(lookback_path, filename)\n\n            # Read and parse the JSON file\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n\n                # Extract relevant data and store it in the results dictionary\n                for location, details in data.items():\n                    result_entry = {\n                        \"location\": location,\n                        \"best_params\": details.get(\"best_params\", {}),\n                        \"look_back\": details.get(\"look_back\", 0),\n                        \"evaluation\": details.get(\"evaluation\", {}),\n                    }\n                    results[lookback_dir][key].append(result_entry)\n\n    return results\n\n# Function to calculate mean performance metrics\ndef calculate_mean_performance(results):\n    # Initialize dictionaries to store the sum and count of performance metrics\n    performance_sums = defaultdict(lambda: {\"mae\": 0, \"rmse\": 0, \"count\": 0})\n    mean_performance = {}\n\n    # Sum the performance metrics and count the number of entries\n    for lookback in results:\n        for auto_type in results[lookback]:\n            for entry in results[lookback][auto_type]:\n                evaluation = entry[\"evaluation\"]\n                if \"mae\" in evaluation and \"rmse\" in evaluation:\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"mae\"] += evaluation[\"mae\"]\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"rmse\"] += evaluation[\"rmse\"]\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"count\"] += 1\n\n    # Calculate the mean performance metrics\n    for key, sums in performance_sums.items():\n        if sums[\"count\"] &gt; 0:\n            mean_performance[key] = {\n                \"mean_mae\": np.round(sums[\"mae\"] / sums[\"count\"], 4),\n                \"mean_rmse\": np.round(sums[\"rmse\"] / sums[\"count\"], 4)\n            }\n        else:\n            mean_performance[key] = {\"mean_mae\": None, \"mean_rmse\": None}\n\n    return mean_performance\n\n# Base directory where the results folders are located\nbase_dir = os.path.join(\"results\", \"modelling\")\n\n# Read JSON files and extract data\nresults_data = read_json_files(base_dir)\n\n# Calculate mean performance metrics\nmean_performance_data = calculate_mean_performance(results_data)\n\n\ngloba_auto_path = os.path.join(base_dir, \"results_l30\", \"results_global_auto_l30.json\")\nglobal_noauto_path = os.path.join(base_dir, \"results_l30\", \"results_global_noauto_l30.json\")\n\ndef get_mae_rmse_from_json(json_path):\n    with open(json_path, 'r') as file:\n        data = json.load(file)\n    return np.round(data[\"evaluation\"][\"mae\"],3), np.round(data[\"evaluation\"][\"rmse\"],3)\n\nglobal_results = get_mae_rmse_from_json(globa_auto_path), get_mae_rmse_from_json(global_noauto_path)\n\nimport pandas as pd\n# Convert the dictionary to a DataFrame\ndata = []\nfor key, values in mean_performance_data.items():\n    lookback, auto_type = key.rsplit('_', 1)\n    data.append({\n        'model' : 'local',\n        'lookback': lookback[-2:],\n        'type': auto_type,\n        'mean_mae': values['mean_mae'],\n        'mean_rmse': values['mean_rmse']\n    })\n\n# Add global results to the data\ndata.append({\n    'model' : 'global',\n    'lookback': '30',\n    'type': 'auto',\n    'mean_mae': global_results[0][0],\n    'mean_rmse': global_results[0][1]\n})\ndata.append({\n    'model' : 'global',\n    'lookback': '30',\n    'type': 'noauto',\n    'mean_mae': global_results[1][0],\n    'mean_rmse': global_results[1][1]\n})\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n\n\n            \n            \n\n\n    model lookback    type  mean_mae  mean_rmse\n0   local       15    auto    0.0150     0.0217\n1   local       15  noauto    0.0161     0.0228\n2   local       30    auto    0.0144     0.0212\n3   local       30  noauto    0.0142     0.0210\n4   local       45    auto    0.0149     0.0218\n5   local       45  noauto    0.0145     0.0211\n6  global       30    auto    0.0330     0.0450\n7  global       30  noauto    0.0330     0.0500\n\n\n\nComparison between local models.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import cm\n\ndef plot_performance_data(mean_performance_data):\n    # Define colors and markers\n    colors = {\n        'auto': \"blue\",\n        'noauto': \"orange\"\n    }\n    shades = {\n        'results_l15': 0.4,\n        'results_l30': 0.7,\n        'results_l45': 1\n    }\n    markers = ['o', 's', 'D', '^']\n\n    # Create a new figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot data for each key in the mean performance data\n    for idx, (key, performance) in enumerate(mean_performance_data.items()):\n        lookback, auto_type = key.rsplit('_', 1)\n        color = colors[auto_type]\n        shade = shades[lookback]\n\n        plt.scatter(performance['mean_mae'], performance['mean_rmse'], \n                    color=color, alpha=shade, label=key, s=100)\n\n\n\n    # Add labels and title\n    plt.xlabel('Mean MAE')\n    plt.ylabel('Mean RMSE')\n    plt.title('Model Performance Comparison local Models', fontsize=16)\n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    plt.grid(True)\n    plt.tight_layout()\n\n    # Show plot\n    plt.show()\n\n# Call the function to plot the data\nplot_performance_data(mean_performance_data)"
  }
]