[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "",
    "text": "Outline\nThis is the quarto book for the project “Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning”. This project was created as part of two course projects in the “Earth System Data Science and Remote Sensing” Master at Leipzig University. The courses are Scientific Writing by Prof. Dr. Miguel Mahecha and Spatio-temporal Data by Dr. Guido Kraemer. This quarto book contains all code used for the analysis, shows the main results and briefly discusses them, as well as a Data Management Plan. The project is fully reproducible and all data is openly available. The project can be found at this Gitlab repository."
  },
  {
    "objectID": "quarto_book/intro.html",
    "href": "quarto_book/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCode\n1+1\n\n\n2\n\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/references.html",
    "href": "quarto_book/references.html",
    "title": "References",
    "section": "",
    "text": "Hersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András\nHorányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The\nERA5 Global Reanalysis.” Quarterly Journal of the Royal\nMeteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product\nof Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and\nReanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "quarto_book/intro.html#section",
    "href": "quarto_book/intro.html#section",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 ",
    "text": "1.1 \n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-initalize-gee",
    "href": "quarto_book/intro.html#function-to-initalize-gee",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Function to initalize GEE",
    "text": "1.1 Function to initalize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data-for-year-2000",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to download the Corine LC 100m data for year 2000",
    "text": "1.2 Function to download the Corine LC 100m data for year 2000\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "href": "quarto_book/intro.html#function-to-download-the-corine-lc-100m-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function to download the Corine LC 100m data",
    "text": "1.3 Function to download the Corine LC 100m data\nThis function will download the Corine LC 100m data for the year 2000 from google earth engine. Only forest classes are retained: - 311: Decidous forest - 312: Coniferous forest - 313: Mixed forest\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function",
    "href": "quarto_book/intro.html#function",
    "title": "1  Downloading Auxillary data",
    "section": "1.3 Function",
    "text": "1.3 Function\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#function-to-download-german-border-data",
    "href": "quarto_book/intro.html#function-to-download-german-border-data",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Function to Download German border data",
    "text": "1.2 Function to Download German border data\nThis function will download a shapefile delineating the german border, from google earth engine.\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry"
  },
  {
    "objectID": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "href": "quarto_book/intro.html#function-to-create-a-sif-sample-tif",
    "title": "1  Downloading Auxillary data",
    "section": "1.4 Function to create a sif sample tif",
    "text": "1.4 Function to create a sif sample tif\nThis tif will late be used as a reference for the cube spatial grid and transform\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_book/intro.html#compiling-all-functions-into-one",
    "href": "quarto_book/intro.html#compiling-all-functions-into-one",
    "title": "1  Downloading Auxillary data",
    "section": "1.5 Compiling all functions into one",
    "text": "1.5 Compiling all functions into one\nThis function\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif\n    create_sif_sample(out_path = tif_sample_path, cube_subset= cube_subset, write=download)"
  },
  {
    "objectID": "quarto_book/intro.html#run-the-script-if-its-called",
    "href": "quarto_book/intro.html#run-the-script-if-its-called",
    "title": "1  Downloading Auxillary data",
    "section": "1.2 Run the script if its called",
    "text": "1.2 Run the script if its called\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "quarto_book/intro.html#packages-and-functions",
    "href": "quarto_book/intro.html#packages-and-functions",
    "title": "1  Downloading Auxillary data",
    "section": "1.1 Packages and Functions",
    "text": "1.1 Packages and Functions\nThis code stores 4 Functions. - 1. to initalize Google Earth Engine - 2. to download a shapefile of the German border - 3. to download Corine Landcover data from Google Earth Engine - 4. to create a sample tif of the - 5. A function that loads all\nAdditionaly\n# scripts/load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Main workflow function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "href": "quarto_book/01_load_aux_data.html#packages-and-functions",
    "title": "2  Downloading Auxillary data",
    "section": "2.1 Packages and Functions",
    "text": "2.1 Packages and Functions\nThis code stores 5 Functions:\n\nto initialize Google Earth Engine\nto download a shapefile of the German border\nto download Corine Landcover data from Google Earth Engine\nto create a sample tif\nA function that wraps all other functions\n\nAdditionaly two functions are loaded in the beginning from utils.py\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\n# scripts/01_load_aux_data.py\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n\nfrom utils import create_paths\nfrom utils import create_cube_subset\n\n# load custom function from utils.py\n\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n    print(\"Downloading Corine data\")\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "href": "quarto_book/01_load_aux_data.html#run-the-script-if-its-called",
    "title": "2  Downloading Auxillary data",
    "section": "2.2 Run the script if its called",
    "text": "2.2 Run the script if its called\n\n\n\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#run-the-functions",
    "href": "quarto_book/01_load_aux_data.html#run-the-functions",
    "title": "2  Loading auxillary data",
    "section": "2.3 Run the functions",
    "text": "2.3 Run the functions\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "href": "quarto_book/02_cube_preprocessing.html#packages-and-functions",
    "title": "3  Preprocessing ESDC",
    "section": "3.1 Packages and Functions",
    "text": "3.1 Packages and Functions\n\ncalculate_forest_percentage: Computes the percentage of forest cover within a specified window of land cover data by identifying pixels that match predefined forest classes.\nresample_corine_to_sif: Resamples CORINE land cover data to match the resolution and dimensions of a sample SIF raster, calculating forest cover percentages for each resampled cell and returning a flipped array of these percentages.\ncube_preprocess: Clips a given data cube to the borders of Germany, calculates forest cover percentages over the grid, adds this data to the cube, creates a binary forest cover layer, and optionally writes the processed data to disk.\n\nimport xarray as xr\nimport rioxarray as rio\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport geopandas as gpd\nfrom utils import create_paths, create_cube_subset\n\n# Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n    \"\"\"\n    Calculate the percentage of forest cover in a specified window.\n\n    Parameters:\n    lc_window (Window): The window of the land cover data to analyze.\n    lc_data (ndarray): The land cover data array.\n    forest_classes (list): List of land cover classes considered as forest.\n\n    Returns:\n    float: Percentage of forest cover in the specified window.\n    \"\"\"\n    forest_mask = np.isin(\n        lc_data[lc_window.row_off:lc_window.row_off + lc_window.height,\n                lc_window.col_off:lc_window.col_off + lc_window.width],\n        forest_classes\n    )\n\n    total_pixels = forest_mask.size\n    forest_pixels = np.sum(forest_mask)\n    percentage = (forest_pixels / total_pixels) * 100\n\n    return percentage\n\n# Function to calculate the forest percentages of the CORINE land cover data over the cube grid\ndef resample_corine_to_sif(corine_file_path, sample_path):\n    \"\"\"\n    Resample CORINE land cover data to match the resolution and dimensions of a sample SIF raster,\n    and calculate forest cover percentages for each resampled cell.\n\n    Parameters:\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n\n    Returns:\n    ndarray: Array of forest cover percentages for each resampled cell.\n    \"\"\"\n    # Open the land cover raster\n    with rasterio.open(corine_file_path) as src_lc:\n        lc_data = src_lc.read()\n        lc_transform = src_lc.transform\n\n    # Open the sample SIF raster\n    with rasterio.open(sample_path) as src_sif:\n        sif_transform = src_sif.transform\n        sif_meta = src_sif.meta\n\n    # Determine the new shape and transform for the resampled raster\n    new_height = sif_meta['height']\n    new_width = sif_meta['width']\n\n    # Initialize the new resampled data array\n    resampled_forest_percentage = np.zeros((new_height, new_width), dtype=np.float32)\n\n    # Define forest classes\n    forest_classes = [311, 312, 313]\n\n    # Calculate the window size in the original land cover data\n    window_height = int(abs(sif_transform[4] / lc_transform[4]))\n    window_width = int(abs(sif_transform[0] / lc_transform[0]))\n\n    # Loop through each cell in the SIF raster resolution\n    for i in range(new_height):\n        for j in range(new_width):\n            # Define the window in the land cover data\n            window = Window(col_off=j*window_width, row_off=i*window_height, width=window_width, height=window_height)\n            \n            # Calculate the forest percentage in the window\n            forest_percentage = calculate_forest_percentage(window, lc_data.squeeze(), forest_classes)\n            \n            # Assign the percentage to the resampled data array\n            resampled_forest_percentage[i, j] = forest_percentage\n\n    resampled_forest_percentage_flip = np.flipud(resampled_forest_percentage)\n\n    return resampled_forest_percentage_flip\n\ndef cube_preprocess(cube_subset, germany_gpd, corine_file_path, sample_path, out_path_crop, out_path_mask, all_touched=True, write=True):\n    \"\"\"\n    Preprocess the data cube by clipping to Germany's border, calculating forest cover percentages,\n    and adding this data to the cube. Optionally write the processed data to disk.\n\n    Parameters:\n    cube_subset (xarray.Dataset): The data cube subset to preprocess.\n    germany_gpd (GeoDataFrame): GeoDataFrame containing Germany's borders.\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n    out_path_crop (str): Path to save the cropped data cube.\n    out_path_mask (str): Path to save the masked data cube.\n    all_touched (bool): Whether to include all pixels touched by the geometry. Defaults to True.\n    write (bool): Whether to write the output to disk. Defaults to True.\n\n    Returns:\n    xarray.Dataset: The processed data cube subset.\n    \"\"\"\n    print(\"Preprocessing cube\")\n\n    # Clip the xarray dataset using the Germany geometry\n    print(\"Clipping cube to Germany border\")\n    cube_subset_crop = cube_subset.rio.clip(\n        germany_gpd.geometry.values,\n        germany_gpd.crs,\n        drop=False, \n        all_touched=all_touched\n    )\n    \n    # Calculate forest cover percentage over cube grid\n    print(\"Calculate forest cover percentage over cube grid\")\n    resampled_forest_percentages = resample_corine_to_sif(corine_file_path, sample_path)\n\n    # Setup the dimensions for the resampled forest percentage\n    dims = ('lat', 'lon')  \n\n    # Add the resampled forest cover to the cube\n    cube_subset_crop['forest_cover'] = xr.DataArray(\n        resampled_forest_percentages, dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # Add a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\n    cube_subset_crop['forest_cover_50'] = xr.DataArray(\n        (resampled_forest_percentages &gt;= 50).astype(int), dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # Mask the cube where forest cover is less than 50%\n    cube_subset_crop_mask = cube_subset_crop.where(cube_subset_crop['forest_cover_50'] == 1)\n\n    if write:\n        cube_subset_crop.to_netcdf(out_path_crop)\n        cube_subset_crop_mask.to_netcdf(out_path_mask)\n        print(\"Wrote cropped cube with added forest percentages and binary mask to disk at:\", out_path_crop)\n        print(\"Wrote cropped and masked cube to disk at:\", out_path_mask)\n                                       \n    return cube_subset_crop, cube_subset_crop_mask"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html",
    "href": "quarto_book/02_cube_preprocessing.html",
    "title": "3  Preprocessing ESDC",
    "section": "",
    "text": "4 Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n1+1\n```"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#packages",
    "href": "quarto_book/01_load_aux_data.html#packages",
    "title": "2  Loading auxillary data",
    "section": "2.1 Packages",
    "text": "2.1 Packages\nHere we load packages and additionaly two functions from utils.py.\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n# load custom function from utils.py\nfrom utils import create_paths"
  },
  {
    "objectID": "quarto_book/01_load_aux_data.html#functions",
    "href": "quarto_book/01_load_aux_data.html#functions",
    "title": "2  Loading auxillary data",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nThis code stores 5 Functions:\n\ninitialize_gee: Authenticates and initializes the Google Earth Engine (GEE) API for further data processing\n\ndownload_german_border: Downloads the German border data from GEE and optionally exports it to a specified file path\n\nload_corine: Downloads and preprocesses CORINE land cover data for a specified region, exporting it to a file if needed\n\ncreate_sif_sample: Creates a sample TIFF file from the SIF data in the provided data cube subset and saves it to a specified output path\n\nload_aux_data: Integrates the auxiliary data loading process by initializing GEE, downloading the German border and CORINE data, and creating a SIF sample TIFF, managing file paths and downloads\n\n# Initialize GEE\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing and downloading Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif"
  },
  {
    "objectID": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "href": "quarto_book/02_cube_preprocessing.html#run-the-script",
    "title": "3  Preprocessing ESDC",
    "section": "3.2 Run the script",
    "text": "3.2 Run the script\nif __name__ == \"__main__\":\n    from utils import create_cube_subset\n\n    data_path = \"data\"\n\n    # Load the cube subset\n    cube_subset = create_cube_subset()\n\n    # Create file paths and if they don't exist, create folders\n    germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the Germany border geometry\n    germany_gpd = gpd.read_file(germany_shp_path)\n\n    # Preprocess the cube\n    cube_preprocess(\n        cube_subset, germany_gpd, corine_file_path, tif_sample_path, \n        out_path_crop=cube_crop_path, out_path_mask=cube_crop_mask_path, \n        all_touched=True, write=True\n    )"
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#packages-and-functions",
    "href": "quarto_book/03_base_analysis.html#packages-and-functions",
    "title": "4  Basic sif analysis",
    "section": "4.1 Packages and Functions",
    "text": "4.1 Packages and Functions\n\nplot_save_diff: Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two. Saves the figure to a specified path.\nplot_timeseries: Plots and saves the time series of SIF data within a specified time range. Optionally displays the plot.\nbase_analysis: Calculates the summer mean for each year in the dataset and the changes in SIF compared to the baseline period up to 2017. Returns the summer mean cube, the baseline mean to 2017, and the changes for specified years.\n\n# scripts/base_analysis.py\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom utils import create_paths\n\n\ndef base_analysis(cube, years=[2018, 2019]):\n    \"\"\"\n    Perform the base analysis by calculating the summer mean for each year and the change compared to the baseline up to 2017.\n\n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    years : list, optional\n    \"\"\"\n\n\n    # Calculate summer mean for each year for the sif variable\n    summer_sif_data = cube.sif_gosif.sel(time=cube['time.season'] == 'JJA')\n    summer_sif_mean_cube = summer_sif_data.groupby('time.year').mean(dim='time')\n\n    # Calculate change in summer mean SIF for each year compared to baseline up to 2017\n    changes = {}\n    summer_mean_to_2017 = summer_sif_mean_cube.sel(year=slice(None, 2017)).mean(dim='year')\n\n    for year in years:\n        summer_mean = summer_sif_mean_cube.sel(year=year)\n        change = summer_mean - summer_mean_to_2017\n        changes[year] = change\n    \n\n    return summer_sif_mean_cube, summer_mean_to_2017, changes \n\n# Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two.\ndef change_plot(ref_period,data_2018, changes, save_path = None):\n\n    # Create the figure and 2x2 subplots\n    fig, axd = plt.subplot_mosaic([['upleft', 'right'],\n                                ['lowleft', 'right']], layout='constrained', figsize=(10, 7))\n\n    # Plot each time slice on a different subplot\n    img1 = ref_period.plot(ax=axd[\"upleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"upleft\"].set_title(\"Mean 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"upleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"upleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img2 = data_2018.plot(ax=axd[\"lowleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"lowleft\"].set_title(\"Mean 2018\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"lowleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"lowleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img3 = changes[2018].plot(ax=axd[\"right\"], cmap=\"RdBu\", vmin=-0.15, vmax=0.15, add_colorbar=False)\n    axd[\"right\"].set_title(\"Difference SIF 2018 to mean of 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"right\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"right\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    # Add colorbars for each row\n    divider1 = make_axes_locatable(axd[\"upleft\"])\n    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img1, cax=cax1, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider2 = make_axes_locatable(axd[\"lowleft\"])\n    cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img2, cax=cax2, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider3 = make_axes_locatable(axd[\"right\"])\n    cax3 = divider3.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img3, cax=cax3, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n\n    if save_path:\n        # save the plot\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    return plt\n\n\n\n\n# Plotting a timeseries of the mean german SIF data\ndef plot_timeseries(time_series, save_path, time_range=[None, None], show = False):\n    \"\"\"\n    Plot and save the timeseries of the SIF data.\n    \n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    save_path : str \n        The path where the plot should be saved.\n    variable : str, optional\n        The variable to plot.\n    show : bool, optional\n        Whether to show the plot.\n    \n    \"\"\"\n\n    time_series = time_series.sel(time=slice(time_range[0], time_range[1]))\n\n    plt.figure(figsize=(10, 6))\n    \n    time_series.plot(marker='o', color='blue', linestyle='dashed')\n\n    plt.title(f'Time Series of SIF', fontsize=14)\n    plt.xlabel('Time', fontsize=12)\n    plt.ylabel('Sun-Induced Chlorophyll Fluorescence at 757 nm \\n [W m^-2 sr^-1 um^-1]', fontsize=12)\n    plt.grid(True, which='major', axis='both')\n    #plt.ylim(.1, .6) \n            \n            \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    if show:\n        plt.show()\n\ndef main():\n    \n    data_path = \"data\"\n\n    # Get path to the cube subset\n    _, _, _, cube_crop_path, cube_crop_mask_path = create_paths(data_path)\n\n    # Load masked cube subset (croped with forest mask and germany border)\n    cube_subset_mask = xr.open_dataset(cube_crop_mask_path)\n\n    # only use sif variable\n    cube_subset_mask_sif = cube_subset_mask.sif_gosif\n\n    # Calculate the mean of the SIF data over time\n    cube_sif_mean = cube_subset_mask_sif.mean(dim=['lat', 'lon'])\n\n    # Create the results directory\n    os.makedirs(os.path.join(\"results\", \"figures\"), exist_ok=True)\n\n    # Save plot of timeseries:\n    plot_timeseries(cube_sif_mean, save_path = os.path.join(\"results\", \"figures\", \"timeseries_full.png\"))\n    plot_timeseries(cube_sif_mean, time_range= [\"2015-01-01\", \"2022-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))"
  },
  {
    "objectID": "quarto_book/03_base_analysis.html#run-the-script",
    "href": "quarto_book/03_base_analysis.html#run-the-script",
    "title": "4  Basic sif analysis",
    "section": "4.2 Run the script",
    "text": "4.2 Run the script\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom utils import create_paths\n\n\ndef base_analysis(cube, years=[2018, 2019]):\n    \"\"\"\n    Perform the base analysis by calculating the summer mean for each year and the change compared to the baseline up to 2017.\n\n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    years : list, optional\n    \"\"\"\n\n\n    # Calculate summer mean for each year for the sif variable\n    summer_sif_data = cube.sif_gosif.sel(time=cube['time.season'] == 'JJA')\n    summer_sif_mean_cube = summer_sif_data.groupby('time.year').mean(dim='time')\n\n    # Calculate change in summer mean SIF for each year compared to baseline up to 2017\n    changes = {}\n    summer_mean_to_2017 = summer_sif_mean_cube.sel(year=slice(None, 2017)).mean(dim='year')\n\n    for year in years:\n        summer_mean = summer_sif_mean_cube.sel(year=year)\n        change = summer_mean - summer_mean_to_2017\n        changes[year] = change\n    \n\n    return summer_sif_mean_cube, summer_mean_to_2017, changes \n\n# Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two.\ndef change_plot(ref_period,data_2018, changes, save_path = None):\n\n    # Create the figure and 2x2 subplots\n    fig, axd = plt.subplot_mosaic([['upleft', 'right'],\n                                ['lowleft', 'right']], layout='constrained', figsize=(10, 7))\n\n    # Plot each time slice on a different subplot\n    img1 = ref_period.plot(ax=axd[\"upleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"upleft\"].set_title(\"Mean 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"upleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"upleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img2 = data_2018.plot(ax=axd[\"lowleft\"], cmap=\"viridis\", vmin=0, vmax=0.5, add_colorbar=False)\n    axd[\"lowleft\"].set_title(\"Mean 2018\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"lowleft\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"lowleft\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    img3 = changes[2018].plot(ax=axd[\"right\"], cmap=\"RdBu\", vmin=-0.15, vmax=0.15, add_colorbar=False)\n    axd[\"right\"].set_title(\"Difference SIF 2018 to mean of 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    axd[\"right\"].set_xlabel(\"Longitude\", fontsize=12)\n    axd[\"right\"].set_ylabel(\"Latitude\", fontsize=12)\n\n    # Add colorbars for each row\n    divider1 = make_axes_locatable(axd[\"upleft\"])\n    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img1, cax=cax1, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider2 = make_axes_locatable(axd[\"lowleft\"])\n    cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img2, cax=cax2, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n    divider3 = make_axes_locatable(axd[\"right\"])\n    cax3 = divider3.append_axes(\"right\", size=\"5%\", pad=0.5)\n    fig.colorbar(img3, cax=cax3, orientation=\"vertical\").ax.tick_params(labelsize=12)\n\n\n    if save_path:\n        # save the plot\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    return plt\n\n\n\n\n# Plotting a timeseries of the mean german SIF data\ndef plot_timeseries(time_series, save_path, time_range=[None, None], show = False):\n    \"\"\"\n    Plot and save the timeseries of the SIF data.\n    \n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    save_path : str \n        The path where the plot should be saved.\n    variable : str, optional\n        The variable to plot.\n    show : bool, optional\n        Whether to show the plot.\n    \n    \"\"\"\n\n    time_series = time_series.sel(time=slice(time_range[0], time_range[1]))\n\n    plt.figure(figsize=(10, 6))\n    \n    time_series.plot(marker='o', color='blue', linestyle='dashed')\n\n    plt.title(f'Time Series of SIF', fontsize=14)"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#packages-and-functions",
    "href": "quarto_book/04_test_modelling.html#packages-and-functions",
    "title": "6  Prelimnary Modelling",
    "section": "6.1 Packages and Functions",
    "text": "6.1 Packages and Functions\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import create_paths, start_logging\nfrom config import variables\nfrom modelling_functions import  setup_model_data, save_results, full_modelling\n\n\n\ndef main():\n\n    data_path = \"data\"\n   \n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n   \n    # do preprocessing: scaling and creating a dataframe, as well as getting the lat lon pairs defining all pixels\n    all_data_scaled, lat_lon_pairs, scalar_y = setup_model_data(cube_subset_crop_mask, variables)\n\n\n\n    ############  Modelling ############\n\n    # Create lookback array\n    look_backs = [15,30,45]\n\n    # Define the parameter grid for the local model\n    param_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n    }\n\n\n\n    cv = TimeSeriesSplit(n_splits=3)\n    # Run the local models for a subset\n    # and saving the results - filename is created based on model type and lookback\n    for look_back in look_backs:\n        \n        # os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n        \n        # Create a log file: \n        #start_logging(os.path.join(\"results\", \"logs\", f\"local_models_{look_back}.log\"))\n\n        # not auto regressive\n        output_data_local_auto = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=False, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n\n        save_results(output_data_local_auto, look_back, auto_regressive=False, global_model=False)\n\n        # auto regressive\n        output_data_local_noauto = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=True, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n        \n        save_results(output_data_local_noauto, look_back, auto_regressive=True, global_model=False)\n\n\n    # Run the global model on the full dataset\n    # and saving the results - filename is created based on model type and lookback\n\n    # grid for global model\n    # The parameters were reduced based on a first run for the results of the local model\n\n    param_grid_global = {\n        'units_lstm': [64, 128],\n        'activation': ['tanh'], \n        'epochs': [100],    \n        'learning_rate': [0.0001],\n        'dropout_rate': [0.2],\n        'batch_size': [25],\n        'num_lstm_layers': [1, 2]\n    }\n\n    # The lookback is set to 30 for the global model, as it was the best performing for the local models\n    look_back = 30\n\n    # not auto regressive\n    output_data_global_auto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_auto, look_back, auto_regressive=False, global_model=False)\n\n    # auto regressive\n    output_data_global_noauto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_noauto, look_back, auto_regressive=True, global_model=False)"
  },
  {
    "objectID": "quarto_book/04_test_modelling.html#run-the-script",
    "href": "quarto_book/04_test_modelling.html#run-the-script",
    "title": "6  Prelimnary Modelling",
    "section": "6.2 Run the script",
    "text": "6.2 Run the script\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_book/05_modelling.html#packages-and-functions",
    "href": "quarto_book/05_modelling.html#packages-and-functions",
    "title": "7  Actual Modelling",
    "section": "7.1 Packages and Functions",
    "text": "7.1 Packages and Functions\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import start_logging, create_paths\nfrom config import variables, param_grid_final\nfrom modelling_functions import full_modelling, save_results"
  },
  {
    "objectID": "quarto_book/05_modelling.html#run-the-script",
    "href": "quarto_book/05_modelling.html#run-the-script",
    "title": "7  Actual Modelling",
    "section": "7.2 Run the script",
    "text": "7.2 Run the script\n\ndef main():\n\n    data_path = \"data\"\n   \n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n   \n    # do preprocessing: scaling and creating a dataframe, as well as getting the lat lon pairs defining all pixels\n    all_data_scaled, lat_lon_pairs, scalar_y = setup_model_data(cube_subset_crop_mask, variables)\n\n\n    ############  Modelling ############\n\n    # Set lookback to 30 as it was the best performing in the test modelling\n    look_back = 30\n\n\n    # print grid searc parameter grid for the local model (can be found in config.py)\n    logging.info(print(param_grid_final))\n\n    cv = TimeSeriesSplit(n_splits=2)\n    \n    os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n\n\n    # Run the gridsearch gridsearch and training again with the local model without auto regression as it was the best perfoming model in 04_test_modelling.py\n    # Also relu is not used as activation function as tanh was performing better in every case\n            \n    # Create a log file: \n    start_logging(os.path.join(\"results\", \"logs\", f\"final_local_models_{look_back}.log\"))\n\n    # not auto regressive\n    output_data_local_auto = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_final, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_local_auto, look_back,\n                 auto_regressive=False, global_model=False,\n                 out_path=os.path.join(\"results\", \"modelling\", \"final\", \"results_full_local_auto_l30.json\"))\n\n    \nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_book/config.html",
    "href": "quarto_book/config.html",
    "title": "Config",
    "section": "",
    "text": "This scripts stores callable variables, used in mutiple scripts.\n# scripts/config.py\n\n\nvariables = [\n    \"sif_gosif\",\n    \"evaporation_era5\",\n    \"precipitation_era5\",\n    \"radiation_era5\",\n    \"air_temperature_2m\",\n    \"max_air_temperature_2m\",\n    \"min_air_temperature_2m\",\n]\n\n\n# Define the time and spatial subset\nmin_time = '2002-01-01'\nmax_time = '2021-12-31'\nlon_min, lon_max = 5.866, 15.042\nlat_min, lat_max = 47.270, 55.058\n\n\n############ Hyperparameter Grid setup for GridSearchCV  ############  \n\n# for local model\nparam_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n}\n\n# for global model\nparam_grid_global = {\n    'units_lstm': [64, 128],\n    'activation': ['tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2]\n}\n\n# for final model ( without \"relu\", 3 layer setup and dropout of 0.4, as these hyperparameters were not performing well in the test)\nparam_grid_final = {\n    'units_lstm': [64, 128],\n    'activation': ['tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2]\n}"
  },
  {
    "objectID": "quarto_book/utils.html",
    "href": "quarto_book/utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "These script stores some basic functions that are used in multiple scripts.\n\nconfigure_logging: Sets up logging to a specified file with INFO level.\nstart_logging: Resets logging configuration and sets it up with a new filename, ensuring a new log file is created when run in the same session.\ncreate_dir: Creates a directory if it does not already exist based on the provided file path.\ncreate_paths: Creates and returns paths for Germany’s border shapefile, CORINE land cover data for the year 2000, a SIF sample TIFF, the croped datacube, and the croped and masked datacube.\ncreate_cube_subset: Loads the Earth System Data Cube, subsets it based for the time range of sif and spatial boundaries of germany, selects the relevant variables, and adds the CRS (Coordinate Reference System).\n\n# scripts/utils.py\n\n\nfrom config import variables, min_time, max_time, lon_min, lon_max, lat_min, lat_max\nfrom xcube.core.store import new_data_store\nimport rioxarray as rio\nimport os\nimport logging\n\n\n\n# Function to setup logging to file\ndef configure_logging(filename):\n    logging.basicConfig(\n        filename=filename,\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n    )\n\n# Function to reset logging (so it creates a new file when run in the same session)\ndef start_logging(filename):\n    # Get the root logger\n    root_logger = logging.getLogger()\n    \n    # Remove all handlers associated with the root logger\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Configure logging with the new filename\n    configure_logging(filename)\n\n\ndef create_dir(file_path):\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n\ndef create_paths(data_path):\n\n    # Create path and folder for germany border shapefile\n    germany_shp_path = os.path.join(data_path, \"germany_shape\", 'germany_border.shp')\n    create_dir(germany_shp_path)\n\n    # Create path and folder for corine data year 2000\n    corine_file_path = os.path.join(data_path, \"landcover\", f\"forest_cover_2000.tif\")\n    create_dir(corine_file_path)\n\n    # Create path and folderfor sif sample tif\n    tif_sample_path= os.path.join(data_path, \"cubes\", \"cube_sif_sample.tif\")\n    create_dir(tif_sample_path)\n\n    cube_crop_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop.nc\")\n    cube_crop_mask_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop_mask.nc\")\n\n    return germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path\n\n\ndef create_cube_subset(variables = variables, \n                        min_time = min_time, max_time = max_time,\n                        lon_min = lon_min, lon_max = lon_max,\n                        lat_min = lat_min, lat_max = lat_max):\n\n    # Initalize xcube store\n    store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n    store.list_data_ids()\n\n    # Load cube from store\n    cube = store.open_data( 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr')\n\n    # Subset the cube\n    cube_subset = cube.sel(time=slice(min_time, max_time)) \\\n                    .sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n\n    # select only specified variables from esdc\n    cube_subset = cube_subset[variables]\n\n    # Add crs to the cube\n    cube_subset.rio.write_crs(4326, inplace = True)\n\n    return cube_subset"
  },
  {
    "objectID": "quarto_book/modelling_functions.html",
    "href": "quarto_book/modelling_functions.html",
    "title": "Modelling functions",
    "section": "",
    "text": "In this script all functions used for modelling and data specific modelling, as well as model evaluation and plotting.\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y\n\n\n\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index\n\n\n\n\n########################################################\n# Modelling\n########################################################\n\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_\n\n\n\n\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array\n\n\n############  Evaluate model ############\ndef evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }\n\n\n\n############  Writing results ############\n\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")\n\n\n############  Full modelling function ############\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data\n\n\n    # Save the results to a JSON file, in a folder named after the lookback period\n\n\n############  Plotting ############\n\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#section",
    "href": "quarto_book/modelling_functions.html#section",
    "title": "Modelling functions",
    "section": "",
    "text": "import os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y\n\n\n\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index\n\n\n\n\n########################################################\n# Modelling\n########################################################\n\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_\n\n\n\n\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array\n\n\n############  Evaluate model ############\ndef evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }\n\n\n\n############  Writing results ############\n\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")\n\n\n############  Full modelling function ############\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data\n\n\n    # Save the results to a JSON file, in a folder named after the lookback period\n\n\n############  Plotting ############\n\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#data-preprocessing",
    "href": "quarto_book/modelling_functions.html#data-preprocessing",
    "title": "Modelling functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#data-restructuring-and-splitting",
    "href": "quarto_book/modelling_functions.html#data-restructuring-and-splitting",
    "title": "Modelling functions",
    "section": "Data restructuring and splitting",
    "text": "Data restructuring and splitting\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#modelling",
    "href": "quarto_book/modelling_functions.html#modelling",
    "title": "Modelling functions",
    "section": "Modelling",
    "text": "Modelling\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#predicting",
    "href": "quarto_book/modelling_functions.html#predicting",
    "title": "Modelling functions",
    "section": "Predicting",
    "text": "Predicting\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#fit-model-with-best-params-and-evaluating",
    "href": "quarto_book/modelling_functions.html#fit-model-with-best-params-and-evaluating",
    "title": "Modelling functions",
    "section": "Fit model with best params and evaluating",
    "text": "Fit model with best params and evaluating\ndef fit_evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#write-results",
    "href": "quarto_book/modelling_functions.html#write-results",
    "title": "Modelling functions",
    "section": "Write results",
    "text": "Write results\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#modelling-with-gridsearchcv",
    "href": "quarto_book/modelling_functions.html#modelling-with-gridsearchcv",
    "title": "Modelling functions",
    "section": "Modelling with GridSearchCV",
    "text": "Modelling with GridSearchCV\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#full-modelling-workflow-function",
    "href": "quarto_book/modelling_functions.html#full-modelling-workflow-function",
    "title": "Modelling functions",
    "section": "Full modelling workflow function",
    "text": "Full modelling workflow function\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#plotting",
    "href": "quarto_book/modelling_functions.html#plotting",
    "title": "Modelling functions",
    "section": "Plotting",
    "text": "Plotting\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-mod_pre",
    "href": "quarto_book/modelling_functions.html#sec-mod_pre",
    "title": "Modelling functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-restruc",
    "href": "quarto_book/modelling_functions.html#sec-restruc",
    "title": "Modelling functions",
    "section": "Data restructuring and splitting",
    "text": "Data restructuring and splitting\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-model",
    "href": "quarto_book/modelling_functions.html#sec-model",
    "title": "Modelling functions",
    "section": "Modelling with GridSearchCV",
    "text": "Modelling with GridSearchCV\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-pred",
    "href": "quarto_book/modelling_functions.html#sec-pred",
    "title": "Modelling functions",
    "section": "Predicting",
    "text": "Predicting\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-eval",
    "href": "quarto_book/modelling_functions.html#sec-eval",
    "title": "Modelling functions",
    "section": "Fit model with best params and evaluating",
    "text": "Fit model with best params and evaluating\ndef fit_evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-write",
    "href": "quarto_book/modelling_functions.html#sec-write",
    "title": "Modelling functions",
    "section": "Write results",
    "text": "Write results\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-full",
    "href": "quarto_book/modelling_functions.html#sec-full",
    "title": "Modelling functions",
    "section": "Full modelling workflow function",
    "text": "Full modelling workflow function\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data"
  },
  {
    "objectID": "quarto_book/modelling_functions.html#sec-plot",
    "href": "quarto_book/modelling_functions.html#sec-plot",
    "title": "Modelling functions",
    "section": "Plotting",
    "text": "Plotting\n# Function to plot predicted vs. actual values with MSE in subplots\ndef plot_multiple_results(results, evaluation, lat_lon_pairs, test_index):\n    num_plots = len(results)\n    num_cols = 2\n    num_rows = (num_plots + 1) // num_cols\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; num_plots:\n            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n            lat, lon = lat_lon_pairs.iloc[i]\n            time_index = sorted(test_index)\n\n            ax.plot(time_index[:-1], testY, label=\"Actual\")\n            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Value\")\n            ax.legend()\n            ax.grid(True)\n\n            # Add MSE to the corner\n            ax.text(\n                0.95,\n                0.05,\n                f\"RMSE: {rmse:.2f}\",\n                verticalalignment=\"bottom\",\n                horizontalalignment=\"right\",\n                transform=ax.transAxes,\n                color=\"red\",\n                fontsize=12,\n            )\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Authors",
    "text": "Authors\nLuis Maecker - maecker@studserv.uni-leipzig.de\nImke Ott -\nMoritz Mischi -"
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Abstract",
    "text": "Abstract\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. In this study, we use a Long Short-Term Memory (LSTM) neural network to forecast SIF for German forests, with ERA5 climate data as predictors. Data of the years 2018 and 2019 is used to evaluate the models performance during and after the heatwave in Germany. We show that the model does generally well in capturing the temporal dynamics of SIF in response to climatic variations, with an R-squared of 0.75 and a mean absolute error 0.05. However at sites with particularly harsh conditions in 2018 the model did not capture the strong response of the vegetation. Here more research is needed. Still the results show the potential of SIF for ecosystem analysis as well as LSTM for modelling vegetation dynamics based on climatic drivers. Reliable SIF forecasts under differing climatic conditions are a valuable tool to asses potential future impact of climate change on German forests. This can lead to more informed decision-making and better mitigation of these challenges."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Motivation",
    "text": "Motivation\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. In this study, we use a Long Short-Term Memory (LSTM) neural network to forecast SIF for German forests, with ERA5 climate data as predictors. Data of the years 2018 and 2019 is used to evaluate the models performance during and after the heatwave in Germany. Sucessfully modelling SIF using climatic predictors, would allow us to asses the impact of potential future climate changes on the photosynthetic activity of forests and their role in the carbon cycle."
  },
  {
    "objectID": "quarto_book/intro.html#motivation",
    "href": "quarto_book/intro.html#motivation",
    "title": "Introduction",
    "section": "Motivation",
    "text": "Motivation\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. Sucessfully modelling SIF using climatic predictors, would allow us to asses the impact of potential future climate changes on the photosynthetic activity of forests and their role in the carbon cycle."
  },
  {
    "objectID": "quarto_book/intro.html#data",
    "href": "quarto_book/intro.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\n\nEarth System Data Cube\n\nThis dataset is a a data cube provided by the Earth System Data Lab. It is a cube created by preprocessing and harmonising different datasets to a common spatio-temporal grid. The Variables we use:\n\nSIF-Data originally created by by Xing Li and Jingfeng Xiao (Li and Xiao 2019)\n6 ERA-5-variables originally created by Hersbach et al (Hersbach et al. 2020):\n\nevaporation\nprecipitation\nradiation\nair_temperature\nmax_air_temperature\nmin_air_temperature\n\n\n\n\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The ERA5 Global Reanalysis.” Quarterly Journal of the Royal Meteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product of Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "main.html#packages",
    "href": "main.html#packages",
    "title": "1  Full Workflow",
    "section": "1.1 Packages",
    "text": "1.1 Packages\n\nimport sys\nimport os\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rio\n\n# Add the parent directory to sys.path\nsys.path.append(os.path.abspath(os.path.join('scripts')))\n\n# from scripts.config import *\nfrom scripts.utils import create_cube_subset, create_paths\nfrom scripts.load_aux_data_01 import load_aux_data\nfrom scripts.cube_preprocessing_02 import cube_preprocess\nfrom scripts.base_analysis_03 import base_analysis, change_plot, plot_timeseries\nfrom scripts.config import variables\nimport matplotlib.pyplot as plt\n\n# Write data to disk set to False\nwrite_data = False"
  },
  {
    "objectID": "main.html#downloading-auxillary-data",
    "href": "main.html#downloading-auxillary-data",
    "title": "1  Full Workflow",
    "section": "1.3 Downloading auxillary data",
    "text": "1.3 Downloading auxillary data\nThe first part of the analysis is to download and preprocess all the necassaty auxillary data:\n\ndownload german border shapefile\ndownload and preprocess Corine data (for the forest mask) and use the border data to define the AOI\ncreate sif sample tif for spatial resolution and transform (used later to resample corine data)\n\nMore Information\n\n# Download auxiliary data (Germany border, Corine landcover data, sample tif)\nload_aux_data(data_path, cube_subset, download = write_data)\n\n# Load the germany border shapefile\ngermany_gpd = gpd.read_file(germany_shp_path)\n\n\n            \n            \n\n\nDownloading German border data...\n----------------------------------------------------------------------------------------------------\nProcessing Corine data...\nDownloading Corine data\n----------------------------------------------------------------------------------------------------\nSample path created at: data/cubes/cube_sif_sample.tif\n----------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "main.html#preprocessing-the-cube",
    "href": "main.html#preprocessing-the-cube",
    "title": "1  Full Workflow",
    "section": "1.4 Preprocessing the cube",
    "text": "1.4 Preprocessing the cube\nNext we want to preprocess our data cube.\nThis includes: - croping the cube with the german border shape\n\nmasking the cube with the corine forest cover:\n\nCalculate the forest percentage of the Corine landcover data over the cube grid\nadd the forest percentages to the cube\nadd a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\nuse the layer to mask the cube\n\n\nMore Information\n\n\n# Crop the cube to the extent of Germany and mask it with the Corine landcover data (50% forest cover)\ncube_subset_crop, cube_subset_crop_mask = cube_preprocess(\n        cube_subset, germany_gpd, corine_file_path, cube_sample_path, \n        out_path_crop=cube_crop_path, out_path_mask=cube_crop_mask_path, \n        all_touched=True, write=write_data\n    )\n    \n# Calculate the temporal changes in the variables \nsummer_sif_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n\n\n\n            \n            \n\n\nPreprocessing cube\nClipping cube to Germany border\nCalculate forest cover percentage over cube grid\n\n\n\nsummer_sif_mean_cube\n\n\n            \n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'sif_gosif' (year: 20, lat: 31, lon: 37)&gt; Size: 92kB\ndask.array&lt;stack, shape=(20, 31, 37), dtype=float32, chunksize=(1, 31, 25), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat          (lat) float64 248B 47.38 47.62 47.88 ... 54.38 54.62 54.88\n  * lon          (lon) float64 296B 5.875 6.125 6.375 ... 14.38 14.62 14.88\n    spatial_ref  int64 8B 0\n  * year         (year) int64 160B 2002 2003 2004 2005 ... 2018 2019 2020 2021\nAttributes: (12/25)\n    acknowledgment:             https://doi.org/10.3390/rs11050517\n    date_modified:              2022-10-11 22:20:05.841847\n    description:                GOSIF Solar-Induced Chlorophyll Fluorescence ...\n    geospatial_lat_max:         89.87499999999999\n    geospatial_lat_min:         -89.87500000000001\n    geospatial_lat_resolution:  0.25\n    ...                         ...\n    standard_name:              sif\n    temporal_resolution:        8D\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        2000-03-01T00:00:00.000000000\n    time_period:                8D\n    units:                      W m^-2 sr^-1 um^-1xarray.DataArray'sif_gosif'year: 20lat: 31lon: 37dask.array&lt;chunksize=(1, 31, 25), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n89.61 kiB\n3.03 kiB\n\n\nShape\n(20, 31, 37)\n(1, 31, 25)\n\n\nDask graph\n40 chunks in 69 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (4)lat(lat)float6447.38 47.62 47.88 ... 54.62 54.88long_name :latitudestandard_name :latitudeunits :degrees_northaxis :Yarray([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875])lon(lon)float645.875 6.125 6.375 ... 14.62 14.88long_name :longitudestandard_name :longitudeunits :degrees_eastaxis :Xarray([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :5.75 0.25 0.0 47.25 0.0 0.25array(0)year(year)int642002 2003 2004 ... 2019 2020 2021array([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])Indexes: (3)latPandasIndexPandasIndex(Index([47.375, 47.625, 47.875, 48.125, 48.375, 48.625, 48.875, 49.125, 49.375,\n       49.625, 49.875, 50.125, 50.375, 50.625, 50.875, 51.125, 51.375, 51.625,\n       51.875, 52.125, 52.375, 52.625, 52.875, 53.125, 53.375, 53.625, 53.875,\n       54.125, 54.375, 54.625, 54.875],\n      dtype='float64', name='lat'))lonPandasIndexPandasIndex(Index([ 5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875],\n      dtype='float64', name='lon'))yearPandasIndexPandasIndex(Index([2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n       2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021],\n      dtype='int64', name='year'))Attributes: (25)acknowledgment :https://doi.org/10.3390/rs11050517date_modified :2022-10-11 22:20:05.841847description :GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Datageospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 757 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :0.0001processing_steps :['Merging tif files', 'Converting water bodies and snow covered areas to NaN', 'Applying original scale factor', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.3390/rs11050517']reported_day :5.0source :['https://globalecology.unh.edu/data.html']standard_name :siftemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :W m^-2 sr^-1 um^-1\n\n\n\nchange_plot(ref_period = summer_mean_to_2017, data_2018 = summer_sif_mean_cube.sel(year=2018), changes = changes);"
  },
  {
    "objectID": "main.html#setup",
    "href": "main.html#setup",
    "title": "1  Full Workflow",
    "section": "1.2 Setup",
    "text": "1.2 Setup\nFirst we setup the data path and use create_paths() from utils to create paths necessary throughout the analysis.\nSecond we use create_subset() to create a subset of the Earth System Data Cube, croped to:\n\nour AOI, the extent of germany\nthe time span where the SIF variable is avaialble\nto the relevant variables\n\nMore Information\n\n# Create a data directory\ndata_path = \"data\"\nos.makedirs(data_path, exist_ok=True)\n\n# Create paths to the data\ngermany_shp_path, corine_file_path, cube_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n# Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\ncube_subset = create_cube_subset()\n\nprint(cube_subset)\n\n\n            \n            \n\n\n&lt;xarray.Dataset&gt; Size: 30MB\nDimensions:                 (time: 920, lat: 31, lon: 37)\nCoordinates:\n  * lat                     (lat) float64 248B 47.38 47.62 47.88 ... 54.62 54.88\n  * lon                     (lon) float64 296B 5.875 6.125 6.375 ... 14.62 14.88\n  * time                    (time) datetime64[ns] 7kB 2002-01-05 ... 2021-12-31\n    spatial_ref             int64 8B 0\nData variables:\n    sif_gosif               (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    evaporation_era5        (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    precipitation_era5      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    radiation_era5          (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    air_temperature_2m      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    max_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    min_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1"
  }
]