[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "",
    "text": "Outline\nThis is the quarto book for the project “Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning”. This project was created as part of two course projects in the “Earth System Data Science and Remote Sensing” Master at Leipzig University. The courses are Scientific Writing by Prof. Dr. Miguel Mahecha and Spatio-temporal Data by Dr. Guido Kraemer. This quarto book contains all code used for the analysis, shows the main results and briefly discusses them, as well as a Data Management Plan. The project is fully reproducible and all data is openly available. The project can be found at this Gitlab repository."
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Structure",
    "text": "Structure"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Forecasting Solar Induced Fluorescence for German Forest after the Heatwave in 2018 using Deep Learning",
    "section": "Authors",
    "text": "Authors\nLuis Maecker - maecker@studserv.uni-leipzig.de\nImke Ott - imke.ott@studserv.uni-leipzig.de\nMoritz Mischi - moritz.mischi@studserv.uni-leipzig.de"
  },
  {
    "objectID": "quarto_files/intro.html#motivation",
    "href": "quarto_files/intro.html#motivation",
    "title": "Introduction",
    "section": "Motivation",
    "text": "Motivation\nGerman Forest have faced a strong dieback in the last years. Climate extremes like the Heatwave in 2018 are regarded as one of the major drivers for this process. Remote Sensing data can be used to effectively estimate vegetation variables over large areas. While most commonly spectral indices like the NDVI are applied, plant traits like the sun-induced fluorescence (SIF) provide a more direct link to the physiological processes in vegetation. SIF is linked to photosynthetic activity and it reacts near-instantaneously to changes in the environment e.g. light or temperature. So far there is a lack of reliable forecasts of vegetation variables for German forests under varying climatic conditions. Particularly for extreme weather events forecasts were so far not successful. Sucessfully modelling SIF using climatic predictors, would allow us to asses the impact of potential future climate changes on the photosynthetic activity of forests and their role in the carbon cycle."
  },
  {
    "objectID": "quarto_files/intro.html#data",
    "href": "quarto_files/intro.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\n\nEarth System Data Cube\nThis dataset is a a data cube provided by the Earth System Data Lab. It is a cube created by preprocessing and harmonising different datasets to a common spatio-temporal grid. The Variables we use:\n\nSIF-Data originally created by by Xing Li and Jingfeng Xiao (Li and Xiao 2019)\n6 ERA-5-variables originally created by Hersbach et al (Hersbach et al. 2020):\n\nevaporation\nprecipitation\nradiation\nair_temperature\nmax_air_temperature\nmin_air_temperature\n\n\n\n\nCorine Landcover Data\nThe Corine Landcover data created by the European Environment Agency as part of the Copernicus Land Monitoring Service (European Environment Agency 2019). It has a spatial resolution of 1000 meter and is availabe every 6 years since 2000. Here we only use data from the year 2000.\nIt contains a range of classes, covering different landcover types. In this analysis we only used the 3 forest related classes:\n\n311: Broadleafed Forest\n312: Coniferous Forest\n313: Mixed Forest"
  },
  {
    "objectID": "quarto_files/intro.html#methodology",
    "href": "quarto_files/intro.html#methodology",
    "title": "Introduction",
    "section": "Methodology",
    "text": "Methodology\nThe basic structure of the analysis is:\n\n1: Downloading all auxillary data\n2: Preprocessing the Earth System Data Cube\n3: Analyzing Sif Change in 2018 compared to the preceeding years\n4: Prelimnary Modelling - Testing different modelling structures\n5: Actual Modelling of all timeseries\n\nThe next page will walk you through the full workflow and methodlogy, linking to the relevant code for each section. The full analysis was done using the code found under “Executable Scripts”. The following page “Full Workflow” plots and evaluates these results and will walk you through the general workflow.\n\n\n\n\nEuropean Environment Agency. 2019. “CORINE Land Cover 2000 (Raster 100 m), Europe, 6-Yearly - Version 2020_20u1, May 2020.” European Environment Agency. https://doi.org/10.2909/ddacbd5e-068f-4e52-a596-d606e8de7f40.\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The ERA5 Global Reanalysis.” Quarterly Journal of the Royal Meteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product of Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "main_workflow.html#setup",
    "href": "main_workflow.html#setup",
    "title": "1  Full Workflow",
    "section": "1.1 Setup",
    "text": "1.1 Setup\nThroughout the scripts some functions and variables are used multiple times. These functions and variables were defined in config.py and utils.py.\nFrom the config script we use the variables, the latitude and longitude range as well as a time range to subset our datacube, containg climatic data and SIF data.\nDetails: Config\nThe two most important functions from utils are:\n\ncreate_paths() from utils to create paths necessary throughout the analysis.\ncreate_subset() to create a subset of the Earth System Data Cube, croped to:\n\nour AOI, the extent of germany\nthe time span where the SIF variable is avaialble\nto the relevant variables\n\n\nDetails: Utility functions\n\n\nCode\nimport sys\nimport os\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rio\nimport json\nimport numpy as np\nfrom matplotlib.colors import ListedColormap, Normalize\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nfrom matplotlib.gridspec import GridSpec\n\n# Add the parent directory to sys.path\nsys.path.append(os.path.abspath(os.path.join('scripts')))\n\n# from scripts.config import *\nfrom scripts.utils import create_cube_subset, create_paths\nfrom scripts.s03_base_analysis import base_analysis, change_plot, plot_timeseries\nfrom scripts.config import variables\nfrom scripts.modelling_functions import plot_forecasts_from_dict;\nimport matplotlib.pyplot as plt\n\n\n2024-07-29 08:22:57.086573: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-29 08:22:57.088989: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-07-29 08:22:57.125407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-07-29 08:22:58.200263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\nHere you can see the subsetted cube\n\n\nCode\n# Create a data directory\ndata_path = \"data\"\nos.makedirs(data_path, exist_ok=True)\n\n# Create paths to the data\ngermany_shp_path, corine_file_path, cube_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n# Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\ncube_subset = create_cube_subset()\n\n# remove attributes\ncube_subset.attrs = {}\n\nprint(cube_subset)\n\n\n&lt;xarray.Dataset&gt; Size: 30MB\nDimensions:                 (time: 920, lat: 31, lon: 37)\nCoordinates:\n  * lat                     (lat) float64 248B 47.38 47.62 47.88 ... 54.62 54.88\n  * lon                     (lon) float64 296B 5.875 6.125 6.375 ... 14.62 14.88\n  * time                    (time) datetime64[ns] 7kB 2002-01-05 ... 2021-12-31\n    spatial_ref             int64 8B 0\nData variables:\n    sif_gosif               (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    evaporation_era5        (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    precipitation_era5      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    radiation_era5          (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    air_temperature_2m      (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    max_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;\n    min_air_temperature_2m  (time, lat, lon) float32 4MB dask.array&lt;chunksize=(222, 31, 25), meta=np.ndarray&gt;"
  },
  {
    "objectID": "main_workflow.html#downloading-auxillary-data",
    "href": "main_workflow.html#downloading-auxillary-data",
    "title": "1  Full Workflow",
    "section": "1.2 Downloading auxillary data",
    "text": "1.2 Downloading auxillary data\nThe first part of the analysis is to download and preprocess all the necassaty auxillary data:\n\ndownload german border shapefile\ndownload Corine data for the year 2000 and select forest classes (for the forest mask)\ncreate sif sample tif for spatial resolution and transform (used later to resample corine data)\n\nDetails: 2 Loading auxillary data\n\nPlot showing the Corine Forest Cover data\n\n\nCode\n# Load the germany border shapefile\ngermany_gpd = gpd.read_file(germany_shp_path)\n\n# Load the CORINE land cover data\ncorine_raster = rio.open_rasterio(corine_file_path)\n\n# set 0 to na\ncorine_raster = corine_raster.where(corine_raster != 0)\n\n# create parameters for plotting\ncmap = ListedColormap(['#66a61e', '#1b9e77', '#7570b3'])\nvmin = 311; vmax = 313\nlegend_labels=['Decidious Forest', 'Coniferous Forest', 'Mixed Forest']\n\n\n# Plot the CORINE land cover data with the Germany border overlayed\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot Germany with gray fill and black boundary\ngermany_gpd.plot(ax=ax, color='lightgray', edgecolor='black', alpha=1, linewidth=1.5)\n\n# Plot corine data\ncorine_raster.plot(ax=ax, alpha=1, vmin=vmin, vmax=vmax, cmap= cmap, add_colorbar=False)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm=Normalize(vmin=vmin, vmax=vmax), cmap=cmap),\n                    ax=ax, fraction=0.02, pad=0.04)\n\n\n# set title\nax.set_title(\"CORINE Land Cover Data\", fontsize=16)\n\n# Change color bar\nticks = np.linspace(vmin, vmax, len(legend_labels))\ncbar.set_ticks(ticks)\ncbar.ax.set_yticklabels(legend_labels)\n\nplt.show()"
  },
  {
    "objectID": "main_workflow.html#preprocessing-the-cube",
    "href": "main_workflow.html#preprocessing-the-cube",
    "title": "1  Full Workflow",
    "section": "1.3 Preprocessing the cube",
    "text": "1.3 Preprocessing the cube\nNext we preprocessed the subsetted data cube.\nThis includes:\n\ncroping the cube with the german border shape\nmasking the cube with the corine forest cover:\n\nCalculate the forest percentage of the Corine landcover data over the cube grid\nadd the forest percentages to the cube\nadd a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\nuse the layer to mask the cube\n\n\nThe results are cube_subset_crop which is croped with the germany border and cube_subset_mask, masked with the forest cover.\nDetails: 2 Preprocessing ESDC\n\nVisualization of Corine Data Processing.\n\n\nCode\n# Load cube subset croped and cube subset mask\ncube_subset_crop = xr.open_dataset(cube_crop_path)\ncube_subset_mask = xr.open_dataset(cube_crop_mask_path)\n\n# Load the forest percentages raster\nforest_percentages = rio.open_rasterio(cube_crop_path.replace(\".nc\", \"_percentages.tif\"))\n\n# Create a grid spec with extra space at the bottom\nfig = plt.figure(figsize=(14, 8))\ngs = GridSpec(2, 3, height_ratios=[1, 0.03], hspace=0.15)\n\n# Create subplots in the grid spec\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[0, 2])\n\n# Plot rasterio dataset\ncorine_raster.plot(ax=ax0, alpha=1, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\nax0.set_title(\"Corine Forest Cover data\")\nax0.set_ylabel(\"Latitude\")\nax0.set_xlabel(\"Longitude\")\n\n# Plot the forest percentages raster\nforest_im = forest_percentages.plot(ax=ax1, alpha=1, add_colorbar=False)\nax1.set_title(\"Resampled to percentages\")\nax1.set_ylabel(\"\")\nax1.set_xlabel(\"Longitude\")\n\n# Add a colorbar at the bottom for the forest percentages raster\ncax = fig.add_subplot(gs[1, 1])  # Add a subplot in the second row, middle column\ncbar = fig.colorbar(forest_im, cax=cax, orientation='horizontal')\ncbar.set_label(\"Forest cover in %\")\n\n# Plot the forest cover 50% threshold\ncube_subset_crop.forest_cover_50.plot(ax=ax2, vmin=0, add_colorbar=False)\nax2.set_title(\"50% Threshold\")\nax2.set_ylabel(\"\")\nax2.set_xlabel(\"Longitude\")\n\n# Main title for the entire figure\nfig.suptitle(\"Corine Data Processing\", fontsize=16)\n\n# Adjust the layout to ensure everything fits well\nplt.subplots_adjust(left=0.05, right=0.95, top=0.90, bottom=0)\nplt.show()\n\n\n\n\n\n\n\nShowing examplary timestep of croped and masked + croped cube\n\n\nCode\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n\n# Plot the croped dataset\nim = cube_subset_crop.sif_gosif.isel(time=20).plot(ax=axes[0],vmin=0.2,vmax=0.6, add_colorbar=False, cmap='viridis')\naxes[0].set_title(\"Cube Subset Cropped\")\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\n\n# Plot the masked dataset\ncube_subset_mask.sif_gosif.isel(time=20).plot(ax=axes[1],vmin=0.2,vmax=0.6, add_colorbar=True, cmap='viridis')\naxes[1].set_title(\"Cube Subset Masked\")\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"\")\n\n# Add germany border\ngermany_gpd.plot(ax=axes[0], edgecolor='black', alpha=1, linewidth=1.5, facecolor = \"none\")\ngermany_gpd.plot(ax=axes[1], edgecolor='black', alpha=1, linewidth=1.5, facecolor = \"none\" )\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust rect to make room for the colorbar\nplt.show()"
  },
  {
    "objectID": "main_workflow.html#basic-analysis-of-sif-data",
    "href": "main_workflow.html#basic-analysis-of-sif-data",
    "title": "1  Full Workflow",
    "section": "1.4 Basic Analysis of SIF-Data",
    "text": "1.4 Basic Analysis of SIF-Data\nThe next step in our analysis was to\n\nPerform a change detection by calculating the summer mean for each year and the change for the year 2018 to the baseline up to 2017\n\nDetails: “3 Basic SIF Analysis”\n\nPlot summer means for the SIF variable for the reference period 2000-2017, 2018 and the difference of them.\nWe can see that the SIF values are lower in 2018 compared to the reference period, all over germany, except in some small areas, mostly in the Alps. This overlaps with our expectation, although this methodology does not relate the change to the Heatwave, but only shows that 2018 was a year with comparably low SIF over germany.\n\n\nCode\n# Calculate the temporal changes in the variables using from s03_base_analysis.py\nsummer_sif_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n# Select only year 2018 from mean over summer months\nsummer_mean_2018 = summer_sif_mean_cube.sel(year=2018)\n\n# Plot the changes using the function defined in s03_base_analysis.py\nchange_plot(ref_period = summer_mean_to_2017, data_2018 = summer_mean_2018, changes = changes);\n\n\n\n\n\n\nPlot Sif Time series (mean over masked cells in germany)\nIn the timeseries we can see also see that the year 2018 had comparably low SIF values in the summer months compared to the preceeding years.\n\n\nCode\n# Create plot of timeseries\nplot_timeseries(cube_subset_mask, time_range= [\"2013-01-01\", \"2018-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))"
  },
  {
    "objectID": "main_workflow.html#prelimnary-modelling",
    "href": "main_workflow.html#prelimnary-modelling",
    "title": "1  Full Workflow",
    "section": "1.5 Prelimnary Modelling",
    "text": "1.5 Prelimnary Modelling\nTo determine what is the appropriate model structure, we tested for different approaches.\nFour general model setups were tested. These are differing in whether they are global or local models and whether they have an auto regressive component or not, meaning whether they encoperate a shifted version of the target variable (SIF) as a predictor.\nTo find the best hyperparameters 3 look back periods were tested and a grid search cross-validation was done.\nDetails: 5 Prelimnary Modelling\n\nTable showing mae and rmse model performances\nWe can see that the local models clearly performed better. For that reason global models were not further considered.\n\n\nCode\nimport os\nimport json\nfrom collections import defaultdict\n\n# Function to read JSON files and extract relevant data\ndef read_json_files(base_dir):\n    # Initialize a dictionary to store the extracted data\n    results = {\n        \"results_l15\": {\"auto\": [], \"noauto\": []},\n        \"results_l30\": {\"auto\": [], \"noauto\": [], \"global_auto\": [], \"global_noauto\": []},\n        \"results_l45\": {\"auto\": [], \"noauto\": []},\n    }\n\n    # Traverse through the base directory and subdirectories\n    for lookback_dir in [\"results_l15\", \"results_l30\", \"results_l45\"]:\n        lookback_path = os.path.join(base_dir, lookback_dir)\n        if not os.path.isdir(lookback_path):\n            continue\n\n        # Identify files containing \"auto\", \"noauto\", or \"global\" in their names\n        for filename in os.listdir(lookback_path):\n            if \"global\" in filename:\n                continue\n\n            if \"noauto\" in filename:\n                key = \"noauto\"\n            elif \"auto\" in filename:\n                key = \"auto\"\n            else:\n                continue\n\n            file_path = os.path.join(lookback_path, filename)\n\n            # Read and parse the JSON file\n            with open(file_path, 'r') as file:\n                data = json.load(file)\n\n                # Extract relevant data and store it in the results dictionary\n                for location, details in data.items():\n                    result_entry = {\n                        \"location\": location,\n                        \"best_params\": details.get(\"best_params\", {}),\n                        \"look_back\": details.get(\"look_back\", 0),\n                        \"evaluation\": details.get(\"evaluation\", {}),\n                    }\n                    results[lookback_dir][key].append(result_entry)\n\n    return results\n\n# Function to calculate mean performance metrics\ndef calculate_mean_performance(results):\n    # Initialize dictionaries to store the sum and count of performance metrics\n    performance_sums = defaultdict(lambda: {\"mae\": 0, \"rmse\": 0, \"count\": 0})\n    mean_performance = {}\n\n    # Sum the performance metrics and count the number of entries\n    for lookback in results:\n        for auto_type in results[lookback]:\n            for entry in results[lookback][auto_type]:\n                evaluation = entry[\"evaluation\"]\n                if \"mae\" in evaluation and \"rmse\" in evaluation:\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"mae\"] += evaluation[\"mae\"]\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"rmse\"] += evaluation[\"rmse\"]\n                    performance_sums[f\"{lookback}_{auto_type}\"][\"count\"] += 1\n\n    # Calculate the mean performance metrics\n    for key, sums in performance_sums.items():\n        if sums[\"count\"] &gt; 0:\n            mean_performance[key] = {\n                \"mean_mae\": np.round(sums[\"mae\"] / sums[\"count\"], 4),\n                \"mean_rmse\": np.round(sums[\"rmse\"] / sums[\"count\"], 4)\n            }\n        else:\n            mean_performance[key] = {\"mean_mae\": None, \"mean_rmse\": None}\n\n    return mean_performance\n\n# Base directory where the results folders are located\nbase_dir = os.path.join(\"results\", \"modelling\")\n\n# Read JSON files and extract data\nresults_data = read_json_files(base_dir)\n\n# Calculate mean performance metrics\nmean_performance_data = calculate_mean_performance(results_data)\n\n\ngloba_auto_path = os.path.join(base_dir, \"results_l30\", \"results_global_auto_l30.json\")\nglobal_noauto_path = os.path.join(base_dir, \"results_l30\", \"results_global_noauto_l30.json\")\n\ndef get_mae_rmse_from_json(json_path):\n    with open(json_path, 'r') as file:\n        data = json.load(file)\n    return np.round(data[\"evaluation\"][\"mae\"],3), np.round(data[\"evaluation\"][\"rmse\"],3)\n\nglobal_results = get_mae_rmse_from_json(globa_auto_path), get_mae_rmse_from_json(global_noauto_path)\n\nimport pandas as pd\n# Convert the dictionary to a DataFrame\ndata = []\nfor key, values in mean_performance_data.items():\n    lookback, auto_type = key.rsplit('_', 1)\n    data.append({\n        'model' : 'local',\n        'lookback': lookback[-2:],\n        'type': auto_type,\n        'mean_mae': values['mean_mae'],\n        'mean_rmse': values['mean_rmse']\n    })\n\n# Add global results to the data\ndata.append({\n    'model' : 'global',\n    'lookback': '30',\n    'type': 'auto',\n    'mean_mae': global_results[0][0],\n    'mean_rmse': global_results[0][1]\n})\ndata.append({\n    'model' : 'global',\n    'lookback': '30',\n    'type': 'noauto',\n    'mean_mae': global_results[1][0],\n    'mean_rmse': global_results[1][1]\n})\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n\n    model lookback    type  mean_mae  mean_rmse\n0   local       15    auto    0.0150     0.0217\n1   local       15  noauto    0.0161     0.0228\n2   local       30    auto    0.0144     0.0212\n3   local       30  noauto    0.0142     0.0210\n4   local       45    auto    0.0149     0.0218\n5   local       45  noauto    0.0145     0.0211\n6  global       30    auto    0.0330     0.0450\n7  global       30  noauto    0.0330     0.0500\n\n\n\nComparison between local models.\nThe plot below shows the mean performances of all local models. The local model without autoregressive component and a lookback period of 30 was selected as the best model setup.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import cm\n\ndef plot_performance_data(mean_performance_data):\n    # Define colors and markers\n    colors = {\n        'auto': \"blue\",\n        'noauto': \"orange\"\n    }\n    shades = {\n        'results_l15': 0.4,\n        'results_l30': 0.7,\n        'results_l45': 1\n    }\n    markers = ['o', 's', 'D', '^']\n\n    # Create a new figure\n    plt.figure(figsize=(10, 6))\n\n    # Plot data for each key in the mean performance data\n    for idx, (key, performance) in enumerate(mean_performance_data.items()):\n        lookback, auto_type = key.rsplit('_', 1)\n        color = colors[auto_type]\n        shade = shades[lookback]\n\n        plt.scatter(performance['mean_mae'], performance['mean_rmse'], \n                    color=color, alpha=shade, label=key, s=100)\n\n\n\n    # Add labels and title\n    plt.xlabel('Mean MAE')\n    plt.ylabel('Mean RMSE')\n    plt.title('Mean Model Performance Comparison local Models', fontsize=16)\n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    plt.grid(True)\n    plt.tight_layout()\n\n    # Show plot\n    plt.show()\n\n# Call the function to plot the data\nplot_performance_data(mean_performance_data)"
  },
  {
    "objectID": "main_workflow.html#final-modelling",
    "href": "main_workflow.html#final-modelling",
    "title": "1  Full Workflow",
    "section": "1.6 Final Modelling",
    "text": "1.6 Final Modelling\nBased on the prelimary results a local model with a lookback of 30 time steps (30 * 8 days = 240 days) was chosen.\nThe modelling setup was the same as before, except that now the model was fitted for all locations and the number of hyperparameters was reduced. In the prelimnary results the hyperparameters of using 3 lstm layers, and relu as an activation function never were selected as the best hyperparameters. Therefore they were no longer considered.\nDetails: 5 Final Modelling\n\nCalculate mean RMSE and MAE\nThe RMSE and MAE show the model achieved a good fit with a mean RMSE of 0.02 and a mean MAE of 0.014.\n\n\nCode\n# Reading the results and evaluation from the file\nwith open(\"results/modelling/final/results_full_local_auto_l30.json\", 'r') as file:\n    results_dict = json.load(file)\n\nrmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\nmae_values = {loc: data['evaluation']['mae'] for loc, data in results_dict.items()}\n\nmean_rmse = np.mean(list(rmse_values.values()))\nmean_mae = np.mean(list(mae_values.values()))\n\nprint(f\"Mean RMSE: {mean_rmse:.3f}\")\nprint(f\"Mean MAE: {mean_mae:.3f}\")\n\n\nMean RMSE: 0.020\nMean MAE: 0.014\n\n\n\nPlotting timerseries for the best and worst performing models\nIn the plot below showing the two best and worst performing models, we can also that the forecasted values match well with the true values. Even so for the two worst performing models. We can however observe that steep increases or decreases in SIF were in many cases not well predicted.\n\n\nCode\n\nimport pickle\n\n# Function to load a generic object\ndef load_object(filename):\n    with open(filename, 'rb') as file:\n        return pickle.load(file)\n\ntest_index = load_object(\"results/modelling/final/test_index.pkl\")\n\n\n# Call the function with the example dictionary\nplot_forecasts_from_dict(results_dict, test_index)"
  },
  {
    "objectID": "main_workflow.html#conclusion",
    "href": "main_workflow.html#conclusion",
    "title": "1  Full Workflow",
    "section": "1.7 Conclusion",
    "text": "1.7 Conclusion\nWe showed that the SIF values in 2018 were lower than in the reference period 2000-2017. We also showed that a local model with a lookback of 30 time steps was the best model setup. Our results support that LSTM models are a good choice for predicting SIF values and that doing so using only climatic drivers is possible. Even for timesteps further in the future the model still produced good results. However it was observable that strong spikes and drops in SIF values were not well predicted. This area needs further research. Nonetheless these results are promising and the approach could be used to predict SIF values into the actual future, using climate forecasts. This could allow us to better quantify possible impacts of climate change on forests and how their contribution to the carbon cycle could change, based on different scenarios."
  },
  {
    "objectID": "quarto_files/01_load_aux_data.html#imports",
    "href": "quarto_files/01_load_aux_data.html#imports",
    "title": "2  Loading auxillary data",
    "section": "2.1 Imports",
    "text": "2.1 Imports\nHere we load packages and additionaly two functions from utils.py.\n\ncreate_paths: is used frequently over the project. It simply creates the paths reused over the analysis.\ncreate_cube_subset: this function creates the basic cube from the ESD-Cube (croping it in space, time and variables)\n\nimport os\nimport rioxarray as rio\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport ee\nimport geemap\n\n# load custom function from utils.py\nfrom utils import create_paths, create_cube_subset"
  },
  {
    "objectID": "quarto_files/01_load_aux_data.html#functions",
    "href": "quarto_files/01_load_aux_data.html#functions",
    "title": "2  Loading auxillary data",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nThis code stores 5 Functions:\n\ninitialize_gee: Authenticates and initializes the Google Earth Engine (GEE) API for further data processing\n\ndownload_german_border: Downloads the German border data from GEE and optionally exports it to a specified file path\n\nload_corine: Downloads and preprocesses CORINE land cover data for a specified region, exporting it to a file if needed\n\ncreate_sif_sample: Creates a sample TIFF file from the SIF data in the provided data cube subset and saves it to a specified output path\n\nload_aux_data: Integrates the auxiliary data loading process by initializing GEE, downloading the German border and CORINE data, and creating a SIF sample TIFF, managing file paths and downloads\n\ndef initialize_gee():\n    ee.Authenticate(force=False)\n    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n\n# Download German border data\ndef download_german_border(path, download=False):\n\n    print(\"Downloading German border data...\")\n\n    germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n       \n    germany_geometry = germany.geometry()\n\n    if download:\n        geemap.ee_export_vector(germany, filename=path)\n\n    print(100 * \"-\")\n\n    return germany_geometry\n\n\n# Download and preprocess Corine data\ndef load_corine(path, region, download=True):\n\n    print(\"Processing and downloading Corine data...\")\n\n    landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n\n    landcover_year = landcover_collection.filterDate(f'1999-01-01', f'2000-12-31').first()\n\n    zones = ee.Image(0) \\\n        .where(landcover_year.eq(311), 311) \\\n        .where(landcover_year.eq(312), 312) \\\n        .where(landcover_year.eq(313), 313)\n\n\n    if download:\n        geemap.ee_export_image(zones, filename=path, crs=\"EPSG:4326\", scale=500, region=region)\n\n    print(100 * \"-\")\n\n\n# Create sif sample tif for spatial resolution and transform\ndef create_sif_sample(out_path, cube_subset, write=True):\n\n    cube_sample = cube_subset[\"sif_gosif\"].isel(time=0)\n\n    if write:\n        cube_sample.rio.to_raster(out_path)\n\n    print(\"Sample path created at:\", out_path)\n\n    print(100 * \"-\")\n\n\n\n# Full function\ndef load_aux_data(data_path, cube_subset, download = True):\n\n    # Initialize GEE\n    initialize_gee()\n\n    # Create file paths and if they dont exist folders\n    germany_shp_path, corine_file_path, tif_sample_path, _, _ = create_paths(data_path=data_path)\n\n    # Download German border data \n    german_geometry = download_german_border(download=download, path=germany_shp_path)\n\n    # Download and preprocess Corine data and use germany_geometry to define the AOI\n    load_corine(path=corine_file_path, region=german_geometry, download=download)\n\n    # Create sif sample tif\n    create_sif_sample(out_path = tif_sample_path, cube_subset= cube_subset, write=download)"
  },
  {
    "objectID": "quarto_files/01_load_aux_data.html#processing",
    "href": "quarto_files/01_load_aux_data.html#processing",
    "title": "2  Loading auxillary data",
    "section": "2.3 Processing",
    "text": "2.3 Processing\nif __name__ == \"__main__\":\n\n    print(\"Loading auxiliary data...\")\n    \n    data_path = \"data\"\n    \n    os.makedirs(data_path, exist_ok=True)\n\n    # Create a subset of the Earth System Data Cube, containing only relevant variables and the desired spatial and temporal extent\n    cube_subset = create_cube_subset()\n\n    # Download auxiliary data (Germany border, Corine landcover data, sample tif)\n    load_aux_data(data_path, cube_subset, download = True)\n\n    print(100 * \"-\")"
  },
  {
    "objectID": "quarto_files/02_cube_preprocessing.html#imports-and-functions",
    "href": "quarto_files/02_cube_preprocessing.html#imports-and-functions",
    "title": "3  Preprocessing ESDC",
    "section": "3.1 Imports and Functions",
    "text": "3.1 Imports and Functions\n\ncalculate_forest_percentage: Computes the percentage of forest cover within a specified window of land cover data by identifying pixels that match predefined forest classes.\nresample_corine_to_sif: Resamples CORINE land cover data to match the resolution and dimensions of a sample SIF raster, calculating forest cover percentages for each resampled cell and returning a flipped array of these percentages.\ncube_preprocess: Clips a given data cube to the borders of Germany, calculates forest cover percentages over the grid, adds this data to the cube, creates a binary forest cover layer, and optionally writes the processed data to disk.\n\nimport xarray as xr\nimport rioxarray as rio\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport geopandas as gpd\nfrom utils import create_paths, create_cube_subset\n\n# Function to calculate forest percentages in a given window\ndef calculate_forest_percentage(lc_window, lc_data, forest_classes):\n    \"\"\"\n    Calculate the percentage of forest cover in a specified window.\n\n    Parameters:\n    lc_window (Window): The window of the land cover data to analyze.\n    lc_data (ndarray): The land cover data array.\n    forest_classes (list): List of land cover classes considered as forest.\n\n    Returns:\n    float: Percentage of forest cover in the specified window.\n    \"\"\"\n    forest_mask = np.isin(\n        lc_data[lc_window.row_off:lc_window.row_off + lc_window.height,\n                lc_window.col_off:lc_window.col_off + lc_window.width],\n        forest_classes\n    )\n\n    total_pixels = forest_mask.size\n    forest_pixels = np.sum(forest_mask)\n    percentage = (forest_pixels / total_pixels) * 100\n\n    return percentage\n\n# Function to calculate the forest percentages of the CORINE land cover data over the cube grid\ndef resample_corine_to_sif(corine_file_path, sample_path):\n    \"\"\"\n    Resample CORINE land cover data to match the resolution and dimensions of a sample SIF raster,\n    and calculate forest cover percentages for each resampled cell.\n\n    Parameters:\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n\n    Returns:\n    ndarray: Array of forest cover percentages for each resampled cell.\n    \"\"\"\n    # Open the land cover raster\n    with rasterio.open(corine_file_path) as src_lc:\n        lc_data = src_lc.read()\n        lc_transform = src_lc.transform\n\n    # Open the sample SIF raster\n    with rasterio.open(sample_path) as src_sif:\n        sif_transform = src_sif.transform\n        sif_meta = src_sif.meta\n\n    # Determine the new shape and transform for the resampled raster\n    new_height = sif_meta['height']\n    new_width = sif_meta['width']\n\n    # Initialize the new resampled data array\n    resampled_forest_percentage = np.zeros((new_height, new_width), dtype=np.float32)\n\n    # Define forest classes\n    forest_classes = [311, 312, 313]\n\n    # Calculate the window size in the original land cover data\n    window_height = int(abs(sif_transform[4] / lc_transform[4]))\n    window_width = int(abs(sif_transform[0] / lc_transform[0]))\n\n    # Loop through each cell in the SIF raster resolution\n    for i in range(new_height):\n        for j in range(new_width):\n            # Define the window in the land cover data\n            window = Window(col_off=j*window_width, row_off=i*window_height, width=window_width, height=window_height)\n            \n            # Calculate the forest percentage in the window\n            forest_percentage = calculate_forest_percentage(window, lc_data.squeeze(), forest_classes)\n            \n            # Assign the percentage to the resampled data array\n            resampled_forest_percentage[i, j] = forest_percentage\n\n    resampled_forest_percentage_flip = np.flipud(resampled_forest_percentage)\n\n    return resampled_forest_percentage_flip\n\ndef cube_preprocess(cube_subset, germany_gpd, corine_file_path, sample_path, out_path_crop, out_path_mask, all_touched=True, write=True):\n    \"\"\"\n    Preprocess the data cube by clipping to Germany's border, calculating forest cover percentages,\n    and adding this data to the cube. Optionally write the processed data to disk.\n\n    Parameters:\n    cube_subset (xarray.Dataset): The data cube subset to preprocess.\n    germany_gpd (GeoDataFrame): GeoDataFrame containing Germany's borders.\n    corine_file_path (str): Path to the CORINE land cover file.\n    sample_path (str): Path to the sample SIF raster file.\n    out_path_crop (str): Path to save the cropped data cube.\n    out_path_mask (str): Path to save the masked data cube.\n    all_touched (bool): Whether to include all pixels touched by the geometry. Defaults to True.\n    write (bool): Whether to write the output to disk. Defaults to True.\n\n    Returns:\n    xarray.Dataset: The processed data cube subset.\n    \"\"\"\n    print(\"Preprocessing cube\")\n\n    # Clip the xarray dataset using the Germany geometry\n    print(\"Clipping cube to Germany border\")\n    cube_subset_crop = cube_subset.rio.clip(\n        germany_gpd.geometry.values,\n        germany_gpd.crs,\n        drop=False, \n        all_touched=all_touched\n    )\n    \n    # Calculate forest cover percentage over cube grid\n    print(\"Calculate forest cover percentage over cube grid\")\n    resampled_forest_percentages = resample_corine_to_sif(corine_file_path, sample_path)\n\n\n    # Setup the dimensions for the resampled forest percentage\n    dims = ('lat', 'lon')  \n\n    # Add the resampled forest cover to the cube\n    cube_subset_crop['forest_cover'] = xr.DataArray(\n        resampled_forest_percentages, dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # Add a binary forest cover layer to the cube (0 for &lt;50% forest cover, 1 for &gt;=50% forest cover)\n    cube_subset_crop['forest_cover_50'] = xr.DataArray(\n        (resampled_forest_percentages &gt;= 50).astype(int), dims=dims, coords={dim: cube_subset_crop.coords[dim] for dim in dims}\n    )\n\n    # write just the percentages (only needed for worklfow demonstration)\n    cube_subset_crop.forest_cover.rio.to_raster(out_path_crop.replace(\".nc\", \"_percentages.tif\"))\n\n    # Mask the cube where forest cover is less than 50%\n    cube_subset_crop_mask = cube_subset_crop.where(cube_subset_crop['forest_cover_50'] == 1)\n\n    if write:\n        cube_subset_crop.to_netcdf(out_path_crop)\n        cube_subset_crop_mask.to_netcdf(out_path_mask)\n        print(\"Wrote cropped cube with added forest percentages and binary mask to disk at:\", out_path_crop)\n        print(\"Wrote cropped and masked cube to disk at:\", out_path_mask)\n                                       \n    return cube_subset_crop, cube_subset_crop_mask"
  },
  {
    "objectID": "quarto_files/02_cube_preprocessing.html#processing",
    "href": "quarto_files/02_cube_preprocessing.html#processing",
    "title": "3  Preprocessing ESDC",
    "section": "3.2 Processing",
    "text": "3.2 Processing\nif __name__ == \"__main__\":\n    from utils import create_cube_subset\n\n    data_path = \"data\"\n\n    # Load the cube subset\n    cube_subset = create_cube_subset()\n\n    # Create file paths and if they don't exist, create folders\n    germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the Germany border geometry\n    germany_gpd = gpd.read_file(germany_shp_path)\n\n    # Preprocess the cube\n    cube_preprocess(\n        cube_subset, germany_gpd, corine_file_path, tif_sample_path, \n        out_path_crop=cube_crop_path, out_path_mask=cube_crop_mask_path, \n        all_touched=True, write=True\n    )"
  },
  {
    "objectID": "quarto_files/03_base_analysis.html#imports-and-functions",
    "href": "quarto_files/03_base_analysis.html#imports-and-functions",
    "title": "4  Basic SIF Analysis",
    "section": "4.1 Imports and Functions",
    "text": "4.1 Imports and Functions\n\nplot_save_diff: Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two. Saves the figure to a specified path.\nplot_timeseries: Plots and saves the time series of SIF data within a specified time range. Optionally displays the plot.\nbase_analysis: Calculates the summer mean for each year in the dataset and the changes in SIF compared to the baseline period up to 2017. Returns the summer mean cube, the baseline mean to 2017, and the changes for specified years.\n\n# scripts/base_analysis.py\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport os\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom utils import create_paths\nimport matplotlib.gridspec as gridspec\n\n\ndef base_analysis(cube, years=[2018, 2019]):\n    \"\"\"\n    Perform the base analysis by calculating the summer mean for each year and the change compared to the baseline up to 2017.\n\n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    years : list, optional\n    \"\"\"\n\n\n    # Calculate summer mean for each year for the sif variable\n    summer_sif_data = cube.sif_gosif.sel(time=cube['time.season'] == 'JJA')\n    summer_sif_mean_cube = summer_sif_data.groupby('time.year').mean(dim='time')\n\n    # Calculate change in summer mean SIF for each year compared to baseline up to 2017\n    changes = {}\n    summer_mean_to_2017 = summer_sif_mean_cube.sel(year=slice(None, 2017)).mean(dim='year')\n\n    for year in years:\n        summer_mean = summer_sif_mean_cube.sel(year=year)\n        change = summer_mean - summer_mean_to_2017\n        changes[year] = change\n    \n\n    return summer_sif_mean_cube, summer_mean_to_2017, changes \n\n# Creates a figure with 2x2 subplots to visualize reference period data, 2018 data, and the difference between the two.\ndef change_plot(ref_period, data_2018, changes, save_path=None):\n    # Create the figure with GridSpec for custom layout\n    fig = plt.figure(figsize=(14, 7))\n    gs = gridspec.GridSpec(2, 5, width_ratios=[1, 0.05, 0.15, 1,0.05 ], wspace=0.3, hspace=0.3)\n\n    # Create the subplots\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[1, 0])\n    ax3 = fig.add_subplot(gs[:, 3])\n    cax = fig.add_subplot(gs[:, 1])\n    cax_2 = fig.add_subplot(gs[:, 4])\n\n    # Plot each dataset on a different subplot\n    img1 = ref_period.plot(ax=ax1, cmap=\"viridis\", vmin=0.1, vmax=0.5, add_colorbar=False)\n    ax1.set_title(\"Mean 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    ax1.set_xlabel(\"Longitude\", fontsize=12)\n    ax1.set_ylabel(\"Latitude\", fontsize=12)\n\n    img2 = data_2018.plot(ax=ax2, cmap=\"viridis\", vmin=0.1, vmax=0.5, add_colorbar=False)\n    ax2.set_title(\"Mean 2018\", fontsize=13, fontweight='bold', pad=15)\n    ax2.set_xlabel(\"Longitude\", fontsize=12)\n    ax2.set_ylabel(\"Latitude\", fontsize=12)\n\n    img3 = changes[2018].plot(ax=ax3, cmap=\"RdBu\", vmin=-0.15, vmax=0.15, add_colorbar=False)\n    ax3.set_title(\"Difference SIF 2018 to mean of 2002 - 2017\", fontsize=13, fontweight='bold', pad=15)\n    ax3.set_xlabel(\"Longitude\", fontsize=12)\n    ax3.set_ylabel(\"Latitude\", fontsize=12)\n\n    # Add a shared colorbar for the left plots\n    cbar = fig.colorbar(img1, cax=cax, orientation='vertical')\n    cbar.ax.tick_params(labelsize=12)\n    cbar.set_label(\"Sun-Induced Chlorophyll Fluorescence \\n [W m^-2 sr^-1 um^-1]\", fontsize=10)\n\n    # Add a shared colorbar for the left plots\n    cbar2 = fig.colorbar(img3, cax=cax_2, orientation='vertical')\n    cbar2.ax.tick_params(labelsize=12)\n    cbar2.set_label(\"Sun-Induced Chlorophyll - Change\", fontsize=10)\n    \n    ax1.set_xlabel(\"\")\n\n    if save_path:\n        # Save the plot\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n\n    plt.show()\n\n\n\n\n# Plotting a timeseries of the mean german SIF data\ndef plot_timeseries(cube_subset_mask, save_path, time_range=[None, None], show = False):\n    \"\"\"\n    Plot and save the timeseries of the SIF data.\n    \n    Parameters\n    ----------\n    cube : xarray.Dataset\n        The input cube containing the SIF data.\n    save_path : str \n        The path where the plot should be saved.\n    variable : str, optional\n        The variable to plot.\n    show : bool, optional\n        Whether to show the plot.\n    \n    \"\"\"\n\n    # only use sif variable\n    cube_subset_mask_sif = cube_subset_mask.sif_gosif\n\n    # Calculate the mean of the SIF data over germany\n    time_series = cube_subset_mask_sif.mean(dim=['lat', 'lon'])\n\n    # select range\n    time_series = time_series.sel(time=slice(time_range[0], time_range[1]))\n\n    plt.figure(figsize=(10, 6))\n    \n    time_series.plot(marker='o', color='blue', linestyle='dashed')\n\n    plt.title(f'Time Series of SIF', fontsize=14)\n    plt.xlabel('Time', fontsize=12)\n    plt.ylabel('Sun-Induced Chlorophyll Fluorescence at 757 nm \\n [W m^-2 sr^-1 um^-1]', fontsize=12)\n    plt.grid(True, which='major', axis='both')\n    #plt.ylim(.1, .6) \n            \n            \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    if show:\n        plt.show()"
  },
  {
    "objectID": "quarto_files/03_base_analysis.html#processing",
    "href": "quarto_files/03_base_analysis.html#processing",
    "title": "4  Basic SIF Analysis",
    "section": "4.2 Processing",
    "text": "4.2 Processing\ndef main():\n    \n    data_path = \"data\"\n\n    # Get path to the cube subset\n    _, _, _, cube_crop_path, cube_crop_mask_path = create_paths(data_path)\n\n    # Load masked cube subset (croped with forest mask and germany border)\n    cube_subset_mask = xr.open_dataset(cube_crop_mask_path)\n\n    # Create the results directory\n    os.makedirs(os.path.join(\"results\", \"figures\"), exist_ok=True)\n\n    # Save plot of timeseries:\n    plot_timeseries(cube_subset_mask, save_path = os.path.join(\"results\", \"figures\", \"timeseries_full.png\"))\n    plot_timeseries(cube_subset_mask, time_range= [\"2013-01-01\", \"2018-12-31\"], save_path = os.path.join(\"results\", \"figures\", \"timeseries_recent.png\"))\n\n    # Load croped cube subset (not masked yet)\n    cube_subset_crop = xr.open_dataset(cube_crop_path)\n\n    # Calculate the summer mean for each year and the change compared to the baseline up to 2017\n    summer_mean_cube, summer_mean_to_2017, changes = base_analysis(cube_subset_crop, years=[2018, 2019])\n\n    # Select only year 2018\n    summer_mean_2018 = summer_mean_cube.sel(year=2018)\n\n    # Create and save plot showing differences\n    save_path = os.path.join(\"results\", \"figures\", \"base_analysis.png\")\n    change_plot(summer_mean_to_2017,summer_mean_2018, changes, save_path)\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_files/04_test_modelling.html#imports",
    "href": "quarto_files/04_test_modelling.html#imports",
    "title": "5  Prelimnary Modelling",
    "section": "5.1 Imports",
    "text": "5.1 Imports\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom utils import create_paths, start_logging\nfrom config import variables\nfrom modelling_functions import  setup_model_data, save_results, full_modelling"
  },
  {
    "objectID": "quarto_files/04_test_modelling.html#processing",
    "href": "quarto_files/04_test_modelling.html#processing",
    "title": "5  Prelimnary Modelling",
    "section": "5.2 Processing",
    "text": "5.2 Processing\ndef main():\n\n    data_path = \"data\"\n   \n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n   \n    # do preprocessing: scaling and creating a dataframe, as well as getting the lat lon pairs defining all pixels\n    all_data_scaled, lat_lon_pairs, scalar_y = setup_model_data(cube_subset_crop_mask, variables)\n\n\n\n    ############  Modelling ############\n\n    # Create lookback array\n    look_backs = [15,30,45]\n\n    # Define the parameter grid for the local model\n    param_grid_local = {\n    'units_lstm': [64, 128],\n    'activation': ['relu', 'tanh'], \n    'epochs': [100],    \n    'learning_rate': [0.0001],\n    'dropout_rate': [0.2,0.4],\n    'batch_size': [25],\n    'num_lstm_layers': [1, 2, 3]\n    }\n\n\n\n    cv = TimeSeriesSplit(n_splits=3)\n    # Run the local models for a subset\n    # and saving the results - filename is created based on model type and lookback\n    for look_back in look_backs:\n        \n        # os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n        \n        # Create a log file: \n        #start_logging(os.path.join(\"results\", \"logs\", f\"local_models_{look_back}.log\"))\n\n        # not auto regressive\n        output_data_local_auto, _ = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=False, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n\n        save_results(output_data_local_auto, look_back, auto_regressive=False, global_model=False)\n\n        # auto regressive\n        output_data_local_noauto, _ = full_modelling(all_data_scaled, look_back, \n                        lat_lon_pairs, param_grid_local, scalar_y,\n                        auto_regressive=True, global_model=False,\n                        subset=True, n_subset=2, cv=cv)\n        \n        save_results(output_data_local_noauto, look_back, auto_regressive=True, global_model=False)\n\n\n    # Run the global model on the full dataset\n    # and saving the results - filename is created based on model type and lookback\n\n    # grid for global model\n    # The parameters were reduced based on a first run for the results of the local model\n\n    param_grid_global = {\n        'units_lstm': [64, 128],\n        'activation': ['tanh'], \n        'epochs': [100],    \n        'learning_rate': [0.0001],\n        'dropout_rate': [0.2],\n        'batch_size': [25],\n        'num_lstm_layers': [1, 2]\n    }\n\n    # The lookback is set to 30 for the global model, as it was the best performing for the local models\n    look_back = 30\n\n    # not auto regressive\n    output_data_global_auto, _ = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_auto, look_back, auto_regressive=False, global_model=False)\n\n    # auto regressive\n    output_data_global_noauto, _ = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_global, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv)\n\n    save_results(output_data_global_noauto, look_back, auto_regressive=True, global_model=False)\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_files/05_modelling.html#imports",
    "href": "quarto_files/05_modelling.html#imports",
    "title": "6  Final Modelling",
    "section": "6.1 Imports",
    "text": "6.1 Imports\nimport os\nimport rioxarray as rio\nimport xarray as xr\nimport logging\nimport pickle\nfrom sklearn.model_selection import TimeSeriesSplit\nimport pandas as pd\nfrom utils import start_logging, create_paths\nfrom config import variables, param_grid_final\nfrom modelling_functions import full_modelling, save_results\n\nfrom modelling_functions import setup_model_data"
  },
  {
    "objectID": "quarto_files/05_modelling.html#processing",
    "href": "quarto_files/05_modelling.html#processing",
    "title": "6  Final Modelling",
    "section": "6.2 Processing",
    "text": "6.2 Processing\n\n# Function to save a generic object\ndef save_object(obj, filename):\n    with open(filename, 'wb') as file:\n        pickle.dump(obj, file)\n        \ndef main():\n\n    data_path = \"data\"\n   \n    # Setup file paths\n    _, _, _, _, cube_crop_mask_path = create_paths(data_path=data_path)\n\n    # Load the croped cube (croped with forest mask and germany border)\n    cube_subset_crop_mask = xr.open_dataset(cube_crop_mask_path)\n   \n    # do preprocessing: scaling and creating a dataframe, as well as getting the lat lon pairs defining all pixels\n    all_data_scaled, lat_lon_pairs, scalar_y = setup_model_data(cube_subset_crop_mask, variables)\n\n\n    ############  Modelling ############\n\n    # Set lookback to 30 as it was the best performing in the test modelling\n    look_back = 30\n\n\n    # print grid searc parameter grid for the local model (can be found in config.py)\n    logging.info(print(param_grid_final))\n\n    cv = TimeSeriesSplit(n_splits=2)\n    \n    os.makedirs(os.path.join(\"results\",\"logs\"), exist_ok=True)\n\n\n    # Run the gridsearch gridsearch and training again with the local model without auto regression as it was the best perfoming model in 04_test_modelling.py\n    # Also relu is not used as activation function as tanh was performing better in every case\n            \n    # Create a log file: \n    start_logging(os.path.join(\"results\", \"logs\", f\"final_local_models_{look_back}2.log\"))\n\n    # not auto regressive\n    output_data_local_auto, test_index = full_modelling(all_data_scaled, look_back, \n                    lat_lon_pairs, param_grid_final, scalar_y,\n                    auto_regressive=False, global_model=False, cv=cv\n                                                       )\n\n    save_results(output_data_local_auto, look_back,\n                 auto_regressive=False, global_model=False,\n                 out_path=os.path.join(\"results\", \"modelling\", \"final\", \"results_full_local_auto_l30_test.json\"))\n    \n\n\n    test_index_path = os.path.join(\"results\", \"modelling\", \"final\", \"test_index.pkl\")\n    save_object(test_index,test_index_path)\n    \nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "quarto_files/config.html",
    "href": "quarto_files/config.html",
    "title": "Config",
    "section": "",
    "text": "This scripts stores callable variables, used in mutiple scripts.\n# scripts/config.py\n\n\nvariables = [\n    \"sif_gosif\",\n    \"evaporation_era5\",\n    \"precipitation_era5\",\n    \"radiation_era5\",\n    \"air_temperature_2m\",\n    \"max_air_temperature_2m\",\n    \"min_air_temperature_2m\",\n]\n\n\n# Define the time and spatial subset\nmin_time = '2002-01-01'\nmax_time = '2021-12-31'\nlon_min, lon_max = 5.866, 15.042\nlat_min, lat_max = 47.270, 55.058"
  },
  {
    "objectID": "quarto_files/utils.html",
    "href": "quarto_files/utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "These script stores some basic functions that are used in multiple scripts.\n\nconfigure_logging: Sets up logging to a specified file with INFO level.\nstart_logging: Resets logging configuration and sets it up with a new filename, ensuring a new log file is created when run in the same session.\ncreate_dir: Creates a directory if it does not already exist based on the provided file path.\ncreate_paths: Creates and returns paths for Germany’s border shapefile, CORINE land cover data for the year 2000, a SIF sample TIFF, the croped datacube, and the croped and masked datacube.\ncreate_cube_subset: Loads the Earth System Data Cube, subsets it based for the time range of sif and spatial boundaries of germany, selects the relevant variables, and adds the CRS (Coordinate Reference System).\n\n# scripts/utils.py\n\n\nfrom config import variables, min_time, max_time, lon_min, lon_max, lat_min, lat_max\nfrom xcube.core.store import new_data_store\nimport rioxarray as rio\nimport os\nimport logging\n\n\n\n# Function to setup logging to file\ndef configure_logging(filename):\n    logging.basicConfig(\n        filename=filename,\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n    )\n\n# Function to reset logging (so it creates a new file when run in the same session)\ndef start_logging(filename):\n    # Get the root logger\n    root_logger = logging.getLogger()\n    \n    # Remove all handlers associated with the root logger\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Configure logging with the new filename\n    configure_logging(filename)\n\n\ndef create_dir(file_path):\n    if not os.path.exists(os.path.dirname(file_path)):\n        os.makedirs(os.path.dirname(file_path))\n\n\ndef create_paths(data_path):\n\n    # Create path and folder for germany border shapefile\n    germany_shp_path = os.path.join(data_path, \"germany_shape\", 'germany_border.shp')\n    create_dir(germany_shp_path)\n\n    # Create path and folder for corine data year 2000\n    corine_file_path = os.path.join(data_path, \"landcover\", f\"forest_cover_2000.tif\")\n    create_dir(corine_file_path)\n\n    # Create path and folderfor sif sample tif\n    tif_sample_path= os.path.join(data_path, \"cubes\", \"cube_sif_sample.tif\")\n    create_dir(tif_sample_path)\n\n    cube_crop_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop.nc\")\n    cube_crop_mask_path = os.path.join(data_path, \"cubes\", \"cube_subset_crop_mask.nc\")\n\n    return germany_shp_path, corine_file_path, tif_sample_path, cube_crop_path, cube_crop_mask_path\n\n\ndef create_cube_subset(variables = variables, \n                        min_time = min_time, max_time = max_time,\n                        lon_min = lon_min, lon_max = lon_max,\n                        lat_min = lat_min, lat_max = lat_max):\n\n    # Initalize xcube store\n    store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n    store.list_data_ids()\n\n    # Load cube from store\n    cube = store.open_data( 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr')\n\n    # Subset the cube\n    cube_subset = cube.sel(time=slice(min_time, max_time)) \\\n                    .sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n\n    # select only specified variables from esdc\n    cube_subset = cube_subset[variables]\n\n    # Add crs to the cube\n    cube_subset.rio.write_crs(4326, inplace = True)\n\n    return cube_subset"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-mod_pre",
    "href": "quarto_files/modelling_functions.html#sec-mod_pre",
    "title": "Modelling functions",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nimport os\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom scikeras.wrappers import KerasRegressor\n\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    root_mean_squared_error,\n)\n\nimport json\nimport numpy as np\n\n\n\n\n########################################################\n# Data Preprocessing\n########################################################\n\n# Function to preprocess: scale and restructure the full dataset\ndef data_preprocess(df, variables):\n    \"\"\"\n    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n\n    Parameters:\n    df (DataFrame): Input DataFrame.\n    variables (list of str): Columns to normalize and convert to float32.\n    forest_vars (list of str): Columns to keep unscaled.\n\n    Returns:\n    DataFrame: Processed and normalized DataFrame.\n    \"\"\"\n    df.reset_index(inplace=True)\n    df.sort_values(\"time\", inplace = True)\n    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n    df[variables] = df[variables].astype(\"float32\")\n\n    # Scale the data using to a mean of 0 and standard div of 1\n    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n    scalar_x = StandardScaler()\n    scalar_y = StandardScaler()\n    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n\n    scaled_data = scalar_x.fit_transform(df[variables])\n    \n\n    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n\n    # Combine scaled variables with unscaled forest variables and other columns\n    scaled_df[\"time\"] = df[\"time\"].values\n    scaled_df[\"lat\"] = df[\"lat\"].values\n    scaled_df[\"lon\"] = df[\"lon\"].values\n\n    return scaled_df, scalar_x, scalar_y\n\n# Function to set up the model data - uses data_preprocess function.\ndef setup_model_data(cube_subset_crop_mask, variables):\n\n\n    # transform the cube to a dataframe\n    all_data_df = cube_subset_crop_mask.to_dataframe().dropna()\n\n    # Basic preprocessing - Scaling to mean 0 and std 1 \n    all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n\n    # based on the dataframe create a list of lat lon pairs, defining all timeseries (pixels)\n    lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n\n    return all_data_scaled, lat_lon_pairs, scalar_y"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-restruc",
    "href": "quarto_files/modelling_functions.html#sec-restruc",
    "title": "Modelling functions",
    "section": "Data restructuring and splitting",
    "text": "Data restructuring and splitting\n# Function to convert the DataFrame to 3D array for LSTM model training\ndef convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Convert the dataset into input features and target variable with specified look-back period.\n\n    Parameters:\n    data_arr (np.array): Input dataset with features and target in the last column.\n    look_back (int): Number of past observations each input sample should consist of.\n    target_col (string): Name of target variabel column.\n    exclude_cols (list): List of Strings containing the column names to be excluded.\n\n    Returns:\n    np.array, np.array: Arrays for input features (X) and target variable (Y).\n    \"\"\"\n    data_arr_x = data_arr.drop(columns=target_col)\n    data_arr_y = data_arr[target_col]\n\n    X, Y = [], []\n\n    # if auto-regressive model, we include the target variable in our predictors\n    # we need to shift the target variable by one timestep to use it as a feature\n    if auto_regressive:\n\n        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n        for i in range(1, len(data_arr_x) - look_back):\n            \n            # when modelling timestep t, d is t+1\n            d = i + look_back\n\n            x_seq = np.array(data_arr_x[i:d])\n\n            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n\n            assert x_seq.shape[0] == y_shifted.shape[0]\n\n            x_sequence = np.hstack([x_seq, y_shifted])\n\n            X.append(x_sequence)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    else:\n        for i in range(1, len(data_arr_x) - look_back):\n            d = i + look_back\n            x_seq = np.array(data_arr_x[i:d])\n            X.append(x_seq)\n            Y.append(data_arr_y.iloc[d - 1])\n\n    \n\n    return np.array(X), np.array(Y)\n\n\n# Function to split the data into training, validation, and test sets\ndef split_data(df_scaled, look_back,  lat_lon_pairs, lat = None, lon = None,global_model = False, target_col=\"sif_gosif\", auto_regressive = True):\n    \"\"\"\n    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n\n    Parameters:\n    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n    lat (float): Latitude to filter data.\n    lon (float): Longitude to filter data.\n    look_back (int): Number of past observations each input sample should consist of.\n\n    Returns:\n    tuple: Arrays of features and target variables for training, validation, and test datasets.\n    \"\"\"\n\n    # if the model is global, we store the indices of the lat_lon_pairs in a dictionary \n    if global_model:\n\n        df_scaled = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n\n        pixel_indices = {}\n\n\n        for idx, (lat, lon) in lat_lon_pairs.iterrows():\n            pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n            pixel_indices[(lat, lon)] = pixel_data.index\n\n    # if the model is local, we filter the data for the specific lat and lon\n    else:\n        df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n\n\n    # create an index based on the lookback period, so we can dynamically the data, using the lookback period\n    # we do this as we want our test data to start at 2018, but we need lookback timesteps before to model the first timestep in 2018\n    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n    val_end_index =  first_index_2017 + look_back\n\n    # split the data into training, validation and test data\n    train_data = df_scaled[df_scaled[\"time\"].dt.year &lt;= 2014]\n    \n    val_data = df_scaled[\n        (df_scaled[\"time\"].dt.year == 2015) | \n        (df_scaled[\"time\"].dt.year == 2016) | \n        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index &lt; val_end_index))\n        ]\n\n    test_data = df_scaled[\n        (df_scaled.index &gt;= val_end_index) |\n        (df_scaled[\"time\"].dt.year &gt;= 2018)\n        ]\n\n    # drop features not wanted for modelling\n    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n\n\n    # Create modelling samples, either with or without auto_regressive component\n    trainX, trainY = convert_to_matrix(train, look_back, target_col, auto_regressive=auto_regressive)\n    valX, valY = convert_to_matrix(val, look_back, target_col, auto_regressive=auto_regressive)\n    testX, testY = convert_to_matrix(test, look_back, target_col, auto_regressive=auto_regressive)\n\n    # Reshape the data for LSTM model\n    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n\n    # Get a time index for the test data (used for plotting)\n    test_index = sorted(list(set(test_data.time)))\n\n    # Return the data\n    if global_model:\n        return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n    else:\n        return trainX, trainY, valX, valY, testX, testY, test_index"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-model",
    "href": "quarto_files/modelling_functions.html#sec-model",
    "title": "Modelling functions",
    "section": "Modelling with GridSearchCV",
    "text": "Modelling with GridSearchCV\n############  Create LSTM model structure ############\ndef create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n    \"\"\"\n    Create an LSTM model with the specified hyperparameters.\n    \n    Parameters:\n    look_back (int): The number of previous time steps to use as input.\n    features (int): The number of features in the input data.\n    units_lstm (int): Number of units in the LSTM layer(s).\n    activation (str): Activation function to use.\n    learning_rate (float): Learning rate for the optimizer.\n    dropout_rate (float): Dropout rate to use after LSTM layers.\n    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n\n    Returns:\n    model (Sequential): The compiled Keras model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(look_back, features)))\n\n    if num_lstm_layers == 1:\n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n    elif num_lstm_layers == 2:\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n       \n        model.add(LSTM(units_lstm, activation=activation))\n\n    elif num_lstm_layers == 3:\n    \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n        \n        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n\n\n\n    model.add(Dense(1, activation='linear'))\n\n    opt = Adam(learning_rate=learning_rate)\n  \n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    return model\n\n\n# Function to create a KerasRegressor for GridSearchCV\ndef create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n    return KerasRegressor(\n        model=create_lstm_model,\n        look_back=look_back,\n        features = features,\n        units_lstm=units_lstm, \n        learning_rate=learning_rate, \n        dropout_rate=dropout_rate, \n        num_lstm_layers=num_lstm_layers,  \n        activation=activation, \n        optimizer=optimizer,\n        verbose = 0\n    )\n\n\n############ Function to perform grid search cv ############\n\ndef perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv):\n\n    \"\"\"\n    Perform grid search to find the best hyperparameters for the LSTM model.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n\n    Returns:\n    - best_params: Best hyperparameters found by the grid search.\n    \"\"\"\n    # Get the number of features\n    features = trainX.shape[2]\n\n    # Create a KerasRegressor\n    model = create_keras_regressor(look_back, features)\n\n    # Define GridSearchCV\n    lstm_grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=cv,\n        scoring=\"neg_mean_squared_error\",\n        verbose=2,\n        n_jobs=-1,\n    )\n\n    # Define Early Stopping condition\n    callback = EarlyStopping(monitor='val_loss', patience=5)\n    \n    # Perform grid search\n    lstm_grid_search.fit(\n        trainX,\n        trainY,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=0,\n        callbacks=[callback],\n        shuffle=False,\n    )\n\n    # Return the best parameters from the grid search\n    return lstm_grid_search.best_params_"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-pred",
    "href": "quarto_files/modelling_functions.html#sec-pred",
    "title": "Modelling functions",
    "section": "Predicting",
    "text": "Predicting\n# Iterative prediction and substitution (in the case of auto_regressive model, otherwise just predict)\ndef predict_replace(model, X_test, look_back, auto_regressive = True):\n    \"\"\"\n    Generates predictions and updates the test set input for iterative forecasting.\n\n    Parameters:\n    model (keras.Model): Trained LSTM model.\n    X_test (array): Test data to predict.\n\n    Returns:\n    np.array: Array of forecasted values.\n    \"\"\"\n    forecasts = []\n    \n    # sequentially replace shifted sif data (in X_test) by forecasts \n    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n    \n    if auto_regressive:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n            if i &lt; len(X_test) - 1:\n                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n                X_test[i + 1, -1, -1] = forecast[0][0]\n    \n    else:\n        for i in range(len(X_test)):\n            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n            forecasts.append(forecast[0][0])\n\n    forecasts_array = np.array(forecasts)\n\n\n    return forecasts_array"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-eval",
    "href": "quarto_files/modelling_functions.html#sec-eval",
    "title": "Modelling functions",
    "section": "Fit model with best params and evaluating",
    "text": "Fit model with best params and evaluating\ndef fit_evaluate_model(trainX, trainY, valX, valY, testX, testY, look_back, features, best_params, scalar_y, auto_regressive):\n    \"\"\"\n    Train and evaluate the LSTM model with the best hyperparameters.\n\n    Parameters:\n    - trainX: Training features.\n    - trainY: Training labels.\n    - valX: Validation features.\n    - valY: Validation labels.\n    - testX: Testing features.\n    - testY: Testing labels.\n    - look_back: Number of previous time steps to consider for prediction.\n    - features: Number of features in the input data.\n    - best_params: Best hyperparameters found by the grid search.\n    - scalar_y: Scaler for the output variable.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n\n    Returns:\n    - model_results: Dictionary containing the evaluation results and model history.\n    \"\"\"\n\n    # Create LSTM model with the best hyperparameters\n    lstm_model = create_lstm_model(\n        look_back=look_back,\n        features=features,\n        units_lstm=best_params['units_lstm'],\n        activation=best_params['activation'],\n        learning_rate=best_params['learning_rate'],\n        dropout_rate=best_params['dropout_rate'],\n        num_lstm_layers=best_params['num_lstm_layers']\n    )\n\n    # Fit the model\n    history = lstm_model.fit(\n        trainX,\n        trainY,\n        epochs=best_params['epochs'],\n        batch_size=best_params['batch_size'],\n        verbose=1,\n        validation_data=(valX, valY)\n    )\n\n    # Predict - with replacement of shifted target_variable in predictor set in case of an auto_regressive model\n    forecasts = predict_replace(lstm_model, testX, look_back, auto_regressive=auto_regressive)\n\n    # Rescale the data before evaluation\n    testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n    forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n\n    # Evaluate model performance\n    rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n    mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n\n    # Return the evaluation results and model history\n    return {\n        \"best_params\": best_params,\n        \"look_back\": look_back,\n        \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n        \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n        \"history\": history.history\n    }"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-write",
    "href": "quarto_files/modelling_functions.html#sec-write",
    "title": "Modelling functions",
    "section": "Write results",
    "text": "Write results\n# Convert the model results to a serializable format so its writeable as a json\ndef convert_to_serializable(obj):\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()  # Convert numpy arrays to lists\n    elif isinstance(obj, np.generic):\n        return obj.item()  # Convert numpy scalar types to Python scalars\n    elif isinstance(obj, dict):\n        # Recursively convert each item in the dictionary\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        # Recursively convert each item in the list\n        return [convert_to_serializable(v) for v in obj]\n    return obj  # Return the object if it's already serializable\n\n# Save the model evaluation results to a JSON file\ndef save_results(output_data, look_back, global_model=False, auto_regressive=False, out_path = None):\n    \"\"\"\n    Save the model evaluation results to a JSON file.\n\n    Parameters:\n    - output_data: Dictionary containing the model results.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    - global_model: Boolean indicating if the model is a global model.\n    \"\"\"\n\n    # Convert the entire data dictionary to a serializable format\n    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n\n    # Construct the output file path\n    folder_name_json = os.path.join(\"results\",\"modelling\", f\"results_l{look_back}\")\n    os.makedirs(folder_name_json, exist_ok=True)\n\n        \n    \n\n    # Determine the file name based on whether the model is global or local and auto_regressive or not\n    auto_string = \"auto\" if auto_regressive else \"noauto\"\n    glob_string = \"global\" if global_model else \"local\"\n    file_name_json = f\"test_results_{glob_string}_{auto_string}_l{look_back}.json\"\n    \n    output_json_file = os.path.join(folder_name_json, file_name_json)\n\n    if out_path:\n        output_json_file = out_path\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # Write the results to the JSON file\n    with open(output_json_file, \"w\") as file:\n        json.dump(output_data_serializable, file, indent=4)\n    \n    logging.info(f\"Results and evaluation written to: {output_json_file}\")"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-full",
    "href": "quarto_files/modelling_functions.html#sec-full",
    "title": "Modelling functions",
    "section": "Full modelling workflow function",
    "text": "Full modelling workflow function\n# Function to train and evaluate global or local LSTM models with or without auto_regressive component \ndef full_modelling(df_scaled, look_back, lat_lon_pairs, param_grid, scalar_y,\n              epochs=100, \n              batch_size=32, \n              cv=TimeSeriesSplit(n_splits=3),\n              auto_regressive=False,\n              global_model=False,\n              lat=None, lon=None, \n              subset = False, n_subset = None):\n    \"\"\"\n    Function to train and evaluate global or local LSTM models with or without auto_regressive component.\n\n    Parameters:\n    - df_scaled: Scaled input dataframe.\n    - look_back: Number of previous time steps to consider for prediction.\n    - lat_lon_pairs: List of (lat, lon) tuples for local models.\n    - param_grid: Grid of hyperparameters for the grid search.\n    - scalar_y: Scaler for the output variable.\n    - epochs: Number of epochs for training.\n    - batch_size: Batch size for training.\n    - cv: Cross-validation splitting strategy.\n    - auto_regressive: Boolean indicating if the model is auto_regressive.\n    - global_model: Boolean indicating if the model is a global model.\n    - lat: Latitude (for local models).\n    - lon: Longitude (for local models).\n    \"\"\"\n\n    output_data = {}\n\n    if global_model:\n        # Training a global model\n        logging.info(\"Starting Global Model Training\")\n\n        # Split data for the global model\n        trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data(\n            df_scaled, look_back=look_back, lat_lon_pairs=lat_lon_pairs,\n            auto_regressive=auto_regressive, global_model=global_model\n        )\n\n        # Perform grid search to find the best hyperparameters\n        best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n        # Train and evaluate the model\n        model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                        look_back, trainX.shape[2], \n                                        best_params, scalar_y, \n                                        auto_regressive)\n\n        # Store the results\n        output_data = model_results\n\n    else:\n        \n        # if subset, draw 10 random lat_lon_pairs with the same seed\n        if subset:\n            lat_lon_pairs = lat_lon_pairs.sample(n=n_subset, random_state=42)\n\n        n_lat_lon_pairs = len(lat_lon_pairs)\n\n        # Training local models for each pixel\n        for i, row in enumerate(lat_lon_pairs.iterrows()):\n            lat = row[1]['lat']\n            lon = row[1]['lon']\n\n            logging.info(f\"Starting Model Training for \\n lat: {lat}\\n lon: {lon}\")\n\n            # Split data for the specific latitude and longitude\n            trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n                df_scaled, look_back=look_back, lat_lon_pairs = lat_lon_pairs, lat=lat, lon=lon,  \n                auto_regressive=auto_regressive, global_model=global_model\n            )\n\n            # Perform grid search to find the best hyperparameters\n            best_params = perform_grid_search(trainX, trainY, look_back, param_grid, epochs, batch_size, cv)\n\n            # Train and evaluate the model\n            model_results = fit_evaluate_model(trainX, trainY, valX, valY, testX, testY,\n                                           look_back, trainX.shape[2], \n                                           best_params, scalar_y, auto_regressive)\n\n            # Store the results for the specific latitude and longitude\n            output_data[(lat, lon)] = model_results\n\n            logging.info(f\"Iteration {i+1}/{n_lat_lon_pairs}: lat = {lat}, lon = {lon}\")\n\n            logging.info(100*\"-\")\n\n    logging.info(100*\"-\")\n\n    return output_data, test_index"
  },
  {
    "objectID": "quarto_files/modelling_functions.html#sec-plot",
    "href": "quarto_files/modelling_functions.html#sec-plot",
    "title": "Modelling functions",
    "section": "Plotting",
    "text": "Plotting\n# Function to plot predicted vs. actual values for the best and worst performing models\ndef plot_forecasts_from_dict(results_dict, test_index):\n    \"\"\"\n    This function takes a dictionary containing model results and creates plots for the two locations\n    with the highest and two locations with the lowest RMSE values.\n\n    Args:\n    results_dict (dict): Dictionary containing model results with true and predicted values.\n\n    Returns:\n    None\n    \"\"\"\n    # Extract RMSEs and corresponding locations\n    rmse_values = {loc: data['evaluation']['rmse'] for loc, data in results_dict.items()}\n\n    # Sort locations by RMSE\n    sorted_locations = sorted(rmse_values, key=rmse_values.get)\n\n    # Find two locations with highest and two with lowest RMSE\n    lowest_rmse_locations = sorted_locations[:2]\n    highest_rmse_locations = sorted_locations[-2:]\n\n    # Function to plot true vs predicted values\n    def plot_forecast(true_values, predicted_values, title, ax, test_index):\n        line1, = ax.plot(test_index, true_values, label='True Values', color='blue',alpha=0.8, linewidth=2)\n        line2, = ax.plot(test_index, predicted_values, label='Predicted Values', color='red', alpha=1, linestyle='--', linewidth=2)\n        ax.set_title(title, fontsize=14, y = 1.05)\n        ax.set_xlabel('Time', fontsize=12)\n        ax.set_ylabel('SIF', fontsize=12)\n        ax.grid(True)\n        return line1, line2\n\n\n    fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n\n    # Plot for locations with lowest RMSE\n    lines = []\n    for i, loc in enumerate(lowest_rmse_locations):\n        rmse = np.round(results_dict[loc]['evaluation']['rmse'],3)\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        line1, line2 = plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Lowest (best) RMSE prediction (RMSE: {rmse})'if i == 0 else f'2nd Lowest (best) RMSE prediction (RMSE: {rmse})',\n            ax=axs[0, i],\n            test_index = test_index\n        )\n        lines.append((line1, line2))\n\n    # Plot for locations with highest RMSE\n    for i, loc in enumerate(highest_rmse_locations):\n        rmse = np.round(results_dict[loc]['evaluation']['rmse'],3)\n        true_values = results_dict[loc]['results']['true_values']\n        predicted_values = results_dict[loc]['results']['predicted_values']\n        line1, line2 = plot_forecast(\n            [val[0] for val in true_values],\n            [val[0] for val in predicted_values],\n            f'Highest (worst) RMSE prediction (RMSE: {rmse})' if i == 0 else f'2nd Highest (worst) RMSE prediction(RMSE: {rmse})',\n            ax=axs[1, i],\n            test_index = test_index\n        )\n        lines.append((line1, line2))\n\n    # Add a main title\n    fig.suptitle('Forecasted vs True Values', fontsize=16, y=0.95)\n\n\n\n    # Add a single shared legend\n    fig.legend([lines[0][0], lines[0][1]], ['True Values', 'Predicted Values'], loc='lower center', bbox_to_anchor=(0.5, 0.05), ncol=2,\n               fontsize = 14)\n\n    # Adjust layout and show the plot\n    plt.tight_layout(rect=[0, 0.1, 1, 0.93])\n    plt.show()"
  },
  {
    "objectID": "quarto_files/DMP.html#fair-data",
    "href": "quarto_files/DMP.html#fair-data",
    "title": "Data Summary",
    "section": "FAIR Data",
    "text": "FAIR Data\n\nFindable\n\nClimate Data for the analysis\n\nThe forecasting task is completed by training a model on ERA5 climate data, which is accessible within the Earth System Data Cube, provided by the Earth System Data Lab. Link to the data: Earth System Data Cube. The original ERA-5 data was created by Hersbach et al (Hersbach et al. 2020).\n\n\n\nSun Induced Fluoreseence Data\n\nThe Sun induced fluoresence data for this project was created by Xing Li and Jingfeng Xiao (Li and Xiao 2019). It is also accessible throught the Earh System Data Lab.\n\n\n\nAuxiliary data\n\nGerman border shapefile\n\nIn this project, the GAUL 2015 Level 0 dataset from the Food and Agriculture Organization (FAO, 2015) is used to get the administrative unit of Germany as a shapefile.\n\nForest cover\n\nCORINE Land Cover 2000 (CLC2000) acquired by Landsat-7 ETM, was used to extract forest pixels. The full dataset can be found here.\n\n\n\n\n\nAccessible\nAll data used in this project is free of charge and openly accessible (through the links provided).\n\nThe GAUL data is freely accessible through the FAO’s Geospatial Data platform, ensuring that users can easily download and use the data for various applications.\nThe CLC programme is a part of the Copernicus Land Monitoring Service (Copernicus) run by the European Commission and the European Environment Agency. Access to this data is based on a principle of full, open, and free access as established by the Copernicus data and information policy Regulation (EU) No 1159/2013 of 12 July 2013. This regulation establishes registration and licensing conditions for GMES/Copernicus users.\nERA5 data can be accessed through the Climate Data Store (CDS) managed by the Copernicus Climate Change Service (C3S). Users can freely access a comprehensive range of meteorological data in this catalog. For the project all data was accessible through the Earth System Data Cube.\n\n\n\nInteroperable\n\nStandardized Data Formats\n\nThe utilized data is stored in standard formats such as .nc, .tif, and .shp, each documented with metadata.\n\nData Generation with Python\n\nThe generation of new data employed the freely available programming language Python and its open-source libraries.\n\nStudy Area Mask Standardization\n\nThe study area mask was created in the standard .shp format, ensuring reproducibility with the freely available software QGIS.\n\n\n\n\nRe-use\n\nLicenses\n\nThe FAO GAUL 2015 Level 0 data is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 IGO (CC BY-NC-SA 3.0 IGO) license. This means one is free to share and adapt the data, provided one gives appropriate credit. It is not meant for commercial purposes.\nThe CLC2000 dataset is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.\nThe ERA5 dataset used in the Earth System Data Cube is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0).\n\n\n\nData Quality Assurance\n\nERA5: Detailed documentation and validation reports are available, providing insights into the QA measures taken to ensure data quality (ERA5 Documentation).\nFAO GAUL: The dataset’s methodology and technical aspects are well-documented, and a disclaimer accompanies its use to inform users of any potential limitations (FAO GAUL Documentation).\nCLC2000: The EEA provides extensive documentation, including technical guidelines, validation reports, and metadata, detailing the QA processes implemented during the production of the CLC datasets (CLC2000 Documentation)."
  },
  {
    "objectID": "quarto_files/DMP.html#allocation-of-resources",
    "href": "quarto_files/DMP.html#allocation-of-resources",
    "title": "Data Summary",
    "section": "Allocation of Resources",
    "text": "Allocation of Resources\nDue to the high computational demands of the project, it was dependent on resource sponsorship. Thanks to the European Space Agency’s Network of Resources (ESA NoR), the authors were granted access to their computing cluster, valued at €2,495.00, for a period of five months."
  },
  {
    "objectID": "quarto_files/DMP.html#data-security",
    "href": "quarto_files/DMP.html#data-security",
    "title": "Data Summary",
    "section": "Data Security",
    "text": "Data Security\nTo ensure data security, the data is stored both locally and in a repository. Additionally, the data is also stored on hardware. Currently, the data is stored in a non-certified repository."
  },
  {
    "objectID": "quarto_files/DMP.html#ethical-aspects",
    "href": "quarto_files/DMP.html#ethical-aspects",
    "title": "Data Summary",
    "section": "Ethical Aspects",
    "text": "Ethical Aspects\nThere are no ethical or legal issues that can have an impact on data sharing.\n\n\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The ERA5 Global Reanalysis.” Quarterly Journal of the Royal Meteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product of Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  },
  {
    "objectID": "quarto_files/references.html",
    "href": "quarto_files/references.html",
    "title": "References",
    "section": "",
    "text": "European Environment Agency. 2019. “CORINE Land Cover 2000 (Raster\n100 m), Europe, 6-Yearly - Version 2020_20u1, May\n2020.” European Environment Agency. https://doi.org/10.2909/ddacbd5e-068f-4e52-a596-d606e8de7f40.\n\n\nHersbach, Hans, Bill Bell, Paul Berrisford, Shoji Hirahara, András\nHorányi, Joaquín Muñoz–Sabater, Julien Nicolas, et al. 2020. “The\nERA5 Global Reanalysis.” Quarterly Journal of the Royal\nMeteorological Society 146 (730): 1999–2049. https://doi.org/10.1002/qj.3803.\n\n\nLi, Xing, and Jingfeng Xiao. 2019. “A Global, 0.05-Degree Product\nof Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and\nReanalysis Data.” Remote Sensing 11 (5): 517. https://doi.org/10.3390/rs11050517."
  }
]