{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 14:16:25.706295: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-25 14:16:25.708886: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-25 14:16:25.746530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 14:16:26.636018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "import logging\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(filename):\n",
    "    logging.basicConfig(\n",
    "        filename=filename,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    )\n",
    "\n",
    "def reset_logging(filename):\n",
    "    # Get the root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    \n",
    "    # Remove all handlers associated with the root logger\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "    \n",
    "    # Configure logging with the new filename\n",
    "    configure_logging(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Delete\n",
    "# with forest classes\n",
    "def data_preprocess(df, variables, forest_vars):\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    variables (list of str): Columns to normalize and convert to float32.\n",
    "    forest_vars (list of str): Columns to keep unscaled.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed and normalized DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(inplace=False)\n",
    "    df = df.sort_values(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df[variables] = df[variables].astype(\"float32\")\n",
    "\n",
    "    # Scale only the specified variables\n",
    "    scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler_minmax.fit_transform(df[variables])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n",
    "\n",
    "    # Combine scaled variables with unscaled forest variables and other columns\n",
    "    all_data_scaled = scaled_df.copy()\n",
    "    all_data_scaled[forest_vars] = df[forest_vars].values\n",
    "    all_data_scaled[\"time\"] = df[\"time\"].values\n",
    "    all_data_scaled[\"lat\"] = df[\"lat\"].values\n",
    "    all_data_scaled[\"lon\"] = df[\"lon\"].values\n",
    "\n",
    "    return all_data_scaled, scaler_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, variables):\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    variables (list of str): Columns to normalize and convert to float32.\n",
    "    forest_vars (list of str): Columns to keep unscaled.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed and normalized DataFrame.\n",
    "    \"\"\"\n",
    "    df.reset_index(inplace=True)\n",
    "    df.sort_values(\"time\", inplace = True)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df[variables] = df[variables].astype(\"float32\")\n",
    "\n",
    "    # Scale the data using to a mean of 0 and standard div of 1\n",
    "    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n",
    "    scalar_x = StandardScaler()\n",
    "    scalar_y = StandardScaler()\n",
    "    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n",
    "\n",
    "    scaled_data = scalar_x.fit_transform(df[variables])\n",
    "    \n",
    "\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n",
    "\n",
    "    # Combine scaled variables with unscaled forest variables and other columns\n",
    "    scaled_df[\"time\"] = df[\"time\"].values\n",
    "    scaled_df[\"lat\"] = df[\"lat\"].values\n",
    "    scaled_df[\"lon\"] = df[\"lon\"].values\n",
    "\n",
    "    return scaled_df, scalar_x, scalar_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", autoregressive = True):\n",
    "    \"\"\"\n",
    "    Convert the dataset into input features and target variable with specified look-back period.\n",
    "\n",
    "    Parameters:\n",
    "    data_arr (np.array): Input dataset with features and target in the last column.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "    target_col (string): Name of target variabel column.\n",
    "    exclude_cols (list): List of Strings containing the column names to be excluded.\n",
    "\n",
    "    Returns:\n",
    "    np.array, np.array: Arrays for input features (X) and target variable (Y).\n",
    "    \"\"\"\n",
    "    data_arr_x = data_arr.drop(columns=target_col)\n",
    "    data_arr_y = data_arr[target_col]\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    if autoregressive:\n",
    "\n",
    "        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n",
    "        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n",
    "        for i in range(1, len(data_arr_x) - look_back):\n",
    "            \n",
    "            # when modelling timestep t, d is t+1\n",
    "            d = i + look_back\n",
    "\n",
    "            x_seq = np.array(data_arr_x[i:d])\n",
    "\n",
    "            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n",
    "\n",
    "            assert x_seq.shape[0] == y_shifted.shape[0]\n",
    "\n",
    "            x_sequence = np.hstack([x_seq, y_shifted])\n",
    "\n",
    "            X.append(x_sequence)\n",
    "            Y.append(data_arr_y.iloc[d - 1])\n",
    "\n",
    "    else:\n",
    "        for i in range(1, len(data_arr_x) - look_back):\n",
    "            d = i + look_back\n",
    "            x_seq = np.array(data_arr_x[i:d])\n",
    "            X.append(x_seq)\n",
    "            Y.append(data_arr_y.iloc[d - 1])\n",
    "\n",
    "    \n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(df_scaled, lat, lon, look_back, target_col=\"sif_gosif\"):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    lat (float): Latitude to filter data.\n",
    "    lon (float): Longitude to filter data.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "\n",
    "    train_data = df_scaled[df_scaled[\"time\"].dt.year <= 2015]\n",
    "    val_data = df_scaled[\n",
    "        (df_scaled[\"time\"].dt.year == 2016) | (df_scaled[\"time\"].dt.year == 2017)\n",
    "    ]\n",
    "    test_data = df_scaled[(df_scaled[\"time\"].dt.year >= 2018)]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    # Extend the validation and test sets by the look-back period to include necessary preceding time steps\n",
    "    if not train_data.empty:\n",
    "        val = pd.concat([train.iloc[-(look_back):], val])\n",
    "    if not val_data.empty:\n",
    "        test = pd.concat([val.iloc[-(look_back):], test])\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_scaled, lat, lon, look_back, target_col=\"sif_gosif\", autoregressive = True):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    lat (float): Latitude to filter data.\n",
    "    lon (float): Longitude to filter data.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "\n",
    "    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n",
    "    val_end_index =  first_index_2017 + look_back\n",
    "\n",
    "\n",
    "    train_data = df_scaled[df_scaled[\"time\"].dt.year <= 2014]\n",
    "    \n",
    "    val_data = df_scaled[\n",
    "        (df_scaled[\"time\"].dt.year == 2015) | \n",
    "        (df_scaled[\"time\"].dt.year == 2016) | \n",
    "        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index < val_end_index))\n",
    "        ]\n",
    "\n",
    "    test_data = df_scaled[\n",
    "        (df_scaled.index >= val_end_index) |\n",
    "        (df_scaled[\"time\"].dt.year >= 2018)\n",
    "        ]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col, autoregressive=autoregressive)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col, autoregressive=autoregressive)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col, autoregressive=autoregressive)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_lstm_model(look_back, features, units_lstm=50, units_dense=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Create an LSTM model with the specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    look_back (int): The number of previous time steps to use as input.\n",
    "    features (int): The number of features in the input data.\n",
    "    units_lstm (int): Number of units in the LSTM layer(s).\n",
    "    units_dense (int): Number of units in the Dense layer.\n",
    "    activation (str): Activation function to use.\n",
    "    optimizer (str): Optimizer to use ('adam' or 'rmsprop').\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    dropout_rate (float): Dropout rate to use after LSTM layers.\n",
    "    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back, features)))\n",
    "\n",
    "    if num_lstm_layers == 1:\n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 2:\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "       \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 3:\n",
    "    \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "    # model.add(Dense(units_dense, activation=activation))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n",
    "    \"\"\"\n",
    "    Create an LSTM model with the specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    look_back (int): The number of previous time steps to use as input.\n",
    "    features (int): The number of features in the input data.\n",
    "    units_lstm (int): Number of units in the LSTM layer(s).\n",
    "    activation (str): Activation function to use.\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    dropout_rate (float): Dropout rate to use after LSTM layers.\n",
    "    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back, features)))\n",
    "\n",
    "    if num_lstm_layers == 1:\n",
    "        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 2:\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "       \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "\n",
    "    elif num_lstm_layers == 3:\n",
    "    \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "  \n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a KerasRegressor for GridSearchCV\n",
    "def create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n",
    "    return KerasRegressor(\n",
    "        model=create_lstm_model,\n",
    "        look_back=look_back,\n",
    "        features = features,\n",
    "        units_lstm=units_lstm, \n",
    "        learning_rate=learning_rate, \n",
    "        dropout_rate=dropout_rate, \n",
    "        num_lstm_layers=num_lstm_layers,  \n",
    "        activation=activation, \n",
    "        optimizer=optimizer,\n",
    "        verbose = 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterative prediction and substitution\n",
    "def predict_replace(model, X_test, autoregressive = True):\n",
    "    \"\"\"\n",
    "    Generates predictions and updates the test set input for iterative forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.Model): Trained LSTM model.\n",
    "    X_test (array): Test data to predict.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Array of forecasted values.\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    \n",
    "    # sequentially replace shifted sif data (in X_test) by forecasts \n",
    "    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n",
    "    \n",
    "    if autoregressive:\n",
    "        for i in range(len(X_test)):\n",
    "            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n",
    "            forecasts.append(forecast[0][0])\n",
    "            if i < len(X_test) - 1:\n",
    "                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n",
    "                X_test[i + 1, -1, -1] = forecast[0][0]\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(X_test)):\n",
    "            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n",
    "            forecasts.append(forecast[0][0])\n",
    "\n",
    "    forecasts_array = np.array(forecasts)\n",
    "\n",
    "\n",
    "    return forecasts_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating - Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Delete\n",
    "\n",
    "def plot_training(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and metrics from the training history.\n",
    "\n",
    "    Parameters:\n",
    "    history (History): History object from Keras training session.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(history.history[\"mse\"], label=\"Train MSE\")\n",
    "    plt.plot(history.history[\"val_mse\"], label=\"Validation MSE\")\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "    plt.title(\"Model Loss and Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / Metric\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and metrics from the training \n",
    "\n",
    "    Parameters:\n",
    "    history (History): History object from Keras training session.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(history[\"loss\"], label=\"Train MSE\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation MSE\")\n",
    "    plt.title(\"Model Loss and Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / Metric\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation function for model performance\n",
    "def evaluate_model(true_values, predicted_values, data_type=\"Validation\"):\n",
    "    # Remove NaN values\n",
    "    mask = ~np.isnan(predicted_values)\n",
    "\n",
    "    true_values = true_values[mask]\n",
    "    predicted_values = predicted_values[mask]\n",
    "\n",
    "    if len(true_values) > 0 and len(predicted_values) > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "        mae = mean_absolute_error(true_values, predicted_values)\n",
    "        print(f\"{data_type} Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "        print(f\"{data_type} Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    else:\n",
    "        print(f\"{data_type} evaluation skipped due to insufficient data.\")\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot predicted vs. actual values\n",
    "def plot_predicted_vs_actual(testY, forecasts, test_index, look_back):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(sorted(test_index[look_back + 1 :]), testY, label=\"Actual\")\n",
    "    plt.plot(sorted(test_index[look_back + 1 :]), forecasts, label=\"Predicted\")\n",
    "    plt.title(\"Actual vs Predicted Values\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Function to plot predicted vs. actual values with MSE in subplots\n",
    "def plot_multiple_results(results, evaluation, unique_pairs, look_back):\n",
    "    num_plots = len(results)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_plots:\n",
    "            testY, forecasts = results[i]\n",
    "            mae, rmse = evaluation[i][\"mae\"], evaluation[i][\"rmse\"]\n",
    "            lat, lon = unique_pairs.iloc[i]\n",
    "            time_index = sorted(test_index)\n",
    "\n",
    "            ax.plot(time_index[:-1], testY, label=\"Actual\")\n",
    "            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n",
    "            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Value\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            # Add MSE to the corner\n",
    "            ax.text(\n",
    "                0.95,\n",
    "                0.05,\n",
    "                f\"RMSE: {rmse:.2f}\",\n",
    "                verticalalignment=\"bottom\",\n",
    "                horizontalalignment=\"right\",\n",
    "                transform=ax.transAxes,\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Function to plot predicted vs. actual values with MSE in subplots\n",
    "def plot_multiple_results(results, evaluation, unique_pairs, look_back):\n",
    "    num_plots = len(results)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_plots:\n",
    "            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n",
    "            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n",
    "            lat, lon = unique_pairs.iloc[i]\n",
    "            time_index = sorted(test_index)\n",
    "\n",
    "            ax.plot(time_index[:-1], testY, label=\"Actual\")\n",
    "            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n",
    "            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Value\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            # Add MSE to the corner\n",
    "            ax.text(\n",
    "                0.95,\n",
    "                0.05,\n",
    "                f\"RMSE: {rmse:.2f}\",\n",
    "                verticalalignment=\"bottom\",\n",
    "                horizontalalignment=\"right\",\n",
    "                transform=ax.transAxes,\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output data to a serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert numpy arrays to lists\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # Convert numpy scalar types to Python scalars\n",
    "    elif isinstance(obj, dict):\n",
    "        # Recursively convert each item in the dictionary\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        # Recursively convert each item in the list\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    return obj  # Return the object if it's already serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Germany border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "../data/germany_border.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../data/germany_border.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read\u001b[39;00m\n\u001b[1;32m      2\u001b[0m germany_shp_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgermany_border.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m germany_gpd \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgermany_shp_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sif_env3/lib/python3.11/site-packages/geopandas/io/file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sif_env3/lib/python3.11/site-packages/geopandas/io/file.py:315\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    316\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sif_env3/lib/python3.11/site-packages/fiona/env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sif_env3/lib/python3.11/site-packages/fiona/__init__.py:292\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    304\u001b[0m         path,\n\u001b[1;32m    305\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/sif_env3/lib/python3.11/site-packages/fiona/collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../data/germany_border.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Read\n",
    "germany_shp_path = os.path.join(\"..\", \"data\", \"germany_border.shp\")\n",
    "germany_gpd = gpd.read_file(germany_shp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and subset cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_path = os.path.join(\"..\",\"data\", \"processed\", \"cube_preprocessed.nc\")\n",
    "cube_ger = xr.open_dataset(cube_ger_path, chunks={\"time\": 92, \"lat\": -1, \"lon\": -1})\n",
    "cube_ger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Cube only keeping cells with 50% forest cover in 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cube_ger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forest_2000_mask \u001b[38;5;241m=\u001b[39m (\u001b[43mcube_ger\u001b[49m\u001b[38;5;241m.\u001b[39mall_classes\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      2\u001b[0m forest_2000_mask\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cube_ger' is not defined"
     ]
    }
   ],
   "source": [
    "forest_2000_mask = (cube_ger.all_classes.isel(time=0) > 50).astype(int)\n",
    "forest_2000_mask.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_f = cube_ger.where(forest_2000_mask)\n",
    "cube_ger_f.sif_gosif.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop with germany border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_f.rio.write_crs(4326, inplace=True)\n",
    "cube_ger_f_crop = cube_ger_f.rio.clip(\n",
    "    germany_gpd.geometry.values, germany_gpd.crs, drop=False, all_touched=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cube_ger_f_crop.sif_gosif.isel(time=0).plot(ax=ax)\n",
    "germany_gpd.plot(ax=ax, edgecolor=\"red\", facecolor=\"none\")  # Adjust colors as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"sif_gosif\",\n",
    "    \"evaporation_era5\",\n",
    "    \"precipitation_era5\",\n",
    "    \"radiation_era5\",\n",
    "    \"air_temperature_2m\",\n",
    "    \"max_air_temperature_2m\",\n",
    "    \"min_air_temperature_2m\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_ger_f_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_ger_f_crop = xr.open_dataset(\"/net/home/lmaecker/LSTM-SIF/data/cubes/cube_subset_crop.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x7ff47ee05910>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube_ger_f_crop.isel(time=0).sif_gosif.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create dataframe from cube\n",
    "all_data_df = cube_ger_f_crop.to_dataframe().dropna()\n",
    "all_data_df = all_data_df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale data and drop forest variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables = [\n",
    "    \"sif_gosif\",\n",
    "    \"evaporation_era5\",\n",
    "    \"precipitation_era5\",\n",
    "    \"radiation_era5\",\n",
    "    \"air_temperature_2m\",\n",
    "    \"max_air_temperature_2m\",\n",
    "    \"min_air_temperature_2m\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sif_gosif</th>\n",
       "      <th>evaporation_era5</th>\n",
       "      <th>precipitation_era5</th>\n",
       "      <th>radiation_era5</th>\n",
       "      <th>air_temperature_2m</th>\n",
       "      <th>max_air_temperature_2m</th>\n",
       "      <th>min_air_temperature_2m</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.083664</td>\n",
       "      <td>1.347097</td>\n",
       "      <td>-1.188342</td>\n",
       "      <td>-0.959770</td>\n",
       "      <td>-3.253350</td>\n",
       "      <td>-2.531468</td>\n",
       "      <td>-3.357620</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.375</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.976883</td>\n",
       "      <td>1.373292</td>\n",
       "      <td>-1.096533</td>\n",
       "      <td>-1.278136</td>\n",
       "      <td>-1.518758</td>\n",
       "      <td>-1.672419</td>\n",
       "      <td>-1.668526</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.974599</td>\n",
       "      <td>1.355568</td>\n",
       "      <td>-1.087247</td>\n",
       "      <td>-1.281463</td>\n",
       "      <td>-1.521494</td>\n",
       "      <td>-1.669833</td>\n",
       "      <td>-1.732090</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.065111</td>\n",
       "      <td>1.375217</td>\n",
       "      <td>-1.053518</td>\n",
       "      <td>-0.970096</td>\n",
       "      <td>-2.840662</td>\n",
       "      <td>-2.357270</td>\n",
       "      <td>-3.074922</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.625</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.969862</td>\n",
       "      <td>1.339778</td>\n",
       "      <td>-1.063302</td>\n",
       "      <td>-1.281001</td>\n",
       "      <td>-1.522494</td>\n",
       "      <td>-1.672747</td>\n",
       "      <td>-1.752711</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763595</th>\n",
       "      <td>-0.941299</td>\n",
       "      <td>1.368476</td>\n",
       "      <td>-0.057878</td>\n",
       "      <td>-1.420631</td>\n",
       "      <td>-0.450864</td>\n",
       "      <td>-0.369745</td>\n",
       "      <td>-1.087510</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763596</th>\n",
       "      <td>-0.956356</td>\n",
       "      <td>1.382251</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>-0.451635</td>\n",
       "      <td>-0.404656</td>\n",
       "      <td>-1.067127</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763597</th>\n",
       "      <td>-0.908224</td>\n",
       "      <td>1.400193</td>\n",
       "      <td>0.329970</td>\n",
       "      <td>-1.426882</td>\n",
       "      <td>-0.437109</td>\n",
       "      <td>-0.431621</td>\n",
       "      <td>-1.010070</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>10.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763598</th>\n",
       "      <td>-0.964082</td>\n",
       "      <td>1.363269</td>\n",
       "      <td>0.221260</td>\n",
       "      <td>-1.387140</td>\n",
       "      <td>-0.640855</td>\n",
       "      <td>-0.396947</td>\n",
       "      <td>-1.330811</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>13.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763599</th>\n",
       "      <td>-0.907674</td>\n",
       "      <td>1.496442</td>\n",
       "      <td>-0.385430</td>\n",
       "      <td>-1.500551</td>\n",
       "      <td>-0.825804</td>\n",
       "      <td>-0.813373</td>\n",
       "      <td>-0.980841</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>54.875</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sif_gosif  evaporation_era5  precipitation_era5  radiation_era5  \\\n",
       "0       -1.083664          1.347097           -1.188342       -0.959770   \n",
       "1       -0.976883          1.373292           -1.096533       -1.278136   \n",
       "2       -0.974599          1.355568           -1.087247       -1.281463   \n",
       "3       -1.065111          1.375217           -1.053518       -0.970096   \n",
       "4       -0.969862          1.339778           -1.063302       -1.281001   \n",
       "...           ...               ...                 ...             ...   \n",
       "763595  -0.941299          1.368476           -0.057878       -1.420631   \n",
       "763596  -0.956356          1.382251            0.063661       -1.423236   \n",
       "763597  -0.908224          1.400193            0.329970       -1.426882   \n",
       "763598  -0.964082          1.363269            0.221260       -1.387140   \n",
       "763599  -0.907674          1.496442           -0.385430       -1.500551   \n",
       "\n",
       "        air_temperature_2m  max_air_temperature_2m  min_air_temperature_2m  \\\n",
       "0                -3.253350               -2.531468               -3.357620   \n",
       "1                -1.518758               -1.672419               -1.668526   \n",
       "2                -1.521494               -1.669833               -1.732090   \n",
       "3                -2.840662               -2.357270               -3.074922   \n",
       "4                -1.522494               -1.672747               -1.752711   \n",
       "...                    ...                     ...                     ...   \n",
       "763595           -0.450864               -0.369745               -1.087510   \n",
       "763596           -0.451635               -0.404656               -1.067127   \n",
       "763597           -0.437109               -0.431621               -1.010070   \n",
       "763598           -0.640855               -0.396947               -1.330811   \n",
       "763599           -0.825804               -0.813373               -0.980841   \n",
       "\n",
       "             time     lat     lon  \n",
       "0      2002-01-05  47.375   9.875  \n",
       "1      2002-01-05  52.375   8.125  \n",
       "2      2002-01-05  52.375   8.375  \n",
       "3      2002-01-05  47.625  12.625  \n",
       "4      2002-01-05  52.375   8.625  \n",
       "...           ...     ...     ...  \n",
       "763595 2021-12-31  52.125  11.125  \n",
       "763596 2021-12-31  52.125  10.875  \n",
       "763597 2021-12-31  52.125  10.625  \n",
       "763598 2021-12-31  52.125  13.125  \n",
       "763599 2021-12-31  54.875   9.875  \n",
       "\n",
       "[763600 rows x 10 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n",
    "all_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_scaled = all_data_scaled.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sif_gosif</th>\n",
       "      <th>evaporation_era5</th>\n",
       "      <th>precipitation_era5</th>\n",
       "      <th>radiation_era5</th>\n",
       "      <th>air_temperature_2m</th>\n",
       "      <th>max_air_temperature_2m</th>\n",
       "      <th>min_air_temperature_2m</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.083664</td>\n",
       "      <td>1.347097</td>\n",
       "      <td>-1.188342</td>\n",
       "      <td>-0.959770</td>\n",
       "      <td>-3.253350</td>\n",
       "      <td>-2.531468</td>\n",
       "      <td>-3.357620</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.375</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.976883</td>\n",
       "      <td>1.373292</td>\n",
       "      <td>-1.096533</td>\n",
       "      <td>-1.278136</td>\n",
       "      <td>-1.518758</td>\n",
       "      <td>-1.672419</td>\n",
       "      <td>-1.668526</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.974599</td>\n",
       "      <td>1.355568</td>\n",
       "      <td>-1.087247</td>\n",
       "      <td>-1.281463</td>\n",
       "      <td>-1.521494</td>\n",
       "      <td>-1.669833</td>\n",
       "      <td>-1.732090</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.065111</td>\n",
       "      <td>1.375217</td>\n",
       "      <td>-1.053518</td>\n",
       "      <td>-0.970096</td>\n",
       "      <td>-2.840662</td>\n",
       "      <td>-2.357270</td>\n",
       "      <td>-3.074922</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.625</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.969862</td>\n",
       "      <td>1.339778</td>\n",
       "      <td>-1.063302</td>\n",
       "      <td>-1.281001</td>\n",
       "      <td>-1.522494</td>\n",
       "      <td>-1.672747</td>\n",
       "      <td>-1.752711</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>52.375</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763595</th>\n",
       "      <td>-0.941299</td>\n",
       "      <td>1.368476</td>\n",
       "      <td>-0.057878</td>\n",
       "      <td>-1.420631</td>\n",
       "      <td>-0.450864</td>\n",
       "      <td>-0.369745</td>\n",
       "      <td>-1.087510</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763596</th>\n",
       "      <td>-0.956356</td>\n",
       "      <td>1.382251</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>-0.451635</td>\n",
       "      <td>-0.404656</td>\n",
       "      <td>-1.067127</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763597</th>\n",
       "      <td>-0.908224</td>\n",
       "      <td>1.400193</td>\n",
       "      <td>0.329970</td>\n",
       "      <td>-1.426882</td>\n",
       "      <td>-0.437109</td>\n",
       "      <td>-0.431621</td>\n",
       "      <td>-1.010070</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>10.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763598</th>\n",
       "      <td>-0.964082</td>\n",
       "      <td>1.363269</td>\n",
       "      <td>0.221260</td>\n",
       "      <td>-1.387140</td>\n",
       "      <td>-0.640855</td>\n",
       "      <td>-0.396947</td>\n",
       "      <td>-1.330811</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>52.125</td>\n",
       "      <td>13.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763599</th>\n",
       "      <td>-0.907674</td>\n",
       "      <td>1.496442</td>\n",
       "      <td>-0.385430</td>\n",
       "      <td>-1.500551</td>\n",
       "      <td>-0.825804</td>\n",
       "      <td>-0.813373</td>\n",
       "      <td>-0.980841</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>54.875</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sif_gosif  evaporation_era5  precipitation_era5  radiation_era5  \\\n",
       "0       -1.083664          1.347097           -1.188342       -0.959770   \n",
       "1       -0.976883          1.373292           -1.096533       -1.278136   \n",
       "2       -0.974599          1.355568           -1.087247       -1.281463   \n",
       "3       -1.065111          1.375217           -1.053518       -0.970096   \n",
       "4       -0.969862          1.339778           -1.063302       -1.281001   \n",
       "...           ...               ...                 ...             ...   \n",
       "763595  -0.941299          1.368476           -0.057878       -1.420631   \n",
       "763596  -0.956356          1.382251            0.063661       -1.423236   \n",
       "763597  -0.908224          1.400193            0.329970       -1.426882   \n",
       "763598  -0.964082          1.363269            0.221260       -1.387140   \n",
       "763599  -0.907674          1.496442           -0.385430       -1.500551   \n",
       "\n",
       "        air_temperature_2m  max_air_temperature_2m  min_air_temperature_2m  \\\n",
       "0                -3.253350               -2.531468               -3.357620   \n",
       "1                -1.518758               -1.672419               -1.668526   \n",
       "2                -1.521494               -1.669833               -1.732090   \n",
       "3                -2.840662               -2.357270               -3.074922   \n",
       "4                -1.522494               -1.672747               -1.752711   \n",
       "...                    ...                     ...                     ...   \n",
       "763595           -0.450864               -0.369745               -1.087510   \n",
       "763596           -0.451635               -0.404656               -1.067127   \n",
       "763597           -0.437109               -0.431621               -1.010070   \n",
       "763598           -0.640855               -0.396947               -1.330811   \n",
       "763599           -0.825804               -0.813373               -0.980841   \n",
       "\n",
       "             time     lat     lon  \n",
       "0      2002-01-05  47.375   9.875  \n",
       "1      2002-01-05  52.375   8.125  \n",
       "2      2002-01-05  52.375   8.375  \n",
       "3      2002-01-05  47.625  12.625  \n",
       "4      2002-01-05  52.375   8.625  \n",
       "...           ...     ...     ...  \n",
       "763595 2021-12-31  52.125  11.125  \n",
       "763596 2021-12-31  52.125  10.875  \n",
       "763597 2021-12-31  52.125  10.625  \n",
       "763598 2021-12-31  52.125  13.125  \n",
       "763599 2021-12-31  54.875   9.875  \n",
       "\n",
       "[763600 rows x 10 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model multiple timeseries with with local model using hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.375</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.625</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>54.875</td>\n",
       "      <td>8.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>54.625</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>830 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat     lon\n",
       "0    47.375   9.875\n",
       "1    52.375   8.125\n",
       "2    52.375   8.375\n",
       "3    47.625  12.625\n",
       "4    52.375   8.625\n",
       "..      ...     ...\n",
       "825  54.875   9.125\n",
       "826  54.875   8.875\n",
       "827  54.625  13.625\n",
       "828  54.875   9.375\n",
       "829  54.875   9.875\n",
       "\n",
       "[830 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iterrows at 0x7ff47ef97450>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_pairs.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.375</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.625</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>54.875</td>\n",
       "      <td>8.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>54.625</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>830 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat     lon\n",
       "0    47.375   9.875\n",
       "1    52.375   8.125\n",
       "2    52.375   8.375\n",
       "3    47.625  12.625\n",
       "4    52.375   8.625\n",
       "..      ...     ...\n",
       "825  54.875   9.125\n",
       "826  54.875   8.875\n",
       "827  54.625  13.625\n",
       "828  54.875   9.375\n",
       "829  54.875   9.875\n",
       "\n",
       "[830 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lat_lon_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.375\n"
     ]
    }
   ],
   "source": [
    "for i, pair in enumerate(lat_lon_pairs.iterrows()):\n",
    "    if i== 1:\n",
    "       print(pair[1][\"lat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.375</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.625</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.375</td>\n",
       "      <td>8.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>54.875</td>\n",
       "      <td>8.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>54.625</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>54.875</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>830 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat     lon\n",
       "0    47.375   9.875\n",
       "1    52.375   8.125\n",
       "2    52.375   8.375\n",
       "3    47.625  12.625\n",
       "4    52.375   8.625\n",
       "..      ...     ...\n",
       "825  54.875   9.125\n",
       "826  54.875   8.875\n",
       "827  54.625  13.625\n",
       "828  54.875   9.375\n",
       "829  54.875   9.875\n",
       "\n",
       "[830 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique paris \n",
    "unique_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n",
    "unique_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# dd/mm/YY\n",
    "current_date = today.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename= f\"logs/auto_modelling_{current_date}.log\",level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'units_lstm': [64, 128],\n",
    "    'activation': ['relu', 'tanh'], \n",
    "    'epochs': [100],    \n",
    "    'learning_rate': [0.0001],\n",
    "    'dropout_rate': [0.2,0.4],\n",
    "    'batch_size': [25],\n",
    "    'num_lstm_layers': [1, 2, 3]\n",
    "}\n",
    "\n",
    "look_backs = [15,30,45]\n",
    "\n",
    "\n",
    "results = []\n",
    "evaluation = []\n",
    "histories = []\n",
    "best_params = []\n",
    "\n",
    "number = len(unique_pairs)\n",
    "output_data = {}\n",
    "\n",
    "n_splits = 3\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting Modelling and GridsearchCV\")\n",
    "\n",
    "\n",
    "for look_back in look_backs:\n",
    "            \n",
    "    for i in range(10):\n",
    "\n",
    "        \n",
    "        lat, lon = unique_pairs.iloc[i]\n",
    "\n",
    "        logging.info(f\"Starting Grid Search for \\n lat: {lat}\\n lon: {lon}\")\n",
    "\n",
    "        trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "            all_data_scaled, lat, lon, look_back=look_back\n",
    "        )\n",
    "\n",
    "        # Create a KerasRegressor\n",
    "        features = trainX.shape[2]\n",
    "        model = create_keras_regressor(look_back, features)\n",
    "\n",
    "        # Define GridSearchCV\n",
    "        lstm_grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Perform grid search\n",
    "        lstm_grid_search.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Get the best model from the grid search\n",
    "        best_params = lstm_grid_search.best_params_\n",
    "\n",
    "        logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "        logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "        lstm_model = create_lstm_model(\n",
    "            look_back=look_back,\n",
    "            features=features,\n",
    "            units_lstm=best_params['units_lstm'],\n",
    "            activation=best_params['activation'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            num_lstm_layers=best_params['num_lstm_layers']\n",
    "        )\n",
    "\n",
    "        history = lstm_model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            verbose=1,\n",
    "            validation_data=(valX, valY)\n",
    "        )\n",
    "\n",
    "\n",
    "        forecasts = predict_replace(lstm_model, testX)\n",
    "\n",
    "        testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "        forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "        rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "        mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "        results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "        evaluation.append({\"lat\": lat, \"lon\": lon, \"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "        histories.append(history.history)\n",
    "\n",
    "\n",
    "        # Add results to the output dictionary\n",
    "        output_data[(lat, lon)] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"look_back\":look_back,\n",
    "            \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "            \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "            \"history\": history.history\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Convert the entire data dictionary to a serializable format\n",
    "        output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data[(lat, lon)].items()}\n",
    "\n",
    "\n",
    "        # Construct the output file path\n",
    "        folder_name_json = f\"result_jsons_l{look_back}\"\n",
    "        os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "        file_name_json = f\"l{look_back}_{str(lat).replace('.', '_')}_{str(lon).replace('.', '_')}_model_results.json\"\n",
    "        output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "        # Writing the converted data to 'model_results.json'\n",
    "        with open(output_json_file, \"w\") as file:\n",
    "            json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "        logging.info(f\"Results and evaluation written to: {output_json_file}\")\n",
    "\n",
    "        logging.info(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "        print(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "\n",
    "        logging.info(100*\"-\")\n",
    "        logging.info(100*\"-\")\n",
    "\n",
    "\n",
    "    # Convert the entire data dictionary to a serializable format\n",
    "    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "    # Writing the converted data to 'model_results.json'\n",
    "    with open(os.path.join(folder_name_json,f\"results_auto_l{look_back}_relu.json\"), \"w\") as file:\n",
    "        json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "    print(\"Results and evaluation have been written to 'model_results.json'\")\n",
    "\n",
    "\n",
    "    print(2*\"\\n\")\n",
    "    print(100*\"-\")\n",
    "    print(2*\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Setup without autoregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_logging(f\"logs/not_regressive_{current_date}.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting Modelling and GridsearchCV\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for look_back in look_backs:\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        \n",
    "        lat, lon = unique_pairs.iloc[i]\n",
    "\n",
    "        logging.info(f\"Starting non autoregressive Grid Search for \\n lat: {lat}\\n lon: {lon}\")\n",
    "\n",
    "        trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "            all_data_scaled, lat, lon, look_back=look_back, autoregressive=False\n",
    "        )\n",
    "\n",
    "        # Create a KerasRegressor\n",
    "        features = trainX.shape[2]\n",
    "        model = create_keras_regressor(look_back, features)\n",
    "\n",
    "        # Define GridSearchCV\n",
    "        lstm_grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Perform grid search\n",
    "        lstm_grid_search.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Get the best model from the grid search\n",
    "        best_params = lstm_grid_search.best_params_\n",
    "\n",
    "        logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "        logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "        lstm_model = create_lstm_model(\n",
    "            look_back=look_back,\n",
    "            features=features,\n",
    "            units_lstm=best_params['units_lstm'],\n",
    "            activation=best_params['activation'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            num_lstm_layers=best_params['num_lstm_layers']\n",
    "        )\n",
    "\n",
    "        history = lstm_model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            verbose=1,\n",
    "            validation_data=(valX, valY)\n",
    "        )\n",
    "\n",
    "\n",
    "        forecasts = predict_replace(lstm_model, testX, autoregressive=False)\n",
    "\n",
    "        testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "        forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "        rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "        mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "        results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "        evaluation.append({\"lat\": lat, \"lon\": lon, \"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "        histories.append(history.history)\n",
    "\n",
    "\n",
    "        # Add results to the output dictionary\n",
    "        output_data[(lat, lon)] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"look_back\":look_back,\n",
    "            \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "            \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "            \"history\": history.history\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Convert the entire data dictionary to a serializable format\n",
    "        output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data[(lat, lon)].items()}\n",
    "\n",
    "        # Construct the output file path\n",
    "        folder_name_json = f\"result_jsons_l{look_back}_noauto\"\n",
    "        os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "        file_name_json = f\"l{look_back}_{str(lat).replace('.', '_')}_{str(lon).replace('.', '_')}_model_results_noauto.json\"\n",
    "        output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "        # Writing the converted data to 'model_results.json'\n",
    "        with open(output_json_file, \"w\") as file:\n",
    "            json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "        logging.info(f\"Results and evaluation written to: {output_json_file}\")\n",
    "\n",
    "        logging.info(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "        print(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "\n",
    "        logging.info(100*\"-\")\n",
    "        logging.info(100*\"-\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the entire data dictionary to a serializable format\n",
    "    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "    # Writing the converted data to 'model_results.json'\n",
    "    with open(os.path.join(folder_name_json,f\"results_noauto_l{look_back}_relu.json\"), \"w\") as file:\n",
    "        json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "    print(\"Results and evaluation have been written to 'model_results.json'\")\n",
    "\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Results and testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the results and evaluation from the file\n",
    "with open(\"/home/luismaecker/team-extra/imke&luis&moritz/modelling_current/model_results_full.json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extracting the loaded data\n",
    "best_params = loaded_data['best_params']\n",
    "evaluation = loaded_data['evaluation']\n",
    "results = loaded_data['results']\n",
    "history = loaded_data['history']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = unique_pairs.iloc[1]\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "    all_data_scaled, lat, lon, look_back=look_back\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_results(results, evaluation, unique_pairs, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICT PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_multiple_results(results_dict, time_index = test_index):\n",
    "    num_plots = len(results_dict)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "    if num_rows * num_cols > 1:\n",
    "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "    else:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for i, (lat_lon, data_dict) in enumerate(results_dict.items()):\n",
    "        ax = axes[i]\n",
    "        testY = data_dict['results']['true_values']\n",
    "        forecasts = data_dict['results']['predicted_values']\n",
    "        mae = data_dict['evaluation']['mae']\n",
    "        rmse = data_dict['evaluation']['rmse']\n",
    "\n",
    "        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n",
    "        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n",
    "\n",
    "        # Generate a time index from the length of the testY data\n",
    "        # time_index = range(len(testY))\n",
    "\n",
    "        ax.plot(time_index, testY, label=\"Actual\")\n",
    "        ax.plot(time_index, forecasts, label=\"Predicted\")\n",
    "        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Add MSE to the corner\n",
    "        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n",
    "                verticalalignment='bottom', horizontalalignment='right', \n",
    "                transform=ax.transAxes, color='red', fontsize=12)\n",
    "\n",
    "    # Adjust the layout to prevent overlap and make sure all plots are visible\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example of calling the function with your dictionary of results\n",
    "# plot_multiple_results(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the results and evaluation from the file\n",
    "with open(\"/home/luismaecker/team-extra/imke&luis&moritz/modelling_current/model_results_full.json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(lat).replace(\".\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_results(loaded_data, test_index[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_glob(df_scaled, unique_pairs, look_back, target_col=\"sif_gosif\", autoregressive=True):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for all specified locations and look-back periods.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are needed.\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    unique_pairs (DataFrame): DataFrame containing unique pairs of latitudes and longitudes.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets, and pixel indices.\n",
    "    \"\"\"\n",
    "    combined_data = pd.DataFrame()\n",
    "    pixel_indices = {}\n",
    "\n",
    "    for idx, (lat, lon) in unique_pairs.iterrows():\n",
    "        pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "        #combined_data = pd.concat([combined_data, pixel_data])\n",
    "        pixel_indices[(lat, lon)] = pixel_data.index\n",
    "\n",
    "    combined_data = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    first_index_2017 = combined_data[combined_data[\"time\"].dt.year == 2017].index[0]\n",
    "    val_end_index = first_index_2017 + look_back\n",
    "\n",
    "    train_data = combined_data[combined_data[\"time\"].dt.year <= 2014]\n",
    "\n",
    "    val_data = combined_data[\n",
    "        (combined_data[\"time\"].dt.year == 2015) | \n",
    "        (combined_data[\"time\"].dt.year == 2016) | \n",
    "        ((combined_data[\"time\"].dt.year == 2017) & (combined_data.index < val_end_index))\n",
    "    ]\n",
    "\n",
    "    test_data = combined_data[\n",
    "        (combined_data.index >= val_end_index) |\n",
    "        (combined_data[\"time\"].dt.year >= 2018)\n",
    "    ]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col, autoregressive=autoregressive)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col, autoregressive=autoregressive)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col, autoregressive=autoregressive)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Starting splitting\n"
     ]
    }
   ],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting splitting\")\n",
    "\n",
    "\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "look_back = 30\n",
    " \n",
    "\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data_glob(\n",
    "    all_data_scaled, unique_pairs = unique_pairs, look_back=look_back, autoregressive=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46613, 30, 6) (7246, 30, 6) (17808, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape,valX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(testX.shape[0]) + int(valX.shape[0]) + int(trainX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sif_gosif</th>\n",
       "      <th>evaporation_era5</th>\n",
       "      <th>precipitation_era5</th>\n",
       "      <th>radiation_era5</th>\n",
       "      <th>air_temperature_2m</th>\n",
       "      <th>max_air_temperature_2m</th>\n",
       "      <th>min_air_temperature_2m</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.011922</td>\n",
       "      <td>1.248912</td>\n",
       "      <td>-1.180376</td>\n",
       "      <td>-0.826753</td>\n",
       "      <td>-1.925783</td>\n",
       "      <td>-1.800210</td>\n",
       "      <td>-1.667825</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.625</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.009902</td>\n",
       "      <td>1.367647</td>\n",
       "      <td>-1.089934</td>\n",
       "      <td>-1.273543</td>\n",
       "      <td>-2.339386</td>\n",
       "      <td>-1.955863</td>\n",
       "      <td>-2.981955</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.125</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.985991</td>\n",
       "      <td>1.319931</td>\n",
       "      <td>-1.075546</td>\n",
       "      <td>-1.220842</td>\n",
       "      <td>-1.901189</td>\n",
       "      <td>-1.884916</td>\n",
       "      <td>-1.989540</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.004664</td>\n",
       "      <td>1.381150</td>\n",
       "      <td>-0.963128</td>\n",
       "      <td>-1.198117</td>\n",
       "      <td>-2.520454</td>\n",
       "      <td>-2.205761</td>\n",
       "      <td>-2.930598</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>48.875</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.988567</td>\n",
       "      <td>1.377211</td>\n",
       "      <td>-1.040038</td>\n",
       "      <td>-1.240012</td>\n",
       "      <td>-2.166432</td>\n",
       "      <td>-1.972878</td>\n",
       "      <td>-2.397991</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71755</th>\n",
       "      <td>-0.899450</td>\n",
       "      <td>1.381330</td>\n",
       "      <td>1.892409</td>\n",
       "      <td>-1.410915</td>\n",
       "      <td>-0.169287</td>\n",
       "      <td>-0.503028</td>\n",
       "      <td>-0.171374</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71756</th>\n",
       "      <td>-0.917507</td>\n",
       "      <td>1.459768</td>\n",
       "      <td>1.595228</td>\n",
       "      <td>-1.420709</td>\n",
       "      <td>-0.239737</td>\n",
       "      <td>-0.546264</td>\n",
       "      <td>-0.250604</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71757</th>\n",
       "      <td>-0.948588</td>\n",
       "      <td>1.523909</td>\n",
       "      <td>0.908647</td>\n",
       "      <td>-1.440518</td>\n",
       "      <td>-0.336157</td>\n",
       "      <td>-0.573856</td>\n",
       "      <td>-0.421814</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71758</th>\n",
       "      <td>-0.957456</td>\n",
       "      <td>1.553456</td>\n",
       "      <td>1.080570</td>\n",
       "      <td>-1.408002</td>\n",
       "      <td>-0.465537</td>\n",
       "      <td>-0.577041</td>\n",
       "      <td>-0.561901</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>50.625</td>\n",
       "      <td>10.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71759</th>\n",
       "      <td>-0.972470</td>\n",
       "      <td>1.493259</td>\n",
       "      <td>-0.399016</td>\n",
       "      <td>-1.500486</td>\n",
       "      <td>-0.949547</td>\n",
       "      <td>-0.504090</td>\n",
       "      <td>-1.852475</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>53.125</td>\n",
       "      <td>13.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71760 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sif_gosif  evaporation_era5  precipitation_era5  radiation_era5  \\\n",
       "0      -1.011922          1.248912           -1.180376       -0.826753   \n",
       "1      -1.009902          1.367647           -1.089934       -1.273543   \n",
       "2      -0.985991          1.319931           -1.075546       -1.220842   \n",
       "3      -1.004664          1.381150           -0.963128       -1.198117   \n",
       "4      -0.988567          1.377211           -1.040038       -1.240012   \n",
       "...          ...               ...                 ...             ...   \n",
       "71755  -0.899450          1.381330            1.892409       -1.410915   \n",
       "71756  -0.917507          1.459768            1.595228       -1.420709   \n",
       "71757  -0.948588          1.523909            0.908647       -1.440518   \n",
       "71758  -0.957456          1.553456            1.080570       -1.408002   \n",
       "71759  -0.972470          1.493259           -0.399016       -1.500486   \n",
       "\n",
       "       air_temperature_2m  max_air_temperature_2m  min_air_temperature_2m  \\\n",
       "0               -1.925783               -1.800210               -1.667825   \n",
       "1               -2.339386               -1.955863               -2.981955   \n",
       "2               -1.901189               -1.884916               -1.989540   \n",
       "3               -2.520454               -2.205761               -2.930598   \n",
       "4               -2.166432               -1.972878               -2.397991   \n",
       "...                   ...                     ...                     ...   \n",
       "71755           -0.169287               -0.503028               -0.171374   \n",
       "71756           -0.239737               -0.546264               -0.250604   \n",
       "71757           -0.336157               -0.573856               -0.421814   \n",
       "71758           -0.465537               -0.577041               -0.561901   \n",
       "71759           -0.949547               -0.504090               -1.852475   \n",
       "\n",
       "            time     lat     lon  \n",
       "0     2002-01-05  47.625   7.625  \n",
       "1     2002-01-05  50.125   9.875  \n",
       "2     2002-01-05  50.375   6.875  \n",
       "3     2002-01-05  48.875  13.625  \n",
       "4     2002-01-05  50.375  10.875  \n",
       "...          ...     ...     ...  \n",
       "71755 2021-12-31  51.125   7.625  \n",
       "71756 2021-12-31  51.125   7.875  \n",
       "71757 2021-12-31  51.125   8.375  \n",
       "71758 2021-12-31  50.625  10.625  \n",
       "71759 2021-12-31  53.125  13.375  \n",
       "\n",
       "[71760 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'units_lstm': [64, 128],\n",
    "    'activation': ['tanh'], \n",
    "    'epochs': [100],    \n",
    "    'learning_rate': [0.0001],\n",
    "    'dropout_rate': [0.2],\n",
    "    'batch_size': [25],\n",
    "    'num_lstm_layers': [1, 2]\n",
    "}\n",
    "\n",
    "results = []\n",
    "evaluation = []\n",
    "histories = []\n",
    "best_params = []\n",
    "\n",
    "number = len(unique_pairs)\n",
    "output_data = {}\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STarting gcv\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:12:41.068836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:44.789046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=12.4min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=20.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=22.9min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=25.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=43.2min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=41.2min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=45.1min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=43.2min\n",
      "fitting model\n",
      "Epoch 1/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.2501 - val_loss: 0.1362\n",
      "Epoch 2/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1586 - val_loss: 0.1380\n",
      "Epoch 3/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1514 - val_loss: 0.1362\n",
      "Epoch 4/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1458 - val_loss: 0.1515\n",
      "Epoch 5/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1432 - val_loss: 0.1423\n",
      "Epoch 6/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1443 - val_loss: 0.1485\n",
      "Epoch 7/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1415 - val_loss: 0.1686\n",
      "Epoch 8/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1373 - val_loss: 0.1377\n",
      "Epoch 9/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1364 - val_loss: 0.1467\n",
      "Epoch 10/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1321 - val_loss: 0.1493\n",
      "Epoch 11/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1332 - val_loss: 0.1558\n",
      "Epoch 12/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1318 - val_loss: 0.1592\n",
      "Epoch 13/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1320 - val_loss: 0.1521\n",
      "Epoch 14/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1246 - val_loss: 0.1509\n",
      "Epoch 15/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1285 - val_loss: 0.1548\n",
      "Epoch 16/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1261 - val_loss: 0.1445\n",
      "Epoch 17/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1276 - val_loss: 0.1578\n",
      "Epoch 18/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1250 - val_loss: 0.1395\n",
      "Epoch 19/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1266 - val_loss: 0.1521\n",
      "Epoch 20/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1241 - val_loss: 0.1488\n",
      "Epoch 21/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1271 - val_loss: 0.1519\n",
      "Epoch 22/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1227 - val_loss: 0.1483\n",
      "Epoch 23/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.1475\n",
      "Epoch 24/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1211 - val_loss: 0.1471\n",
      "Epoch 25/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1212 - val_loss: 0.1365\n",
      "Epoch 26/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1164 - val_loss: 0.1672\n",
      "Epoch 27/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.1655\n",
      "Epoch 28/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1193 - val_loss: 0.1501\n",
      "Epoch 29/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1168 - val_loss: 0.1528\n",
      "Epoch 30/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1177 - val_loss: 0.1452\n",
      "Epoch 31/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1173 - val_loss: 0.1530\n",
      "Epoch 32/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1202 - val_loss: 0.1536\n",
      "Epoch 33/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1158 - val_loss: 0.1455\n",
      "Epoch 34/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1164 - val_loss: 0.1453\n",
      "Epoch 35/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1136 - val_loss: 0.1463\n",
      "Epoch 36/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1147 - val_loss: 0.1505\n",
      "Epoch 37/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1137 - val_loss: 0.1323\n",
      "Epoch 38/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1152 - val_loss: 0.1386\n",
      "Epoch 39/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1161 - val_loss: 0.1547\n",
      "Epoch 40/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1105 - val_loss: 0.1488\n",
      "Epoch 41/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1127 - val_loss: 0.1515\n",
      "Epoch 42/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1106 - val_loss: 0.1528\n",
      "Epoch 43/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1134 - val_loss: 0.1497\n",
      "Epoch 44/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1109 - val_loss: 0.1482\n",
      "Epoch 45/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1118 - val_loss: 0.1454\n",
      "Epoch 46/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1145 - val_loss: 0.1414\n",
      "Epoch 47/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1102 - val_loss: 0.1493\n",
      "Epoch 48/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1098 - val_loss: 0.1512\n",
      "Epoch 49/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1081 - val_loss: 0.1455\n",
      "Epoch 50/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1118 - val_loss: 0.1384\n",
      "Epoch 51/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1112 - val_loss: 0.1562\n",
      "Epoch 52/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1069 - val_loss: 0.1411\n",
      "Epoch 53/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - loss: 0.1081 - val_loss: 0.1530\n",
      "Epoch 54/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - loss: 0.1066 - val_loss: 0.1517\n",
      "Epoch 55/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1114 - val_loss: 0.1485\n",
      "Epoch 56/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1112 - val_loss: 0.1383\n",
      "Epoch 57/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1079 - val_loss: 0.1648\n",
      "Epoch 58/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1081 - val_loss: 0.1428\n",
      "Epoch 59/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1084 - val_loss: 0.1451\n",
      "Epoch 60/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1076 - val_loss: 0.1427\n",
      "Epoch 61/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1028 - val_loss: 0.1416\n",
      "Epoch 62/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1064 - val_loss: 0.1389\n",
      "Epoch 63/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1057 - val_loss: 0.1503\n",
      "Epoch 64/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1049 - val_loss: 0.1373\n",
      "Epoch 65/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1056 - val_loss: 0.1449\n",
      "Epoch 66/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1059 - val_loss: 0.1389\n",
      "Epoch 67/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1036 - val_loss: 0.1526\n",
      "Epoch 68/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1068 - val_loss: 0.1578\n",
      "Epoch 69/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1042 - val_loss: 0.1596\n",
      "Epoch 70/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1044 - val_loss: 0.1476\n",
      "Epoch 71/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1019 - val_loss: 0.1433\n",
      "Epoch 72/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1041 - val_loss: 0.1410\n",
      "Epoch 73/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1012 - val_loss: 0.1421\n",
      "Epoch 74/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1006 - val_loss: 0.1443\n",
      "Epoch 75/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1027 - val_loss: 0.1410\n",
      "Epoch 76/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1012 - val_loss: 0.1493\n",
      "Epoch 77/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1014 - val_loss: 0.1476\n",
      "Epoch 78/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1020 - val_loss: 0.1403\n",
      "Epoch 79/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0998 - val_loss: 0.1378\n",
      "Epoch 80/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1010 - val_loss: 0.1367\n",
      "Epoch 81/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0993 - val_loss: 0.1500\n",
      "Epoch 82/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1009 - val_loss: 0.1330\n",
      "Epoch 83/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1005 - val_loss: 0.1448\n",
      "Epoch 84/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0995 - val_loss: 0.1346\n",
      "Epoch 85/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1001 - val_loss: 0.1429\n",
      "Epoch 86/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0979 - val_loss: 0.1425\n",
      "Epoch 87/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0972 - val_loss: 0.1383\n",
      "Epoch 88/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.0997 - val_loss: 0.1353\n",
      "Epoch 89/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.0993 - val_loss: 0.1440\n",
      "Epoch 90/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1008 - val_loss: 0.1490\n",
      "Epoch 91/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1000 - val_loss: 0.1514\n",
      "Epoch 92/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0967 - val_loss: 0.1629\n",
      "Epoch 93/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0974 - val_loss: 0.1498\n",
      "Epoch 94/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0969 - val_loss: 0.1440\n",
      "Epoch 95/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0991 - val_loss: 0.1349\n",
      "Epoch 96/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0983 - val_loss: 0.1462\n",
      "Epoch 97/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0969 - val_loss: 0.1450\n",
      "Epoch 98/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0973 - val_loss: 0.1429\n",
      "Epoch 99/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0955 - val_loss: 0.1303\n",
      "Epoch 100/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0985 - val_loss: 0.1482\n"
     ]
    }
   ],
   "source": [
    "look_back = 30\n",
    "\n",
    "# Create a KerasRegressor\n",
    "features = trainX.shape[2]\n",
    "model = create_keras_regressor(look_back, features)\n",
    "\n",
    "print(\"STarting gcv\")\n",
    "# Define GridSearchCV\n",
    "lstm_grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "lstm_grid_search.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_params = lstm_grid_search.best_params_\n",
    "\n",
    "logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "print(\"fitting model\")\n",
    "\n",
    "logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "lstm_model = create_lstm_model(\n",
    "    look_back=look_back,\n",
    "    features=features,\n",
    "    units_lstm=best_params['units_lstm'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    num_lstm_layers=best_params['num_lstm_layers']\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    validation_data=(valX, valY)\n",
    ")\n",
    "\n",
    "\n",
    "forecasts = predict_replace(lstm_model, testX, autoregressive=False)\n",
    "\n",
    "testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "evaluation.append({\"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "histories.append(history.history)\n",
    "\n",
    "\n",
    "# Add results to the output dictionary\n",
    "output_data = {\n",
    "    \"best_params\": best_params,\n",
    "    \"look_back\":look_back,\n",
    "    \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "    \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "    \"history\": history.history\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the entire data dictionary to a serializable format\n",
    "output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "# Construct the output file path\n",
    "folder_name_json = f\"result_jsons_l{look_back}_noauto_glob\"\n",
    "os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "file_name_json = f\"l{look_back}__results_noauto_glob.json\"\n",
    "output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "# Writing the converted data to 'model_results.json'\n",
    "with open(output_json_file, \"w\") as file:\n",
    "    json.dump(output_data_serializable, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Starting splitting\n"
     ]
    }
   ],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting splitting\")\n",
    "\n",
    "\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "look_back = 30\n",
    " \n",
    "\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data_glob(\n",
    "    all_data_scaled, unique_pairs = unique_pairs, look_back=look_back, autoregressive=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STarting gcv\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 15:38:11.150775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.150775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.152418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.152816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:20.468549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.468711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.469246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.469604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=12.1min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=23.0min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=24.6min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=23.7min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=44.3min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=42.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=46.6min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=47.3min\n",
      "fitting model\n",
      "Epoch 1/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.2030 - val_loss: 0.0834\n",
      "Epoch 2/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1216 - val_loss: 0.0910\n",
      "Epoch 3/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.1109 - val_loss: 0.0848\n",
      "Epoch 4/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.1065 - val_loss: 0.0855\n",
      "Epoch 5/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0985 - val_loss: 0.0894\n",
      "Epoch 6/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1001 - val_loss: 0.0897\n",
      "Epoch 7/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0972 - val_loss: 0.0826\n",
      "Epoch 8/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0946 - val_loss: 0.0845\n",
      "Epoch 9/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0950 - val_loss: 0.0785\n",
      "Epoch 10/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0927 - val_loss: 0.0788\n",
      "Epoch 11/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0929 - val_loss: 0.0801\n",
      "Epoch 12/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0929 - val_loss: 0.0780\n",
      "Epoch 13/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0897 - val_loss: 0.0764\n",
      "Epoch 14/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0890 - val_loss: 0.0883\n",
      "Epoch 15/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0883 - val_loss: 0.0876\n",
      "Epoch 16/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0897 - val_loss: 0.0905\n",
      "Epoch 17/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0851 - val_loss: 0.0765\n",
      "Epoch 18/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0871 - val_loss: 0.0813\n",
      "Epoch 19/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0868 - val_loss: 0.0902\n",
      "Epoch 20/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0849 - val_loss: 0.0899\n",
      "Epoch 21/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0925\n",
      "Epoch 22/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0854 - val_loss: 0.0785\n",
      "Epoch 23/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0847 - val_loss: 0.0892\n",
      "Epoch 24/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0854 - val_loss: 0.0829\n",
      "Epoch 25/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0874\n",
      "Epoch 26/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0825 - val_loss: 0.0885\n",
      "Epoch 27/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0831 - val_loss: 0.0888\n",
      "Epoch 28/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0839 - val_loss: 0.0774\n",
      "Epoch 29/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0822 - val_loss: 0.0834\n",
      "Epoch 30/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0817 - val_loss: 0.0792\n",
      "Epoch 31/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0809 - val_loss: 0.0894\n",
      "Epoch 32/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0792 - val_loss: 0.0922\n",
      "Epoch 33/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0786 - val_loss: 0.0846\n",
      "Epoch 34/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0791 - val_loss: 0.0827\n",
      "Epoch 35/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0802 - val_loss: 0.0791\n",
      "Epoch 36/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0802 - val_loss: 0.0861\n",
      "Epoch 37/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0848\n",
      "Epoch 38/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0783 - val_loss: 0.0951\n",
      "Epoch 39/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0852\n",
      "Epoch 40/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0808 - val_loss: 0.0873\n",
      "Epoch 41/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0766 - val_loss: 0.0768\n",
      "Epoch 42/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0798 - val_loss: 0.0910\n",
      "Epoch 43/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0817\n",
      "Epoch 44/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0758 - val_loss: 0.0893\n",
      "Epoch 45/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0750 - val_loss: 0.0859\n",
      "Epoch 46/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0793 - val_loss: 0.0854\n",
      "Epoch 47/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0763 - val_loss: 0.0839\n",
      "Epoch 48/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0742 - val_loss: 0.0791\n",
      "Epoch 49/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0756 - val_loss: 0.0781\n",
      "Epoch 50/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0773 - val_loss: 0.0860\n",
      "Epoch 51/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0751 - val_loss: 0.0896\n",
      "Epoch 52/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0749 - val_loss: 0.0838\n",
      "Epoch 53/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0742 - val_loss: 0.0948\n",
      "Epoch 54/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0735 - val_loss: 0.0791\n",
      "Epoch 55/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0763 - val_loss: 0.0866\n",
      "Epoch 56/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0728 - val_loss: 0.0840\n",
      "Epoch 57/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0749 - val_loss: 0.0805\n",
      "Epoch 58/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0736 - val_loss: 0.0879\n",
      "Epoch 59/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0730 - val_loss: 0.0790\n",
      "Epoch 60/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0718 - val_loss: 0.0834\n",
      "Epoch 61/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0735 - val_loss: 0.0839\n",
      "Epoch 62/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0739 - val_loss: 0.0896\n",
      "Epoch 63/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0718 - val_loss: 0.0815\n",
      "Epoch 64/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0736 - val_loss: 0.0742\n",
      "Epoch 65/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0707 - val_loss: 0.0803\n",
      "Epoch 66/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0708 - val_loss: 0.0808\n",
      "Epoch 67/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0700 - val_loss: 0.0789\n",
      "Epoch 68/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0696 - val_loss: 0.0810\n",
      "Epoch 69/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0830\n",
      "Epoch 70/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0714 - val_loss: 0.0938\n",
      "Epoch 71/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0687 - val_loss: 0.0829\n",
      "Epoch 72/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0709 - val_loss: 0.0802\n",
      "Epoch 73/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0703 - val_loss: 0.0890\n",
      "Epoch 74/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0699 - val_loss: 0.0785\n",
      "Epoch 75/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0790\n",
      "Epoch 76/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0701 - val_loss: 0.0848\n",
      "Epoch 77/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0804\n",
      "Epoch 78/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0678 - val_loss: 0.0740\n",
      "Epoch 79/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0688 - val_loss: 0.0819\n",
      "Epoch 80/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0847\n",
      "Epoch 81/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0680 - val_loss: 0.0725\n",
      "Epoch 82/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0688 - val_loss: 0.0837\n",
      "Epoch 83/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0662 - val_loss: 0.0832\n",
      "Epoch 84/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0659 - val_loss: 0.0802\n",
      "Epoch 85/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0782\n",
      "Epoch 86/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0663 - val_loss: 0.0876\n",
      "Epoch 87/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0655 - val_loss: 0.0869\n",
      "Epoch 88/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0679 - val_loss: 0.0818\n",
      "Epoch 89/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0923\n",
      "Epoch 90/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0677 - val_loss: 0.0747\n",
      "Epoch 91/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0665 - val_loss: 0.0773\n",
      "Epoch 92/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0670 - val_loss: 0.0690\n",
      "Epoch 93/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0658 - val_loss: 0.0713\n",
      "Epoch 94/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0660 - val_loss: 0.0700\n",
      "Epoch 95/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0647 - val_loss: 0.0718\n",
      "Epoch 96/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0638 - val_loss: 0.0823\n",
      "Epoch 97/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0639 - val_loss: 0.0875\n",
      "Epoch 98/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0643 - val_loss: 0.0834\n",
      "Epoch 99/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0643 - val_loss: 0.0865\n",
      "Epoch 100/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0666 - val_loss: 0.0789\n"
     ]
    }
   ],
   "source": [
    "look_back = 30\n",
    "\n",
    "# Create a KerasRegressor\n",
    "features = trainX.shape[2]\n",
    "model = create_keras_regressor(look_back, features)\n",
    "\n",
    "print(\"STarting gcv\")\n",
    "# Define GridSearchCV\n",
    "lstm_grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "lstm_grid_search.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_params = lstm_grid_search.best_params_\n",
    "\n",
    "logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "print(\"fitting model\")\n",
    "\n",
    "logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "lstm_model = create_lstm_model(\n",
    "    look_back=look_back,\n",
    "    features=features,\n",
    "    units_lstm=best_params['units_lstm'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    num_lstm_layers=best_params['num_lstm_layers']\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    validation_data=(valX, valY)\n",
    ")\n",
    "\n",
    "\n",
    "forecasts = predict_replace(lstm_model, testX, autoregressive=True)\n",
    "\n",
    "testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "evaluation.append({\"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "histories.append(history.history)\n",
    "\n",
    "\n",
    "# Add results to the output dictionary\n",
    "output_data = {\n",
    "    \"best_params\": best_params,\n",
    "    \"look_back\":look_back,\n",
    "    \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "    \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "    \"history\": history.history\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the entire data dictionary to a serializable format\n",
    "output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "# Construct the output file path\n",
    "folder_name_json = f\"result_jsons_l{look_back}_noauto_glob\"\n",
    "os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "file_name_json = f\"l{look_back}__results_noauto_glob.json\"\n",
    "output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "# Writing the converted data to 'model_results.json'\n",
    "with open(output_json_file, \"w\") as file:\n",
    "    json.dump(output_data_serializable, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#### Plot the border with specified colors\n",
    "germany_gpd.plot(ax=ax, edgecolor=\"red\", facecolor=\"none\")\n",
    "\n",
    "#### Remove axis\n",
    "ax.axis(\"off\")\n",
    "plt.savefig(\n",
    "    \"output/germany_border.png\", bbox_inches=\"tight\", pad_inches=0, transparent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cube_ger\n",
    "np.nanmax(cube_ger.evaporation_era5)\n",
    "cube_ger.rio.write_crs(4326, inplace=True)\n",
    "cube_ger = cube_ger.rio.clip(\n",
    "    germany_gpd.geometry.values, germany_gpd.crs, drop=False, all_touched=False\n",
    ")\n",
    "w = lexcube.Cube3DWidget(\n",
    "    cube_ger.radiation_era5, cmap=\"viridis\", vmin=0, vmax=1006265\n",
    ")  #\n",
    "w\n",
    "wa = lexcube.Cube3DWidget(\n",
    "    cube_ger.radiation_era5, cmap=\"inferno\", vmin=0, vmax=1006265.8\n",
    ")  #\n",
    "wa\n",
    "c = lexcube.Cube3DWidget(cube_ger.precipitation_era5, cmap=\"YlGnBu\", vmin=0, vmax=20)  #\n",
    "c\n",
    "d = lexcube.Cube3DWidget(\n",
    "    cube_ger.air_temperature_2m, cmap=\"cividis\", vmin=0, vmax=25\n",
    ")  #\n",
    "d\n",
    "w = lexcube.Cube3DWidget(\n",
    "    cube_ger.air_temperature_2m, cmap=\"bamako_r\", vmin=0, vmax=0.7\n",
    ")  #\n",
    "w\n",
    "os.getcwd()\n",
    "w.savefig(fname=\"sif_cube.png\", include_ui=False, dpi_scale=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "83278524313050289be9b55fc6fbebd999bd0974b6bc6780a3338134148971fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
