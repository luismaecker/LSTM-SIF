{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "import logging\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lexcube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(filename):\n",
    "    logging.basicConfig(\n",
    "        filename=filename,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    )\n",
    "\n",
    "def reset_logging(filename):\n",
    "    # Get the root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    \n",
    "    # Remove all handlers associated with the root logger\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "    \n",
    "    # Configure logging with the new filename\n",
    "    configure_logging(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Delete\n",
    "# with forest classes\n",
    "def data_preprocess(df, variables, forest_vars):\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    variables (list of str): Columns to normalize and convert to float32.\n",
    "    forest_vars (list of str): Columns to keep unscaled.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed and normalized DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(inplace=False)\n",
    "    df = df.sort_values(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df[variables] = df[variables].astype(\"float32\")\n",
    "\n",
    "    # Scale only the specified variables\n",
    "    scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler_minmax.fit_transform(df[variables])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n",
    "\n",
    "    # Combine scaled variables with unscaled forest variables and other columns\n",
    "    all_data_scaled = scaled_df.copy()\n",
    "    all_data_scaled[forest_vars] = df[forest_vars].values\n",
    "    all_data_scaled[\"time\"] = df[\"time\"].values\n",
    "    all_data_scaled[\"lat\"] = df[\"lat\"].values\n",
    "    all_data_scaled[\"lon\"] = df[\"lon\"].values\n",
    "\n",
    "    return all_data_scaled, scaler_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, variables):\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by resetting index, sorting, removing NaNs, converting types, and normalizing.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    variables (list of str): Columns to normalize and convert to float32.\n",
    "    forest_vars (list of str): Columns to keep unscaled.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed and normalized DataFrame.\n",
    "    \"\"\"\n",
    "    df.reset_index(inplace=True)\n",
    "    df.sort_values(\"time\", inplace = True)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df[variables] = df[variables].astype(\"float32\")\n",
    "\n",
    "    # Scale the data using to a mean of 0 and standard div of 1\n",
    "    # do this seperately for the target variable to be able to apply inverse_transform on the target variable only data\n",
    "    scalar_x = StandardScaler()\n",
    "    scalar_y = StandardScaler()\n",
    "    scalar_y.fit(pd.DataFrame(df, columns=['sif_gosif']))\n",
    "\n",
    "    scaled_data = scalar_x.fit_transform(df[variables])\n",
    "    \n",
    "\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=variables)\n",
    "\n",
    "    # Combine scaled variables with unscaled forest variables and other columns\n",
    "    scaled_df[\"time\"] = df[\"time\"].values\n",
    "    scaled_df[\"lat\"] = df[\"lat\"].values\n",
    "    scaled_df[\"lon\"] = df[\"lon\"].values\n",
    "\n",
    "    return scaled_df, scalar_x, scalar_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_matrix(data_arr, look_back, target_col =  \"sif_gosif\", autoregressive = True):\n",
    "    \"\"\"\n",
    "    Convert the dataset into input features and target variable with specified look-back period.\n",
    "\n",
    "    Parameters:\n",
    "    data_arr (np.array): Input dataset with features and target in the last column.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "    target_col (string): Name of target variabel column.\n",
    "    exclude_cols (list): List of Strings containing the column names to be excluded.\n",
    "\n",
    "    Returns:\n",
    "    np.array, np.array: Arrays for input features (X) and target variable (Y).\n",
    "    \"\"\"\n",
    "    data_arr_x = data_arr.drop(columns=target_col)\n",
    "    data_arr_y = data_arr[target_col]\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    if autoregressive:\n",
    "\n",
    "        # start range at 1 as we use the shifted target variable as a feature - one timestep before the other features begin  \n",
    "        # we go from i to the next look_back timesteps, so we need to stop look_back timesteps before the end of the array\n",
    "        for i in range(1, len(data_arr_x) - look_back):\n",
    "            \n",
    "            # when modelling timestep t, d is t+1\n",
    "            d = i + look_back\n",
    "\n",
    "            x_seq = np.array(data_arr_x[i:d])\n",
    "\n",
    "            y_shifted = np.array(data_arr_y[(i - 1) : (d - 1)]).reshape((-1, 1))\n",
    "\n",
    "            assert x_seq.shape[0] == y_shifted.shape[0]\n",
    "\n",
    "            x_sequence = np.hstack([x_seq, y_shifted])\n",
    "\n",
    "            X.append(x_sequence)\n",
    "            Y.append(data_arr_y.iloc[d - 1])\n",
    "\n",
    "    else:\n",
    "        for i in range(1, len(data_arr_x) - look_back):\n",
    "            d = i + look_back\n",
    "            x_seq = np.array(data_arr_x[i:d])\n",
    "            X.append(x_seq)\n",
    "            Y.append(data_arr_y.iloc[d - 1])\n",
    "\n",
    "    \n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(df_scaled, lat, lon, look_back, target_col=\"sif_gosif\"):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    lat (float): Latitude to filter data.\n",
    "    lon (float): Longitude to filter data.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "\n",
    "    train_data = df_scaled[df_scaled[\"time\"].dt.year <= 2015]\n",
    "    val_data = df_scaled[\n",
    "        (df_scaled[\"time\"].dt.year == 2016) | (df_scaled[\"time\"].dt.year == 2017)\n",
    "    ]\n",
    "    test_data = df_scaled[(df_scaled[\"time\"].dt.year >= 2018)]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    # Extend the validation and test sets by the look-back period to include necessary preceding time steps\n",
    "    if not train_data.empty:\n",
    "        val = pd.concat([train.iloc[-(look_back):], val])\n",
    "    if not val_data.empty:\n",
    "        test = pd.concat([val.iloc[-(look_back):], test])\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_scaled, lat, lon, look_back, target_col=\"sif_gosif\", autoregressive = True):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for a specified location and look-back period.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are neede\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    lat (float): Latitude to filter data.\n",
    "    lon (float): Longitude to filter data.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    df_scaled = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "\n",
    "    first_index_2017 = df_scaled[df_scaled[\"time\"].dt.year == 2017].index[0]\n",
    "    val_end_index =  first_index_2017 + look_back\n",
    "\n",
    "\n",
    "    train_data = df_scaled[df_scaled[\"time\"].dt.year <= 2014]\n",
    "    \n",
    "    val_data = df_scaled[\n",
    "        (df_scaled[\"time\"].dt.year == 2015) | \n",
    "        (df_scaled[\"time\"].dt.year == 2016) | \n",
    "        ((df_scaled[\"time\"].dt.year == 2017) & (df_scaled.index < val_end_index))\n",
    "        ]\n",
    "\n",
    "    test_data = df_scaled[\n",
    "        (df_scaled.index >= val_end_index) |\n",
    "        (df_scaled[\"time\"].dt.year >= 2018)\n",
    "        ]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col, autoregressive=autoregressive)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col, autoregressive=autoregressive)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col, autoregressive=autoregressive)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_lstm_model(look_back, features, units_lstm=50, units_dense=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Create an LSTM model with the specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    look_back (int): The number of previous time steps to use as input.\n",
    "    features (int): The number of features in the input data.\n",
    "    units_lstm (int): Number of units in the LSTM layer(s).\n",
    "    units_dense (int): Number of units in the Dense layer.\n",
    "    activation (str): Activation function to use.\n",
    "    optimizer (str): Optimizer to use ('adam' or 'rmsprop').\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    dropout_rate (float): Dropout rate to use after LSTM layers.\n",
    "    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back, features)))\n",
    "\n",
    "    if num_lstm_layers == 1:\n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 2:\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "       \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 3:\n",
    "    \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "    # model.add(Dense(units_dense, activation=activation))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_lstm_model(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1, activation='relu'):\n",
    "    \"\"\"\n",
    "    Create an LSTM model with the specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    look_back (int): The number of previous time steps to use as input.\n",
    "    features (int): The number of features in the input data.\n",
    "    units_lstm (int): Number of units in the LSTM layer(s).\n",
    "    activation (str): Activation function to use.\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    dropout_rate (float): Dropout rate to use after LSTM layers.\n",
    "    num_lstm_layers (int): Number of LSTM layers (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back, features)))\n",
    "\n",
    "    if num_lstm_layers == 1:\n",
    "        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "\n",
    "    elif num_lstm_layers == 2:\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "       \n",
    "        model.add(LSTM(units_lstm, activation=activation))\n",
    "\n",
    "    elif num_lstm_layers == 3:\n",
    "    \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "        \n",
    "        model.add(LSTM(units_lstm, activation=activation, dropout=dropout_rate, recurrent_dropout = dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "  \n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a KerasRegressor for GridSearchCV\n",
    "def create_keras_regressor(look_back, features, units_lstm=50, learning_rate=0.001, dropout_rate=0.2, num_lstm_layers=1,  activation='relu', optimizer='adam'):\n",
    "    return KerasRegressor(\n",
    "        model=create_lstm_model,\n",
    "        look_back=look_back,\n",
    "        features = features,\n",
    "        units_lstm=units_lstm, \n",
    "        learning_rate=learning_rate, \n",
    "        dropout_rate=dropout_rate, \n",
    "        num_lstm_layers=num_lstm_layers,  \n",
    "        activation=activation, \n",
    "        optimizer=optimizer,\n",
    "        verbose = 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterative prediction and substitution\n",
    "def predict_replace(model, X_test, autoregressive = True):\n",
    "    \"\"\"\n",
    "    Generates predictions and updates the test set input for iterative forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.Model): Trained LSTM model.\n",
    "    X_test (array): Test data to predict.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Array of forecasted values.\n",
    "    \"\"\"\n",
    "    forecasts = []\n",
    "    \n",
    "    # sequentially replace shifted sif data (in X_test) by forecasts \n",
    "    # after modelling replace according value in X_test with prediction and give all values shifted by 1 timestep to the next sequence.\n",
    "    \n",
    "    if autoregressive:\n",
    "        for i in range(len(X_test)):\n",
    "            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n",
    "            forecasts.append(forecast[0][0])\n",
    "            if i < len(X_test) - 1:\n",
    "                X_test[i + 1, :-1, -1] = X_test[i + 1, 1:, -1]\n",
    "                X_test[i + 1, -1, -1] = forecast[0][0]\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(X_test)):\n",
    "            forecast = model.predict(X_test[i].reshape(1, look_back, -1), verbose=0)\n",
    "            forecasts.append(forecast[0][0])\n",
    "\n",
    "    forecasts_array = np.array(forecasts)\n",
    "\n",
    "\n",
    "    return forecasts_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating - Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Delete\n",
    "\n",
    "def plot_training(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and metrics from the training history.\n",
    "\n",
    "    Parameters:\n",
    "    history (History): History object from Keras training session.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(history.history[\"mse\"], label=\"Train MSE\")\n",
    "    plt.plot(history.history[\"val_mse\"], label=\"Validation MSE\")\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "    plt.title(\"Model Loss and Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / Metric\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and metrics from the training \n",
    "\n",
    "    Parameters:\n",
    "    history (History): History object from Keras training session.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(history[\"loss\"], label=\"Train MSE\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation MSE\")\n",
    "    plt.title(\"Model Loss and Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / Metric\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation function for model performance\n",
    "def evaluate_model(true_values, predicted_values, data_type=\"Validation\"):\n",
    "    # Remove NaN values\n",
    "    mask = ~np.isnan(predicted_values)\n",
    "\n",
    "    true_values = true_values[mask]\n",
    "    predicted_values = predicted_values[mask]\n",
    "\n",
    "    if len(true_values) > 0 and len(predicted_values) > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "        mae = mean_absolute_error(true_values, predicted_values)\n",
    "        print(f\"{data_type} Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "        print(f\"{data_type} Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    else:\n",
    "        print(f\"{data_type} evaluation skipped due to insufficient data.\")\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot predicted vs. actual values\n",
    "def plot_predicted_vs_actual(testY, forecasts, test_index, look_back):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(sorted(test_index[look_back + 1 :]), testY, label=\"Actual\")\n",
    "    plt.plot(sorted(test_index[look_back + 1 :]), forecasts, label=\"Predicted\")\n",
    "    plt.title(\"Actual vs Predicted Values\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Function to plot predicted vs. actual values with MSE in subplots\n",
    "def plot_multiple_results(results, evaluation, unique_pairs, look_back):\n",
    "    num_plots = len(results)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_plots:\n",
    "            testY, forecasts = results[i]\n",
    "            mae, rmse = evaluation[i][\"mae\"], evaluation[i][\"rmse\"]\n",
    "            lat, lon = unique_pairs.iloc[i]\n",
    "            time_index = sorted(test_index)\n",
    "\n",
    "            ax.plot(time_index[:-1], testY, label=\"Actual\")\n",
    "            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n",
    "            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Value\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            # Add MSE to the corner\n",
    "            ax.text(\n",
    "                0.95,\n",
    "                0.05,\n",
    "                f\"RMSE: {rmse:.2f}\",\n",
    "                verticalalignment=\"bottom\",\n",
    "                horizontalalignment=\"right\",\n",
    "                transform=ax.transAxes,\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Function to plot predicted vs. actual values with MSE in subplots\n",
    "def plot_multiple_results(results, evaluation, unique_pairs, look_back):\n",
    "    num_plots = len(results)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_plots:\n",
    "            testY, forecasts = [results[\"true_values\"]][i], [results[\"predicted_values\"]][i]\n",
    "            mae, rmse = [evaluation[\"mae\"]][i], [evaluation[\"rmse\"]][i]\n",
    "            lat, lon = unique_pairs.iloc[i]\n",
    "            time_index = sorted(test_index)\n",
    "\n",
    "            ax.plot(time_index[:-1], testY, label=\"Actual\")\n",
    "            ax.plot(time_index[:-1], forecasts, label=\"Predicted\")\n",
    "            ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.set_ylabel(\"Value\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            # Add MSE to the corner\n",
    "            ax.text(\n",
    "                0.95,\n",
    "                0.05,\n",
    "                f\"RMSE: {rmse:.2f}\",\n",
    "                verticalalignment=\"bottom\",\n",
    "                horizontalalignment=\"right\",\n",
    "                transform=ax.transAxes,\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output data to a serializable format\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert numpy arrays to lists\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # Convert numpy scalar types to Python scalars\n",
    "    elif isinstance(obj, dict):\n",
    "        # Recursively convert each item in the dictionary\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        # Recursively convert each item in the list\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    return obj  # Return the object if it's already serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Germany border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read\n",
    "germany_shp_path = os.path.join(\"..\", \"data\", \"germany_border.shp\")\n",
    "germany_gpd = gpd.read_file(germany_shp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and subset cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_path = os.path.join(\"..\",\"data\", \"processed\", \"cube_preprocessed.nc\")\n",
    "cube_ger = xr.open_dataset(cube_ger_path, chunks={\"time\": 92, \"lat\": -1, \"lon\": -1})\n",
    "cube_ger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Cube only keeping cells with 50% forest cover in 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_2000_mask = (cube_ger.all_classes.isel(time=0) > 50).astype(int)\n",
    "forest_2000_mask.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_f = cube_ger.where(forest_2000_mask)\n",
    "cube_ger_f.sif_gosif.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crop with germany border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cube_ger_f.rio.write_crs(4326, inplace=True)\n",
    "cube_ger_f_crop = cube_ger_f.rio.clip(\n",
    "    germany_gpd.geometry.values, germany_gpd.crs, drop=False, all_touched=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cube_ger_f_crop.sif_gosif.isel(time=0).plot(ax=ax)\n",
    "germany_gpd.plot(ax=ax, edgecolor=\"red\", facecolor=\"none\")  # Adjust colors as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"sif_gosif\",\n",
    "    \"evaporation_era5\",\n",
    "    \"precipitation_era5\",\n",
    "    \"radiation_era5\",\n",
    "    \"air_temperature_2m\",\n",
    "    \"max_air_temperature_2m\",\n",
    "    \"min_air_temperature_2m\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_ger_f_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of forest variable column names and spatial_ref, to remove them from the dataframe\n",
    "forest_vars = list(set(cube_ger_f_crop.data_vars) - set(variables) - {\"spatial_ref\"})\n",
    "\n",
    "# Create dataframe from cube\n",
    "all_data_df = cube_ger_f_crop.to_dataframe().dropna().drop(columns=forest_vars).drop(columns=\"spatial_ref\")\n",
    "all_data_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale data and drop forest variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sif_gosif</th>\n",
       "      <th>evaporation_era5</th>\n",
       "      <th>precipitation_era5</th>\n",
       "      <th>radiation_era5</th>\n",
       "      <th>air_temperature_2m</th>\n",
       "      <th>max_air_temperature_2m</th>\n",
       "      <th>min_air_temperature_2m</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.011922</td>\n",
       "      <td>1.248912</td>\n",
       "      <td>-1.180376</td>\n",
       "      <td>-0.826753</td>\n",
       "      <td>-1.925783</td>\n",
       "      <td>-1.800210</td>\n",
       "      <td>-1.667825</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.625</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.009902</td>\n",
       "      <td>1.367647</td>\n",
       "      <td>-1.089934</td>\n",
       "      <td>-1.273543</td>\n",
       "      <td>-2.339386</td>\n",
       "      <td>-1.955863</td>\n",
       "      <td>-2.981955</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.125</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.985991</td>\n",
       "      <td>1.319931</td>\n",
       "      <td>-1.075546</td>\n",
       "      <td>-1.220842</td>\n",
       "      <td>-1.901189</td>\n",
       "      <td>-1.884916</td>\n",
       "      <td>-1.989540</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.004664</td>\n",
       "      <td>1.381150</td>\n",
       "      <td>-0.963128</td>\n",
       "      <td>-1.198117</td>\n",
       "      <td>-2.520454</td>\n",
       "      <td>-2.205761</td>\n",
       "      <td>-2.930598</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>48.875</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.988567</td>\n",
       "      <td>1.377211</td>\n",
       "      <td>-1.040038</td>\n",
       "      <td>-1.240012</td>\n",
       "      <td>-2.166432</td>\n",
       "      <td>-1.972878</td>\n",
       "      <td>-2.397991</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71755</th>\n",
       "      <td>-0.899450</td>\n",
       "      <td>1.381330</td>\n",
       "      <td>1.892409</td>\n",
       "      <td>-1.410915</td>\n",
       "      <td>-0.169287</td>\n",
       "      <td>-0.503028</td>\n",
       "      <td>-0.171374</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71756</th>\n",
       "      <td>-0.917507</td>\n",
       "      <td>1.459768</td>\n",
       "      <td>1.595228</td>\n",
       "      <td>-1.420709</td>\n",
       "      <td>-0.239737</td>\n",
       "      <td>-0.546264</td>\n",
       "      <td>-0.250604</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71757</th>\n",
       "      <td>-0.948588</td>\n",
       "      <td>1.523909</td>\n",
       "      <td>0.908647</td>\n",
       "      <td>-1.440518</td>\n",
       "      <td>-0.336157</td>\n",
       "      <td>-0.573856</td>\n",
       "      <td>-0.421814</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71758</th>\n",
       "      <td>-0.957456</td>\n",
       "      <td>1.553456</td>\n",
       "      <td>1.080570</td>\n",
       "      <td>-1.408002</td>\n",
       "      <td>-0.465537</td>\n",
       "      <td>-0.577041</td>\n",
       "      <td>-0.561901</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>50.625</td>\n",
       "      <td>10.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71759</th>\n",
       "      <td>-0.972470</td>\n",
       "      <td>1.493259</td>\n",
       "      <td>-0.399016</td>\n",
       "      <td>-1.500486</td>\n",
       "      <td>-0.949547</td>\n",
       "      <td>-0.504090</td>\n",
       "      <td>-1.852475</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>53.125</td>\n",
       "      <td>13.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71760 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sif_gosif  evaporation_era5  precipitation_era5  radiation_era5  \\\n",
       "0      -1.011922          1.248912           -1.180376       -0.826753   \n",
       "1      -1.009902          1.367647           -1.089934       -1.273543   \n",
       "2      -0.985991          1.319931           -1.075546       -1.220842   \n",
       "3      -1.004664          1.381150           -0.963128       -1.198117   \n",
       "4      -0.988567          1.377211           -1.040038       -1.240012   \n",
       "...          ...               ...                 ...             ...   \n",
       "71755  -0.899450          1.381330            1.892409       -1.410915   \n",
       "71756  -0.917507          1.459768            1.595228       -1.420709   \n",
       "71757  -0.948588          1.523909            0.908647       -1.440518   \n",
       "71758  -0.957456          1.553456            1.080570       -1.408002   \n",
       "71759  -0.972470          1.493259           -0.399016       -1.500486   \n",
       "\n",
       "       air_temperature_2m  max_air_temperature_2m  min_air_temperature_2m  \\\n",
       "0               -1.925783               -1.800210               -1.667825   \n",
       "1               -2.339386               -1.955863               -2.981955   \n",
       "2               -1.901189               -1.884916               -1.989540   \n",
       "3               -2.520454               -2.205761               -2.930598   \n",
       "4               -2.166432               -1.972878               -2.397991   \n",
       "...                   ...                     ...                     ...   \n",
       "71755           -0.169287               -0.503028               -0.171374   \n",
       "71756           -0.239737               -0.546264               -0.250604   \n",
       "71757           -0.336157               -0.573856               -0.421814   \n",
       "71758           -0.465537               -0.577041               -0.561901   \n",
       "71759           -0.949547               -0.504090               -1.852475   \n",
       "\n",
       "            time     lat     lon  \n",
       "0     2002-01-05  47.625   7.625  \n",
       "1     2002-01-05  50.125   9.875  \n",
       "2     2002-01-05  50.375   6.875  \n",
       "3     2002-01-05  48.875  13.625  \n",
       "4     2002-01-05  50.375  10.875  \n",
       "...          ...     ...     ...  \n",
       "71755 2021-12-31  51.125   7.625  \n",
       "71756 2021-12-31  51.125   7.875  \n",
       "71757 2021-12-31  51.125   8.375  \n",
       "71758 2021-12-31  50.625  10.625  \n",
       "71759 2021-12-31  53.125  13.375  \n",
       "\n",
       "[71760 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_scaled, scalar_x, scalar_y = data_preprocess(all_data_df, variables)\n",
    "all_data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model multiple timeseries with with local model using hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.625</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.125</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.375</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.875</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.375</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>48.875</td>\n",
       "      <td>12.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>47.625</td>\n",
       "      <td>8.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>49.375</td>\n",
       "      <td>12.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>49.375</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>49.625</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lat     lon\n",
       "0   47.625   7.625\n",
       "1   50.125   9.875\n",
       "2   50.375   6.875\n",
       "3   48.875  13.625\n",
       "4   50.375  10.875\n",
       "..     ...     ...\n",
       "73  48.875  12.875\n",
       "74  47.625   8.125\n",
       "75  49.375  12.625\n",
       "76  49.375  11.125\n",
       "77  49.625   6.875\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique paris \n",
    "unique_pairs = all_data_scaled[[\"lat\", \"lon\"]].drop_duplicates()\n",
    "unique_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# dd/mm/YY\n",
    "current_date = today.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename= f\"logs/auto_modelling_{current_date}.log\",level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'units_lstm': [64, 128],\n",
    "    'activation': ['relu', 'tanh'], \n",
    "    'epochs': [100],    \n",
    "    'learning_rate': [0.0001],\n",
    "    'dropout_rate': [0.2,0.4],\n",
    "    'batch_size': [25],\n",
    "    'num_lstm_layers': [1, 2, 3]\n",
    "}\n",
    "\n",
    "look_backs = [15,30,45]\n",
    "\n",
    "\n",
    "results = []\n",
    "evaluation = []\n",
    "histories = []\n",
    "best_params = []\n",
    "\n",
    "number = len(unique_pairs)\n",
    "output_data = {}\n",
    "\n",
    "n_splits = 3\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting Modelling and GridsearchCV\")\n",
    "\n",
    "\n",
    "for look_back in look_backs:\n",
    "            \n",
    "    for i in range(10):\n",
    "\n",
    "        \n",
    "        lat, lon = unique_pairs.iloc[i]\n",
    "\n",
    "        logging.info(f\"Starting Grid Search for \\n lat: {lat}\\n lon: {lon}\")\n",
    "\n",
    "        trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "            all_data_scaled, lat, lon, look_back=look_back\n",
    "        )\n",
    "\n",
    "        # Create a KerasRegressor\n",
    "        features = trainX.shape[2]\n",
    "        model = create_keras_regressor(look_back, features)\n",
    "\n",
    "        # Define GridSearchCV\n",
    "        lstm_grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Perform grid search\n",
    "        lstm_grid_search.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Get the best model from the grid search\n",
    "        best_params = lstm_grid_search.best_params_\n",
    "\n",
    "        logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "        logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "        lstm_model = create_lstm_model(\n",
    "            look_back=look_back,\n",
    "            features=features,\n",
    "            units_lstm=best_params['units_lstm'],\n",
    "            activation=best_params['activation'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            num_lstm_layers=best_params['num_lstm_layers']\n",
    "        )\n",
    "\n",
    "        history = lstm_model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            verbose=1,\n",
    "            validation_data=(valX, valY)\n",
    "        )\n",
    "\n",
    "\n",
    "        forecasts = predict_replace(lstm_model, testX)\n",
    "\n",
    "        testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "        forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "        rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "        mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "        results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "        evaluation.append({\"lat\": lat, \"lon\": lon, \"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "        histories.append(history.history)\n",
    "\n",
    "\n",
    "        # Add results to the output dictionary\n",
    "        output_data[(lat, lon)] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"look_back\":look_back,\n",
    "            \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "            \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "            \"history\": history.history\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Convert the entire data dictionary to a serializable format\n",
    "        output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data[(lat, lon)].items()}\n",
    "\n",
    "\n",
    "        # Construct the output file path\n",
    "        folder_name_json = f\"result_jsons_l{look_back}\"\n",
    "        os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "        file_name_json = f\"l{look_back}_{str(lat).replace('.', '_')}_{str(lon).replace('.', '_')}_model_results.json\"\n",
    "        output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "        # Writing the converted data to 'model_results.json'\n",
    "        with open(output_json_file, \"w\") as file:\n",
    "            json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "        logging.info(f\"Results and evaluation written to: {output_json_file}\")\n",
    "\n",
    "        logging.info(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "        print(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "\n",
    "        logging.info(100*\"-\")\n",
    "        logging.info(100*\"-\")\n",
    "\n",
    "\n",
    "    # Convert the entire data dictionary to a serializable format\n",
    "    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "    # Writing the converted data to 'model_results.json'\n",
    "    with open(os.path.join(folder_name_json,f\"results_auto_l{look_back}_relu.json\"), \"w\") as file:\n",
    "        json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "    print(\"Results and evaluation have been written to 'model_results.json'\")\n",
    "\n",
    "\n",
    "    print(2*\"\\n\")\n",
    "    print(100*\"-\")\n",
    "    print(2*\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Setup without autoregression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_logging(f\"logs/not_regressive_{current_date}.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting Modelling and GridsearchCV\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for look_back in look_backs:\n",
    "\n",
    "    for i in range(6):\n",
    "\n",
    "        \n",
    "        lat, lon = unique_pairs.iloc[i]\n",
    "\n",
    "        logging.info(f\"Starting non autoregressive Grid Search for \\n lat: {lat}\\n lon: {lon}\")\n",
    "\n",
    "        trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "            all_data_scaled, lat, lon, look_back=look_back, autoregressive=False\n",
    "        )\n",
    "\n",
    "        # Create a KerasRegressor\n",
    "        features = trainX.shape[2]\n",
    "        model = create_keras_regressor(look_back, features)\n",
    "\n",
    "        # Define GridSearchCV\n",
    "        lstm_grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # Perform grid search\n",
    "        lstm_grid_search.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # Get the best model from the grid search\n",
    "        best_params = lstm_grid_search.best_params_\n",
    "\n",
    "        logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "        logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "        lstm_model = create_lstm_model(\n",
    "            look_back=look_back,\n",
    "            features=features,\n",
    "            units_lstm=best_params['units_lstm'],\n",
    "            activation=best_params['activation'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            num_lstm_layers=best_params['num_lstm_layers']\n",
    "        )\n",
    "\n",
    "        history = lstm_model.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            verbose=1,\n",
    "            validation_data=(valX, valY)\n",
    "        )\n",
    "\n",
    "\n",
    "        forecasts = predict_replace(lstm_model, testX, autoregressive=False)\n",
    "\n",
    "        testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "        forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "        rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "        mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "        results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "        evaluation.append({\"lat\": lat, \"lon\": lon, \"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "        histories.append(history.history)\n",
    "\n",
    "\n",
    "        # Add results to the output dictionary\n",
    "        output_data[(lat, lon)] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"look_back\":look_back,\n",
    "            \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "            \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "            \"history\": history.history\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Convert the entire data dictionary to a serializable format\n",
    "        output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data[(lat, lon)].items()}\n",
    "\n",
    "        # Construct the output file path\n",
    "        folder_name_json = f\"result_jsons_l{look_back}_noauto\"\n",
    "        os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "        file_name_json = f\"l{look_back}_{str(lat).replace('.', '_')}_{str(lon).replace('.', '_')}_model_results_noauto.json\"\n",
    "        output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "        # Writing the converted data to 'model_results.json'\n",
    "        with open(output_json_file, \"w\") as file:\n",
    "            json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "        logging.info(f\"Results and evaluation written to: {output_json_file}\")\n",
    "\n",
    "        logging.info(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "        print(f\"Completed {i + 1}/{len(unique_pairs)}\")\n",
    "\n",
    "        logging.info(100*\"-\")\n",
    "        logging.info(100*\"-\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the entire data dictionary to a serializable format\n",
    "    output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "    # Writing the converted data to 'model_results.json'\n",
    "    with open(os.path.join(folder_name_json,f\"results_noauto_l{look_back}_relu.json\"), \"w\") as file:\n",
    "        json.dump(output_data_serializable, file, indent=4)\n",
    "\n",
    "    print(\"Results and evaluation have been written to 'model_results.json'\")\n",
    "\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Results and testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the results and evaluation from the file\n",
    "with open(\"/home/luismaecker/team-extra/imke&luis&moritz/modelling_current/model_results_full.json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extracting the loaded data\n",
    "best_params = loaded_data['best_params']\n",
    "evaluation = loaded_data['evaluation']\n",
    "results = loaded_data['results']\n",
    "history = loaded_data['history']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = unique_pairs.iloc[1]\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index = split_data(\n",
    "    all_data_scaled, lat, lon, look_back=look_back\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_results(results, evaluation, unique_pairs, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICT PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_multiple_results(results_dict, time_index = test_index):\n",
    "    num_plots = len(results_dict)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "    if num_rows * num_cols > 1:\n",
    "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "    else:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for i, (lat_lon, data_dict) in enumerate(results_dict.items()):\n",
    "        ax = axes[i]\n",
    "        testY = data_dict['results']['true_values']\n",
    "        forecasts = data_dict['results']['predicted_values']\n",
    "        mae = data_dict['evaluation']['mae']\n",
    "        rmse = data_dict['evaluation']['rmse']\n",
    "\n",
    "        # Assuming lat_lon keys are in the format \"(lat, lon)\" and need to be converted from string\n",
    "        lat, lon = eval(lat_lon)  # Convert the string key back to tuple if necessary\n",
    "\n",
    "        # Generate a time index from the length of the testY data\n",
    "        # time_index = range(len(testY))\n",
    "\n",
    "        ax.plot(time_index, testY, label=\"Actual\")\n",
    "        ax.plot(time_index, forecasts, label=\"Predicted\")\n",
    "        ax.set_title(f\"Lat: {lat}, Lon: {lon}\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Add MSE to the corner\n",
    "        ax.text(0.95, 0.05, f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}\", \n",
    "                verticalalignment='bottom', horizontalalignment='right', \n",
    "                transform=ax.transAxes, color='red', fontsize=12)\n",
    "\n",
    "    # Adjust the layout to prevent overlap and make sure all plots are visible\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example of calling the function with your dictionary of results\n",
    "# plot_multiple_results(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the results and evaluation from the file\n",
    "with open(\"/home/luismaecker/team-extra/imke&luis&moritz/modelling_current/model_results_full.json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(lat).replace(\".\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_results(loaded_data, test_index[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_glob(df_scaled, unique_pairs, look_back, target_col=\"sif_gosif\", autoregressive=True):\n",
    "    \"\"\"\n",
    "    Splits the scaled DataFrame into training, validation, and test sets for all specified locations and look-back periods.\n",
    "    The timeframes for splitting are partly overlapping as to model timestep t, the timesteps from t to t-lookback are needed.\n",
    "\n",
    "    Parameters:\n",
    "    df_scaled (DataFrame): Preprocessed and scaled DataFrame.\n",
    "    unique_pairs (DataFrame): DataFrame containing unique pairs of latitudes and longitudes.\n",
    "    look_back (int): Number of past observations each input sample should consist of.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Arrays of features and target variables for training, validation, and test datasets, and pixel indices.\n",
    "    \"\"\"\n",
    "    combined_data = pd.DataFrame()\n",
    "    pixel_indices = {}\n",
    "\n",
    "    for idx, (lat, lon) in unique_pairs.iterrows():\n",
    "        pixel_data = df_scaled.loc[(df_scaled[\"lat\"] == lat) & (df_scaled[\"lon\"] == lon)]\n",
    "        #combined_data = pd.concat([combined_data, pixel_data])\n",
    "        pixel_indices[(lat, lon)] = pixel_data.index\n",
    "\n",
    "    combined_data = df_scaled.sort_values(by=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    first_index_2017 = combined_data[combined_data[\"time\"].dt.year == 2017].index[0]\n",
    "    val_end_index = first_index_2017 + look_back\n",
    "\n",
    "    train_data = combined_data[combined_data[\"time\"].dt.year <= 2014]\n",
    "\n",
    "    val_data = combined_data[\n",
    "        (combined_data[\"time\"].dt.year == 2015) | \n",
    "        (combined_data[\"time\"].dt.year == 2016) | \n",
    "        ((combined_data[\"time\"].dt.year == 2017) & (combined_data.index < val_end_index))\n",
    "    ]\n",
    "\n",
    "    test_data = combined_data[\n",
    "        (combined_data.index >= val_end_index) |\n",
    "        (combined_data[\"time\"].dt.year >= 2018)\n",
    "    ]\n",
    "\n",
    "    train = train_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    val = val_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "    test = test_data.drop(columns=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    trainX, trainY = convert_to_matrix(train, look_back, target_col, autoregressive=autoregressive)\n",
    "    valX, valY = convert_to_matrix(val, look_back, target_col, autoregressive=autoregressive)\n",
    "    testX, testY = convert_to_matrix(test, look_back, target_col, autoregressive=autoregressive)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    valX = np.reshape(valX, (valX.shape[0], valX.shape[1], valX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    test_index = sorted(list(set(test_data.time)))\n",
    "\n",
    "    return trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Starting splitting\n"
     ]
    }
   ],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting splitting\")\n",
    "\n",
    "\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "look_back = 30\n",
    " \n",
    "\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data_glob(\n",
    "    all_data_scaled, unique_pairs = unique_pairs, look_back=look_back, autoregressive=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46613, 30, 6) (7246, 30, 6) (17808, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape,valX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(testX.shape[0]) + int(valX.shape[0]) + int(trainX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sif_gosif</th>\n",
       "      <th>evaporation_era5</th>\n",
       "      <th>precipitation_era5</th>\n",
       "      <th>radiation_era5</th>\n",
       "      <th>air_temperature_2m</th>\n",
       "      <th>max_air_temperature_2m</th>\n",
       "      <th>min_air_temperature_2m</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.011922</td>\n",
       "      <td>1.248912</td>\n",
       "      <td>-1.180376</td>\n",
       "      <td>-0.826753</td>\n",
       "      <td>-1.925783</td>\n",
       "      <td>-1.800210</td>\n",
       "      <td>-1.667825</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>47.625</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.009902</td>\n",
       "      <td>1.367647</td>\n",
       "      <td>-1.089934</td>\n",
       "      <td>-1.273543</td>\n",
       "      <td>-2.339386</td>\n",
       "      <td>-1.955863</td>\n",
       "      <td>-2.981955</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.125</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.985991</td>\n",
       "      <td>1.319931</td>\n",
       "      <td>-1.075546</td>\n",
       "      <td>-1.220842</td>\n",
       "      <td>-1.901189</td>\n",
       "      <td>-1.884916</td>\n",
       "      <td>-1.989540</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.004664</td>\n",
       "      <td>1.381150</td>\n",
       "      <td>-0.963128</td>\n",
       "      <td>-1.198117</td>\n",
       "      <td>-2.520454</td>\n",
       "      <td>-2.205761</td>\n",
       "      <td>-2.930598</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>48.875</td>\n",
       "      <td>13.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.988567</td>\n",
       "      <td>1.377211</td>\n",
       "      <td>-1.040038</td>\n",
       "      <td>-1.240012</td>\n",
       "      <td>-2.166432</td>\n",
       "      <td>-1.972878</td>\n",
       "      <td>-2.397991</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>50.375</td>\n",
       "      <td>10.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71755</th>\n",
       "      <td>-0.899450</td>\n",
       "      <td>1.381330</td>\n",
       "      <td>1.892409</td>\n",
       "      <td>-1.410915</td>\n",
       "      <td>-0.169287</td>\n",
       "      <td>-0.503028</td>\n",
       "      <td>-0.171374</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71756</th>\n",
       "      <td>-0.917507</td>\n",
       "      <td>1.459768</td>\n",
       "      <td>1.595228</td>\n",
       "      <td>-1.420709</td>\n",
       "      <td>-0.239737</td>\n",
       "      <td>-0.546264</td>\n",
       "      <td>-0.250604</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>7.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71757</th>\n",
       "      <td>-0.948588</td>\n",
       "      <td>1.523909</td>\n",
       "      <td>0.908647</td>\n",
       "      <td>-1.440518</td>\n",
       "      <td>-0.336157</td>\n",
       "      <td>-0.573856</td>\n",
       "      <td>-0.421814</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>51.125</td>\n",
       "      <td>8.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71758</th>\n",
       "      <td>-0.957456</td>\n",
       "      <td>1.553456</td>\n",
       "      <td>1.080570</td>\n",
       "      <td>-1.408002</td>\n",
       "      <td>-0.465537</td>\n",
       "      <td>-0.577041</td>\n",
       "      <td>-0.561901</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>50.625</td>\n",
       "      <td>10.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71759</th>\n",
       "      <td>-0.972470</td>\n",
       "      <td>1.493259</td>\n",
       "      <td>-0.399016</td>\n",
       "      <td>-1.500486</td>\n",
       "      <td>-0.949547</td>\n",
       "      <td>-0.504090</td>\n",
       "      <td>-1.852475</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>53.125</td>\n",
       "      <td>13.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71760 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sif_gosif  evaporation_era5  precipitation_era5  radiation_era5  \\\n",
       "0      -1.011922          1.248912           -1.180376       -0.826753   \n",
       "1      -1.009902          1.367647           -1.089934       -1.273543   \n",
       "2      -0.985991          1.319931           -1.075546       -1.220842   \n",
       "3      -1.004664          1.381150           -0.963128       -1.198117   \n",
       "4      -0.988567          1.377211           -1.040038       -1.240012   \n",
       "...          ...               ...                 ...             ...   \n",
       "71755  -0.899450          1.381330            1.892409       -1.410915   \n",
       "71756  -0.917507          1.459768            1.595228       -1.420709   \n",
       "71757  -0.948588          1.523909            0.908647       -1.440518   \n",
       "71758  -0.957456          1.553456            1.080570       -1.408002   \n",
       "71759  -0.972470          1.493259           -0.399016       -1.500486   \n",
       "\n",
       "       air_temperature_2m  max_air_temperature_2m  min_air_temperature_2m  \\\n",
       "0               -1.925783               -1.800210               -1.667825   \n",
       "1               -2.339386               -1.955863               -2.981955   \n",
       "2               -1.901189               -1.884916               -1.989540   \n",
       "3               -2.520454               -2.205761               -2.930598   \n",
       "4               -2.166432               -1.972878               -2.397991   \n",
       "...                   ...                     ...                     ...   \n",
       "71755           -0.169287               -0.503028               -0.171374   \n",
       "71756           -0.239737               -0.546264               -0.250604   \n",
       "71757           -0.336157               -0.573856               -0.421814   \n",
       "71758           -0.465537               -0.577041               -0.561901   \n",
       "71759           -0.949547               -0.504090               -1.852475   \n",
       "\n",
       "            time     lat     lon  \n",
       "0     2002-01-05  47.625   7.625  \n",
       "1     2002-01-05  50.125   9.875  \n",
       "2     2002-01-05  50.375   6.875  \n",
       "3     2002-01-05  48.875  13.625  \n",
       "4     2002-01-05  50.375  10.875  \n",
       "...          ...     ...     ...  \n",
       "71755 2021-12-31  51.125   7.625  \n",
       "71756 2021-12-31  51.125   7.875  \n",
       "71757 2021-12-31  51.125   8.375  \n",
       "71758 2021-12-31  50.625  10.625  \n",
       "71759 2021-12-31  53.125  13.375  \n",
       "\n",
       "[71760 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'units_lstm': [64, 128],\n",
    "    'activation': ['tanh'], \n",
    "    'epochs': [100],    \n",
    "    'learning_rate': [0.0001],\n",
    "    'dropout_rate': [0.2],\n",
    "    'batch_size': [25],\n",
    "    'num_lstm_layers': [1, 2]\n",
    "}\n",
    "\n",
    "results = []\n",
    "evaluation = []\n",
    "histories = []\n",
    "best_params = []\n",
    "\n",
    "number = len(unique_pairs)\n",
    "output_data = {}\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STarting gcv\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:12:41.068836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:41.068835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 13:12:44.789046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 13:12:44.789909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=12.4min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=20.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=22.9min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=25.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=43.2min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=41.2min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=45.1min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=43.2min\n",
      "fitting model\n",
      "Epoch 1/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.2501 - val_loss: 0.1362\n",
      "Epoch 2/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1586 - val_loss: 0.1380\n",
      "Epoch 3/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1514 - val_loss: 0.1362\n",
      "Epoch 4/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1458 - val_loss: 0.1515\n",
      "Epoch 5/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1432 - val_loss: 0.1423\n",
      "Epoch 6/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1443 - val_loss: 0.1485\n",
      "Epoch 7/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1415 - val_loss: 0.1686\n",
      "Epoch 8/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1373 - val_loss: 0.1377\n",
      "Epoch 9/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1364 - val_loss: 0.1467\n",
      "Epoch 10/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1321 - val_loss: 0.1493\n",
      "Epoch 11/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1332 - val_loss: 0.1558\n",
      "Epoch 12/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1318 - val_loss: 0.1592\n",
      "Epoch 13/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1320 - val_loss: 0.1521\n",
      "Epoch 14/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1246 - val_loss: 0.1509\n",
      "Epoch 15/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1285 - val_loss: 0.1548\n",
      "Epoch 16/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1261 - val_loss: 0.1445\n",
      "Epoch 17/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1276 - val_loss: 0.1578\n",
      "Epoch 18/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1250 - val_loss: 0.1395\n",
      "Epoch 19/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1266 - val_loss: 0.1521\n",
      "Epoch 20/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1241 - val_loss: 0.1488\n",
      "Epoch 21/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.1271 - val_loss: 0.1519\n",
      "Epoch 22/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1227 - val_loss: 0.1483\n",
      "Epoch 23/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.1475\n",
      "Epoch 24/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1211 - val_loss: 0.1471\n",
      "Epoch 25/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1212 - val_loss: 0.1365\n",
      "Epoch 26/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1164 - val_loss: 0.1672\n",
      "Epoch 27/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.1655\n",
      "Epoch 28/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1193 - val_loss: 0.1501\n",
      "Epoch 29/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1168 - val_loss: 0.1528\n",
      "Epoch 30/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1177 - val_loss: 0.1452\n",
      "Epoch 31/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1173 - val_loss: 0.1530\n",
      "Epoch 32/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1202 - val_loss: 0.1536\n",
      "Epoch 33/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1158 - val_loss: 0.1455\n",
      "Epoch 34/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1164 - val_loss: 0.1453\n",
      "Epoch 35/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1136 - val_loss: 0.1463\n",
      "Epoch 36/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1147 - val_loss: 0.1505\n",
      "Epoch 37/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1137 - val_loss: 0.1323\n",
      "Epoch 38/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1152 - val_loss: 0.1386\n",
      "Epoch 39/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1161 - val_loss: 0.1547\n",
      "Epoch 40/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1105 - val_loss: 0.1488\n",
      "Epoch 41/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1127 - val_loss: 0.1515\n",
      "Epoch 42/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1106 - val_loss: 0.1528\n",
      "Epoch 43/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1134 - val_loss: 0.1497\n",
      "Epoch 44/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1109 - val_loss: 0.1482\n",
      "Epoch 45/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1118 - val_loss: 0.1454\n",
      "Epoch 46/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1145 - val_loss: 0.1414\n",
      "Epoch 47/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1102 - val_loss: 0.1493\n",
      "Epoch 48/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1098 - val_loss: 0.1512\n",
      "Epoch 49/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1081 - val_loss: 0.1455\n",
      "Epoch 50/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1118 - val_loss: 0.1384\n",
      "Epoch 51/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1112 - val_loss: 0.1562\n",
      "Epoch 52/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1069 - val_loss: 0.1411\n",
      "Epoch 53/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - loss: 0.1081 - val_loss: 0.1530\n",
      "Epoch 54/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - loss: 0.1066 - val_loss: 0.1517\n",
      "Epoch 55/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1114 - val_loss: 0.1485\n",
      "Epoch 56/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1112 - val_loss: 0.1383\n",
      "Epoch 57/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1079 - val_loss: 0.1648\n",
      "Epoch 58/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.1081 - val_loss: 0.1428\n",
      "Epoch 59/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1084 - val_loss: 0.1451\n",
      "Epoch 60/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1076 - val_loss: 0.1427\n",
      "Epoch 61/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1028 - val_loss: 0.1416\n",
      "Epoch 62/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1064 - val_loss: 0.1389\n",
      "Epoch 63/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1057 - val_loss: 0.1503\n",
      "Epoch 64/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.1049 - val_loss: 0.1373\n",
      "Epoch 65/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1056 - val_loss: 0.1449\n",
      "Epoch 66/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1059 - val_loss: 0.1389\n",
      "Epoch 67/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1036 - val_loss: 0.1526\n",
      "Epoch 68/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1068 - val_loss: 0.1578\n",
      "Epoch 69/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1042 - val_loss: 0.1596\n",
      "Epoch 70/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1044 - val_loss: 0.1476\n",
      "Epoch 71/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1019 - val_loss: 0.1433\n",
      "Epoch 72/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1041 - val_loss: 0.1410\n",
      "Epoch 73/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1012 - val_loss: 0.1421\n",
      "Epoch 74/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1006 - val_loss: 0.1443\n",
      "Epoch 75/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1027 - val_loss: 0.1410\n",
      "Epoch 76/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1012 - val_loss: 0.1493\n",
      "Epoch 77/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1014 - val_loss: 0.1476\n",
      "Epoch 78/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.1020 - val_loss: 0.1403\n",
      "Epoch 79/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0998 - val_loss: 0.1378\n",
      "Epoch 80/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1010 - val_loss: 0.1367\n",
      "Epoch 81/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0993 - val_loss: 0.1500\n",
      "Epoch 82/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1009 - val_loss: 0.1330\n",
      "Epoch 83/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1005 - val_loss: 0.1448\n",
      "Epoch 84/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0995 - val_loss: 0.1346\n",
      "Epoch 85/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.1001 - val_loss: 0.1429\n",
      "Epoch 86/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0979 - val_loss: 0.1425\n",
      "Epoch 87/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0972 - val_loss: 0.1383\n",
      "Epoch 88/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13ms/step - loss: 0.0997 - val_loss: 0.1353\n",
      "Epoch 89/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.0993 - val_loss: 0.1440\n",
      "Epoch 90/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1008 - val_loss: 0.1490\n",
      "Epoch 91/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.1000 - val_loss: 0.1514\n",
      "Epoch 92/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0967 - val_loss: 0.1629\n",
      "Epoch 93/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0974 - val_loss: 0.1498\n",
      "Epoch 94/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0969 - val_loss: 0.1440\n",
      "Epoch 95/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0991 - val_loss: 0.1349\n",
      "Epoch 96/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0983 - val_loss: 0.1462\n",
      "Epoch 97/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0969 - val_loss: 0.1450\n",
      "Epoch 98/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0973 - val_loss: 0.1429\n",
      "Epoch 99/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0955 - val_loss: 0.1303\n",
      "Epoch 100/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0985 - val_loss: 0.1482\n"
     ]
    }
   ],
   "source": [
    "look_back = 30\n",
    "\n",
    "# Create a KerasRegressor\n",
    "features = trainX.shape[2]\n",
    "model = create_keras_regressor(look_back, features)\n",
    "\n",
    "print(\"STarting gcv\")\n",
    "# Define GridSearchCV\n",
    "lstm_grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "lstm_grid_search.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_params = lstm_grid_search.best_params_\n",
    "\n",
    "logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "print(\"fitting model\")\n",
    "\n",
    "logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "lstm_model = create_lstm_model(\n",
    "    look_back=look_back,\n",
    "    features=features,\n",
    "    units_lstm=best_params['units_lstm'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    num_lstm_layers=best_params['num_lstm_layers']\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    validation_data=(valX, valY)\n",
    ")\n",
    "\n",
    "\n",
    "forecasts = predict_replace(lstm_model, testX, autoregressive=False)\n",
    "\n",
    "testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "evaluation.append({\"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "histories.append(history.history)\n",
    "\n",
    "\n",
    "# Add results to the output dictionary\n",
    "output_data = {\n",
    "    \"best_params\": best_params,\n",
    "    \"look_back\":look_back,\n",
    "    \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "    \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "    \"history\": history.history\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the entire data dictionary to a serializable format\n",
    "output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "# Construct the output file path\n",
    "folder_name_json = f\"result_jsons_l{look_back}_noauto_glob\"\n",
    "os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "file_name_json = f\"l{look_back}__results_noauto_glob.json\"\n",
    "output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "# Writing the converted data to 'model_results.json'\n",
    "with open(output_json_file, \"w\") as file:\n",
    "    json.dump(output_data_serializable, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Starting splitting\n"
     ]
    }
   ],
   "source": [
    "# Testing Setup without autoregression\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(100*\"-\")\n",
    "print(2*\"\\n\")\n",
    "\n",
    "print(\"Starting splitting\")\n",
    "\n",
    "\n",
    "\n",
    "n_splits = 2\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "look_back = 30\n",
    " \n",
    "\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY, test_index, pixel_indices = split_data_glob(\n",
    "    all_data_scaled, unique_pairs = unique_pairs, look_back=look_back, autoregressive=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STarting gcv\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 15:38:11.150775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.150775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.152418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:11.152816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 15:38:20.468549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.468711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.469246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-22 15:38:20.469604: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=12.1min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=23.0min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=64; total time=24.6min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=23.7min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=1, units_lstm=128; total time=44.3min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=42.8min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=64; total time=46.6min\n",
      "[CV] END activation=tanh, batch_size=25, dropout_rate=0.2, epochs=100, learning_rate=0.0001, num_lstm_layers=2, units_lstm=128; total time=47.3min\n",
      "fitting model\n",
      "Epoch 1/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.2030 - val_loss: 0.0834\n",
      "Epoch 2/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1216 - val_loss: 0.0910\n",
      "Epoch 3/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.1109 - val_loss: 0.0848\n",
      "Epoch 4/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.1065 - val_loss: 0.0855\n",
      "Epoch 5/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0985 - val_loss: 0.0894\n",
      "Epoch 6/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.1001 - val_loss: 0.0897\n",
      "Epoch 7/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0972 - val_loss: 0.0826\n",
      "Epoch 8/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0946 - val_loss: 0.0845\n",
      "Epoch 9/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0950 - val_loss: 0.0785\n",
      "Epoch 10/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0927 - val_loss: 0.0788\n",
      "Epoch 11/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0929 - val_loss: 0.0801\n",
      "Epoch 12/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0929 - val_loss: 0.0780\n",
      "Epoch 13/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0897 - val_loss: 0.0764\n",
      "Epoch 14/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0890 - val_loss: 0.0883\n",
      "Epoch 15/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0883 - val_loss: 0.0876\n",
      "Epoch 16/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0897 - val_loss: 0.0905\n",
      "Epoch 17/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0851 - val_loss: 0.0765\n",
      "Epoch 18/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0871 - val_loss: 0.0813\n",
      "Epoch 19/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0868 - val_loss: 0.0902\n",
      "Epoch 20/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0849 - val_loss: 0.0899\n",
      "Epoch 21/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0843 - val_loss: 0.0925\n",
      "Epoch 22/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0854 - val_loss: 0.0785\n",
      "Epoch 23/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0847 - val_loss: 0.0892\n",
      "Epoch 24/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0854 - val_loss: 0.0829\n",
      "Epoch 25/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0874\n",
      "Epoch 26/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0825 - val_loss: 0.0885\n",
      "Epoch 27/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0831 - val_loss: 0.0888\n",
      "Epoch 28/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0839 - val_loss: 0.0774\n",
      "Epoch 29/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0822 - val_loss: 0.0834\n",
      "Epoch 30/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0817 - val_loss: 0.0792\n",
      "Epoch 31/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0809 - val_loss: 0.0894\n",
      "Epoch 32/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0792 - val_loss: 0.0922\n",
      "Epoch 33/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0786 - val_loss: 0.0846\n",
      "Epoch 34/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0791 - val_loss: 0.0827\n",
      "Epoch 35/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0802 - val_loss: 0.0791\n",
      "Epoch 36/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0802 - val_loss: 0.0861\n",
      "Epoch 37/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0848\n",
      "Epoch 38/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0783 - val_loss: 0.0951\n",
      "Epoch 39/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0852\n",
      "Epoch 40/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0808 - val_loss: 0.0873\n",
      "Epoch 41/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0766 - val_loss: 0.0768\n",
      "Epoch 42/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0798 - val_loss: 0.0910\n",
      "Epoch 43/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0785 - val_loss: 0.0817\n",
      "Epoch 44/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0758 - val_loss: 0.0893\n",
      "Epoch 45/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0750 - val_loss: 0.0859\n",
      "Epoch 46/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0793 - val_loss: 0.0854\n",
      "Epoch 47/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0763 - val_loss: 0.0839\n",
      "Epoch 48/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0742 - val_loss: 0.0791\n",
      "Epoch 49/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0756 - val_loss: 0.0781\n",
      "Epoch 50/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0773 - val_loss: 0.0860\n",
      "Epoch 51/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0751 - val_loss: 0.0896\n",
      "Epoch 52/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0749 - val_loss: 0.0838\n",
      "Epoch 53/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0742 - val_loss: 0.0948\n",
      "Epoch 54/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0735 - val_loss: 0.0791\n",
      "Epoch 55/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0763 - val_loss: 0.0866\n",
      "Epoch 56/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0728 - val_loss: 0.0840\n",
      "Epoch 57/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0749 - val_loss: 0.0805\n",
      "Epoch 58/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0736 - val_loss: 0.0879\n",
      "Epoch 59/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0730 - val_loss: 0.0790\n",
      "Epoch 60/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0718 - val_loss: 0.0834\n",
      "Epoch 61/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0735 - val_loss: 0.0839\n",
      "Epoch 62/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0739 - val_loss: 0.0896\n",
      "Epoch 63/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0718 - val_loss: 0.0815\n",
      "Epoch 64/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0736 - val_loss: 0.0742\n",
      "Epoch 65/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0707 - val_loss: 0.0803\n",
      "Epoch 66/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0708 - val_loss: 0.0808\n",
      "Epoch 67/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0700 - val_loss: 0.0789\n",
      "Epoch 68/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0696 - val_loss: 0.0810\n",
      "Epoch 69/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0830\n",
      "Epoch 70/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0714 - val_loss: 0.0938\n",
      "Epoch 71/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0687 - val_loss: 0.0829\n",
      "Epoch 72/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0709 - val_loss: 0.0802\n",
      "Epoch 73/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0703 - val_loss: 0.0890\n",
      "Epoch 74/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0699 - val_loss: 0.0785\n",
      "Epoch 75/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0790\n",
      "Epoch 76/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0701 - val_loss: 0.0848\n",
      "Epoch 77/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0698 - val_loss: 0.0804\n",
      "Epoch 78/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0678 - val_loss: 0.0740\n",
      "Epoch 79/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0688 - val_loss: 0.0819\n",
      "Epoch 80/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0847\n",
      "Epoch 81/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0680 - val_loss: 0.0725\n",
      "Epoch 82/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0688 - val_loss: 0.0837\n",
      "Epoch 83/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0662 - val_loss: 0.0832\n",
      "Epoch 84/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0659 - val_loss: 0.0802\n",
      "Epoch 85/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0782\n",
      "Epoch 86/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0663 - val_loss: 0.0876\n",
      "Epoch 87/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0655 - val_loss: 0.0869\n",
      "Epoch 88/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0679 - val_loss: 0.0818\n",
      "Epoch 89/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0676 - val_loss: 0.0923\n",
      "Epoch 90/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0677 - val_loss: 0.0747\n",
      "Epoch 91/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0665 - val_loss: 0.0773\n",
      "Epoch 92/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0670 - val_loss: 0.0690\n",
      "Epoch 93/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0658 - val_loss: 0.0713\n",
      "Epoch 94/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0660 - val_loss: 0.0700\n",
      "Epoch 95/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0647 - val_loss: 0.0718\n",
      "Epoch 96/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0638 - val_loss: 0.0823\n",
      "Epoch 97/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0639 - val_loss: 0.0875\n",
      "Epoch 98/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0643 - val_loss: 0.0834\n",
      "Epoch 99/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0643 - val_loss: 0.0865\n",
      "Epoch 100/100\n",
      "\u001b[1m1865/1865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.0666 - val_loss: 0.0789\n"
     ]
    }
   ],
   "source": [
    "look_back = 30\n",
    "\n",
    "# Create a KerasRegressor\n",
    "features = trainX.shape[2]\n",
    "model = create_keras_regressor(look_back, features)\n",
    "\n",
    "print(\"STarting gcv\")\n",
    "# Define GridSearchCV\n",
    "lstm_grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "lstm_grid_search.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_params = lstm_grid_search.best_params_\n",
    "\n",
    "logging.info(f\"Gridsearch done, The best parameters are: {best_params}\")\n",
    "\n",
    "\n",
    "print(\"fitting model\")\n",
    "\n",
    "logging.info(f\"Running and evaluating model\")\n",
    "\n",
    "lstm_model = create_lstm_model(\n",
    "    look_back=look_back,\n",
    "    features=features,\n",
    "    units_lstm=best_params['units_lstm'],\n",
    "    activation=best_params['activation'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    num_lstm_layers=best_params['num_lstm_layers']\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    validation_data=(valX, valY)\n",
    ")\n",
    "\n",
    "\n",
    "forecasts = predict_replace(lstm_model, testX, autoregressive=True)\n",
    "\n",
    "testY_rescaled = scalar_y.inverse_transform(pd.DataFrame(testY))\n",
    "forecasts_rescaled = scalar_y.inverse_transform(pd.DataFrame(forecasts))\n",
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testY_rescaled, forecasts_rescaled)\n",
    "mae = mean_absolute_error(testY_rescaled, forecasts_rescaled)\n",
    "\n",
    "results.append([testY_rescaled.tolist(), forecasts_rescaled.tolist()])\n",
    "evaluation.append({\"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "histories.append(history.history)\n",
    "\n",
    "\n",
    "# Add results to the output dictionary\n",
    "output_data = {\n",
    "    \"best_params\": best_params,\n",
    "    \"look_back\":look_back,\n",
    "    \"evaluation\": {\"mae\": mae, \"rmse\": rmse},\n",
    "    \"results\": {\"true_values\": testY_rescaled.tolist(), \"predicted_values\": forecasts_rescaled.tolist()},\n",
    "    \"history\": history.history\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the entire data dictionary to a serializable format\n",
    "output_data_serializable = {str(k): convert_to_serializable(v) for k, v in output_data.items()}\n",
    "\n",
    "# Construct the output file path\n",
    "folder_name_json = f\"result_jsons_l{look_back}_noauto_glob\"\n",
    "os.makedirs(folder_name_json, exist_ok=True)\n",
    "\n",
    "\n",
    "file_name_json = f\"l{look_back}__results_noauto_glob.json\"\n",
    "output_json_file = os.path.join(folder_name_json, file_name_json)\n",
    "\n",
    "\n",
    "# Writing the converted data to 'model_results.json'\n",
    "with open(output_json_file, \"w\") as file:\n",
    "    json.dump(output_data_serializable, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#### Plot the border with specified colors\n",
    "germany_gpd.plot(ax=ax, edgecolor=\"red\", facecolor=\"none\")\n",
    "\n",
    "#### Remove axis\n",
    "ax.axis(\"off\")\n",
    "plt.savefig(\n",
    "    \"output/germany_border.png\", bbox_inches=\"tight\", pad_inches=0, transparent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cube_ger\n",
    "np.nanmax(cube_ger.evaporation_era5)\n",
    "cube_ger.rio.write_crs(4326, inplace=True)\n",
    "cube_ger = cube_ger.rio.clip(\n",
    "    germany_gpd.geometry.values, germany_gpd.crs, drop=False, all_touched=False\n",
    ")\n",
    "w = lexcube.Cube3DWidget(\n",
    "    cube_ger.radiation_era5, cmap=\"viridis\", vmin=0, vmax=1006265\n",
    ")  #\n",
    "w\n",
    "wa = lexcube.Cube3DWidget(\n",
    "    cube_ger.radiation_era5, cmap=\"inferno\", vmin=0, vmax=1006265.8\n",
    ")  #\n",
    "wa\n",
    "c = lexcube.Cube3DWidget(cube_ger.precipitation_era5, cmap=\"YlGnBu\", vmin=0, vmax=20)  #\n",
    "c\n",
    "d = lexcube.Cube3DWidget(\n",
    "    cube_ger.air_temperature_2m, cmap=\"cividis\", vmin=0, vmax=25\n",
    ")  #\n",
    "d\n",
    "w = lexcube.Cube3DWidget(\n",
    "    cube_ger.air_temperature_2m, cmap=\"bamako_r\", vmin=0, vmax=0.7\n",
    ")  #\n",
    "w\n",
    "os.getcwd()\n",
    "w.savefig(fname=\"sif_cube.png\", include_ui=False, dpi_scale=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "vscode": {
   "interpreter": {
    "hash": "83278524313050289be9b55fc6fbebd999bd0974b6bc6780a3338134148971fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
